---
layout: post
title: "GAT"
author: dazuo
date: 2020-07-02 20:19:00 +0800
categories: [GNN]
tags: [GAT]
math: true
mermaid: true
---

`GCN`缺点：

- 模型对于同阶邻域上分配给不同邻居的权重是完全相同的（也就是`GAT`论文里说的：无法允许为邻居中的不同节点指定不同的权重）。这一点限制了模型对于空间信息的相关性的捕捉能力，这也是在很多任务上不如`GAT`的根本原因。
- `GCN`结合临近节点特征的方式和图的结构依依相关，这局限了训练所得模型在其他图结构上的泛化能力。

`GAT`提出了用注意力机制对邻近节点特征加权求和。邻近节点特征的权重完全取决于节点特征，独立于图结构。

`GAT`和`GCN`的核心区别在于如何收集并累和距离为1的邻居节点的特征表示。图注意力模型`GAT`用注意力机制替代了`GCN`中固定的标准化操作。本质上，`GAT`只是将原本`GCN`的标准化函数替换为使用注意力权重的邻居节点特征聚合函数。

`GAT`优点：

- 在`GAT`中，图中的每个节点可以根据邻节点的特征，为其分配不同的权值。
- `GAT`的另一个优点在于，引入注意力机制之后，只与相邻节点有关，即共享边的节点有关，无需得到整张图的信息：
  - 该图不需要是无向的(如果边`j→i`不存在，我们可以简单地省略计算`αij`. 
  - 它使我们的技术直接适用于`inductive learning`——包括在训练期间完全看不见的图形上的评估模型的任务。
    

图注意力网络(`Graph Attention Networks`)的重点就是`attention`. `Attention`就是图中每个`node`对于其相邻`node`的相互重要性。这个重要性可以进行量化，可以通过网络训练得到。



GAT tricks:

- 使用attention机制来描述邻接节点对目标节点的重要性。
- 采用邻接矩阵作为mask。
- 引入了attention heads, 即K， 以扩展attention机制的channel.

参考[【GNN】图注意力网络GAT（含代码讲解）](https://wenku.baidu.com/view/534f867c322b3169a45177232f60ddccda38e6ec.html)



[笔记：Pytorch-geometric: GAT代码超详细解读](https://blog.csdn.net/weixin_44839047/article/details/115724958)



- [【GNN】GAT：Attention 在 GNN 中的应用](https://blog.csdn.net/qq_27075943/article/details/106652345)
- [图注意力网络(GAT) ICLR2018, Graph Attention Network论文详解](https://blog.csdn.net/weixin_36474809/article/details/89401552)
- https://github.com/dmlc/dgl/tree/master/examples/pytorch/gat
- https://github.com/PetarV-/GAT
- [【图表示学习】pytorch实现图注意力网络GAT](https://blog.csdn.net/bqw18744018044/article/details/109962741?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-0&spm=1001.2101.3001.4242)
- [【图结构】之图注意力网络GAT详解](https://blog.csdn.net/qq_41995574/article/details/99931294?utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control&dist_request_id=1332049.331.16193551953537937&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control)

