---
layout: post
title: "EGConv-DO WE NEED ANISOTROPIC GRAPH NEURAL NETWORKS"
author: dazuo
date: 2020-07-02 20:19:00 +0800
categories: [GNN]
tags: [EGConv]
math: true
mermaid: true
---

> The Efficient Graph Convolution from the ["Adaptive Filters and Aggregator Fusion for Efficient Graph Convolutions"](https://arxiv.org/abs/2104.01481) paper.

# 摘要

图神经网络社区的常识说明：`anisotropic models`各向异性模型(模型中节点间发送的消息是源节点和目标节点的函数)用来实现最先进的性能。目前的基准测试都表明，这些模型的性能优于类似的` isotropic models`各向同性模型(模型中消息仅是源节点的函数)。

在本文中，我们提供了挑战这一常识的实践证据：我们提出了一种各向同性`GNN`，称之为高效图卷积(`EGC`,  `Efficient Graph Convolution`)，它通过使用空间自适应滤波器(` spatially-varying adaptive filters`)，表现始终优于类似的各向异性模型，包括流行的`GAT`或`PNA`架构。

我们的工作除了向`GNN`社区提出了重要的问题外，还包括对效率的重大实际影响。`EGC`具有更高的模型精度，更低的内存消耗和延迟，以及适合加速器实现的特性，同时也是现有架构即插即用的替代品。

作为各向同性模型，它的复杂度与图中的顶点数成正比，即$\Omicron(V)$，相反，各向异性的杂度与图中的边数成正比，即$\Omicron(E)$。

我们证明了`EGC在`6`个大型和不同的基准数据集上表现优于现有方法。

https://github.com/shyam196/egc提供了我们实验的代码和预训练模型。



# 引言

图神经网络(`GNN`)已成为一种有效的在任意结构的数据上建立模型的方法。例如，它们已被成功应用于计算机视觉任务：`GNN`可在点云数据上提供高性能，并用于跨图像的特征匹配。最近的工作还表明，`GNN`可应用于物理模拟。代码分析是`GNN`获得成功的另一个应用领域。

近年来，研究界对构建更具表达力、性能更好的图像处理模型给予了极大关注。对`GNN`模型进行基准测试的努力，如开放图基准测试(`Open Graph Benchmark`)的工作，试图更严格地量化不同架构的相对性能。

一个常见的结论是，各向异性模型（其中节点之间发送的消息是源节点和目标节点的函数）是性能最好的模型。相比之下，各向同性模型（其中消息仅是源节点的函数）的精度较低，即使它们比可比较的各向异性模型具有效率优势。直觉上，这个结论是令人满意的：各向异性模型本质上更具表达力，因此我们期望它们在大多数情况下表现更好。我们的工作提供了一种各向同性模型，称为高效图卷积(`EGC`)，其性能优于类似的各向异性方法，包括流行的`GAT`和`PNA`架构，从而对这种智慧提出了令人惊讶的挑战。



![image-20220228235349197](../../../../img/gnn/egcConv_paper/EGC-message.png){: width="1086" height="542"}

图`1`：许多`GNN`架构(例如`GAT`、`PNA`)中包含复杂的消息功能以提高准确性(左)。这是有问题的，因为我们必须具体化消息，导致$\Omicron(E)$内存消耗和操作数来计算消息；这些数据流模式也很难在硬件级别进行优化。这项工作表明，我们可以使用简单的消息函数，只需要$\Omicron(V)$内存消耗(右)，并提高现有`GNN`的性能。



除了为社区提供了实证结果外，我们的工作还对效率的实际影响具有重要意义，如上面图1所示。由于`EGC`是一种各向同性模型，实现了高精度，我们可以利用各向同性模型提供的效率优势，而无需在模型精度上妥协。我们已经看到了近年来由于最先进的模型利用各向异性机制来提高精度，使得`GNN`架构复杂度增加到$\Omicron(E)$。

`EGC`将复杂性降低到$\Omicron(V)$，提供了实质性的收益，尽管具体的收益取决于模型作用的图的拓扑结构。读者应该注意，我们的方法也可与其他方法相结合，以提高`GNN`的效率。例如，常见的软硬件协同设计技术（包括量化和剪枝）可与这项工作相结合，这项工作提出了一种通过改进底层架构设计来提高模型效率的正交方法。我们还注意到，我们的方法可以与图采样技术相结合，以进一步提高在具有数百万或数十亿节点的图上进行训练时的可伸缩性。



## 贡献

- 我们提出了一种新的`GNN`架构，高效图卷积`EGC`，并为其提供了空间和谱解释。
- 我们在6个大型图数据集上对我们的架构进行了严格的评估，涵盖了转换和归纳用例，并证明了`EGC`始终比强基线取得更好的结果。
- 我们提供了几项消融研究，以激发我们模型中超参数的选择。
- 我们证明，与竞争方法相比，我们的模型同时实现了更好的参数效率、延迟和内存消耗。我们实验的代码和预训练模型（包括基线）在https://github.com/shyam196/egc. 在出版时，`EGC`被贡献至了PyTorch Geometric库。



# 背景

## 深度学习的软硬件协同设计

引言中已经描述了几种常用的软硬件协同设计方法：量化、修剪和细致的架构设计都是`CNN`和`Transformers`的常见方法。除了能够从通用处理器（如`CPU`和`GPU`）获得更好的性能外，这些技术对于最大化专用加速器的回报也是必不可少的；虽然随着时间的推移，由于`CMOS`技术的改进，性能可能会提高，但进一步的改进在算法层面上没有创新。作为神经网络架构设计师，我们不能简单地依靠硬件的改进来使我们的建议在实际部署中可行。

## 图神经网络

许多`GNN`架构被视为`CNN`架构在非规则域的推广：在`CNN`中，每个节点的表示都是基于局部邻域，使用整图共享的参数来构建的。`GNN`不同，因为我们无法对邻域的大小或顺序进行假设。用于定义`GNN`的一个常见框架是消息传递神经网络`MPNN`范式。

假设图$G=(V,E)$ 有**节点特征**矩阵$\pmb{X} \in \reals^{N\times F}$、**邻接**矩阵$\pmb{A} \in \reals^{N\times N}$和可选的$D$-维**连边特征**矩阵$\pmb{E} \in \reals^{E\times D}$。我们定义一个函数$\phi$，用于计算从节点$u$到节点$v$的消息，这是一个可微且具有排列不变性`permutation-invariant `(是指特征之间没有空间位置关系)的**聚合器$\oplus$**, 和一个**更新函数$\gamma$**来计算$l+1$层的表示
$$
\pmb{h}_{l+1}^{(i)} = \gamma(\pmb{h}_{l}^{(i)}, \oplus_{j \in N(i)}[\phi(\pmb{h}_{l}^{(i)}, \pmb{h}_{l}^{(j)}, \pmb{e}_{ij})])
$$

### `GNN`的相对表达能力

研究界的共识是，各向同性`GNN`比各向异性`GNN`表达能力差；从经验上看，这得到了基准的充分支持。`Brody`等人证明，`GAT`模型可以比各向同性模型更严格地表达。`Bronstein`等人还讨论了不同类别`GNN`层的相对表达能力，并认为卷积（也称为各向同性）模型非常适合利用输入图中的同态的问题。他们进一步认为，注意力或全消息传递模型适合处理异嗜性问题，但他们承认这些架构的资源消耗和可训练性可能会令人望而却步，尤其是在全消息传递的情况下。

### 扩展和部署GNN

虽然`GNN`在一系列领域取得了成功，但在扩展和部署方面仍存在挑战。图采样是对内存存储不下的大型图或模型进行缩放训练的一种方法。不是在整图上训练，而是在采样子图上运行每个迭代；采样方法根据是按节点、按层或按子图进行采样会有所不同。

另外，GNN分布式训练系统已经被提出，以将训练扩展到单个加速器的极限之外。一些工作提出了设计用于适应缩放的架构：图像增强`MLP`，例如`SIGN`被明确设计为浅层架构，因为所有图像操作都是作为预处理步骤完成的。

其他工作包括应用神经架构搜索`NAS`来安排现有`GNN`层，或为`GNN`构建量化技术。最后，最近的一项工作表明，对`GNN`使用内存高效的可逆残差使我们能够训练比以前更深更大的`GNN`模型，从而提高最先进的精度。

### 为什么现有方法不够

值得注意的是，这些方法中有许多都有很大的局限性，我们希望用我们的工作来解决。当应用于许多涉及模型泛化到看不见图的问题时，采样方法通常无效——这是`GNN`的常见用法。我们评估了各种采样方法，并观察到即使是对内存或延迟几乎没有好处的适度采样水平，也会导致模型性能显著下降。此外，这些方法不会加速底层`GNN`，因此它们可能不会对推断延迟提供任何总体好处。也没有证据表明我们知道，当推广到看不见的图时，图增强的`MLP`能够充分发挥作用；事实上，它们在理论上不如标准`GNN`表达。我们还调查了这一设置，发现这些方法与最先进的方法相比，没有提供具有竞争力的准确性。附录`B`提供了实验细节和结果，以及对现有工作局限性的进一步讨论。

总之，我们在高效`GNN`架构设计方面的工作引起了社区的兴趣，原因有两个：

- 它提出了关于常见假设的问题，以及我们如何设计和评估`GNN`模型；我们的工作可能使我们能进一步扩展模型，从而提高准确性。
- 此外，对于需要泛化到看不见的图形的任务，如代码分析或点云处理，我们减少了内存消耗和延迟，从而使我们能够将模型部署到比以前更加资源受限的设备上。我们注意到，有效的架构设计可以有效地与其他方法相结合，包括采样、量化和剪枝。



# 我们的架构

在本节中，我们描述了我们的方法，并将理论分析推迟到下一节。我们提出了两个版本：

- 使用单个聚合器的`EGC-S`(S for Single)
- 通过合并多个聚合器来概括我们的方法的`EGC-M`(M for Multi)

我们的方法如图2所示:

![image-20220718161115255](../../../../img/gnn/egcConv_paper/EGC-S.png){: width="1086" height="542"}

​					 图2: `EGC-S`层的可视化表示。在该可视化中，我们有$3$个基本过滤器（即$B=3$），它们使用每个节点权重$w$进行组合。



## 架构描述



## 提升表达能力

