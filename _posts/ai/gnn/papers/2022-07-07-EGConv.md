---
layout: post
title: "EGConv-DO WE NEED ANISOTROPIC GRAPH NEURAL NETWORKS"
author: dazuo
date: 2020-07-02 20:19:00 +0800
categories: [GNN]
tags: [EGConv]
math: true
mermaid: true
---

> The Efficient Graph Convolution from the ["Adaptive Filters and Aggregator Fusion for Efficient Graph Convolutions"](https://arxiv.org/abs/2104.01481) paper.

# 摘要

`GNN`社区的常识表明：各向异性模型(模型中节点间发送的消息是源节点和目标节点的函数)用来实现最先进的性能。目前的`benchmarks`表明，这些模型的性能优于类似的各向同性模型(模型中消息仅是源节点的函数)。

在本文中，我们提供了挑战这一常识的实践证据：提出了一种各向同性`GNN`，高效图卷积( `EGC`)，它通过使用**空间自适应滤波器**` spatially-varying adaptive filters`，表现始终优于类似的各向异性模型，包括流行的`GAT`或`PNA`架构。

我们的工作除了向`GNN`社区提出了重要的问题外，还包括对效率的重大实际影响。`EGC`具有更高的模型精度，更低的内存消耗和延迟，以及适合加速器实现的特性，同时也是现有架构即插即用的替代品。

作为各向同性模型，它的复杂度与图中的顶点数成正比，即$\Omicron(V)$，相反，各向异性的杂度与图中的边数成正比，即$\Omicron(E)$。

我们证明了`EGC在`6个大型和不同的基准数据集上表现优于现有方法。[我们实验的代码和预训练模型](https://github.com/shyam196/egc)。



# 引言

图神经网络(`GNN`)已成为一种有效的在任意结构的数据上建立模型的方法。例如，它们已被成功应用于计算机视觉任务：`GNN`可在点云数据上提供高性能，并用于跨图像的特征匹配。最近的工作还表明，`GNN`可应用于物理模拟。代码分析是`GNN`获得成功的另一个应用领域。

近年来，研究界对构建更具表达力、性能更好的图像处理模型给予了极大关注。对`GNN`模型进行基准测试的努力，如开放图基准测试(`Open Graph Benchmark`)的工作，试图更严格地量化不同架构的相对性能。

一个常见的结论是，各向异性模型（其中节点之间发送的消息是源节点和目标节点的函数）是性能最好的模型。相比之下，各向同性模型（其中消息仅是源节点的函数）的精度较低，即使它们比可比较的各向异性模型具有效率优势。直觉上，这个结论是令人满意的：各向异性模型本质上更具表达力，因此我们期望它们在大多数情况下表现更好。我们的工作提供了一种各向同性模型，称为高效图卷积(`EGC`)，其性能优于类似的各向异性方法，包括流行的`GAT`和`PNA`架构，从而对这种智慧提出了令人惊讶的挑战。



![image-20220228235349197](../../../../img/gnn/egcConv_paper/EGC-message.png){: width="1086" height="542"}图`1`：许多`GNN`架构(例如`GAT`、`PNA`)中包含复杂的消息功能以提高准确性(左)。这是有问题的，因为我们必须具体化消息，导致$\Omicron(E)$内存消耗和操作数来计算消息；这些数据流模式也很难在硬件级别进行优化。这项工作表明，我们可以使用简单的消息函数，只需要$\Omicron(V)$内存消耗(右)，并提高现有`GNN`的性能。



除了为社区提供了实证结果外，我们的工作还对效率的实际影响具有重要意义，如上面图1所示。由于`EGC`是一种各向同性模型，实现了高精度，我们可以利用各向同性模型提供的效率优势，而无需在模型精度上妥协。我们已经看到了近年来由于最先进的模型利用各向异性机制来提高精度，使得`GNN`架构复杂度增加到$\Omicron(E)$。

`EGC`将复杂性降低到$\Omicron(V)$，提供了实质性的收益，尽管具体的收益取决于模型作用的图的拓扑结构。读者应该注意，我们的方法也可与其他方法相结合，以提高`GNN`的效率。例如，常见的软硬件协同设计技术（包括量化和剪枝）可与这项工作相结合，这项工作提出了一种通过改进底层架构设计来提高模型效率的正交方法。我们还注意到，我们的方法可以与图采样技术相结合，以进一步提高在具有数百万或数十亿节点的图上进行训练时的可伸缩性。



## 贡献

- 我们提出了一种新的`GNN`架构，高效图卷积`EGC`，并为其提供了空间和谱解释。
- 我们在6个大型图数据集上对我们的架构进行了严格的评估，涵盖了转换和归纳用例，并证明了`EGC`始终比强基线取得更好的结果。
- 我们提供了几项消融研究，以激发我们模型中超参数的选择。
- 我们证明，与竞争方法相比，我们的模型同时实现了更好的参数效率、延迟和内存消耗。[我们实验的代码和预训练模型](https://github.com/shyam196/egc). 在出版时，`EGC`被贡献至了`PyTorch Geometric`库。



# 背景

## 深度学习的软硬件协同设计

引言中已经描述了几种常用的软硬件协同设计方法：量化、修剪和细致的架构设计都是`CNN`和`Transformers`的常见方法。除了能够从通用处理器（如`CPU`和`GPU`）获得更好的性能外，这些技术对于最大化专用加速器的回报也是必不可少的；虽然随着时间的推移，由于`CMOS`技术的改进，性能可能会提高，但进一步的改进在算法层面上没有创新。作为神经网络架构设计师，我们不能简单地依靠硬件的改进来使我们的建议在实际部署中可行。

## 图神经网络

许多`GNN`架构被视为`CNN`架构在非规则域的推广：在`CNN`中，每个节点的表示都是基于局部邻域，使用整图共享的参数来构建的。`GNN`不同，因为我们无法对邻域的大小或顺序进行假设。用于定义`GNN`的一个常见框架是消息传递神经网络`MPNN`范式。

假设图$G=(V,E)$ 有**节点特征**矩阵$\pmb{X} \in \mathbb{R}^{N\times F}$、**邻接**矩阵$\pmb{A} \in \mathbb{R}^{N\times N}$和可选的$D$-维**连边特征**矩阵$\pmb{E} \in \mathbb{R}^{E\times D}$。我们定义一个函数$\phi$，用于计算从节点$u$到节点$v$的消息，这是一个可微且具有排列不变性`permutation-invariant `(是指特征之间没有空间位置关系)的**聚合器$\oplus$**, 和一个**更新函数$\gamma$**来计算$l+1$层的表示
$$
\pmb{h}_{l+1}^{(i)} = \gamma(\pmb{h}_{l}^{(i)}, \oplus_{j \in N(i)}[\phi(\pmb{h}_{l}^{(i)}, \pmb{h}_{l}^{(j)}, \pmb{e}_{ij})])
$$

### `GNN`的相对表达能力

研究界的共识是，各向同性`GNN`比各向异性`GNN`表达能力差；从经验上看，这得到了基准的充分支持。`Brody`等人证明，`GAT`模型可以比各向同性模型更严格地表达。`Bronstein`等人还讨论了不同类别`GNN`层的相对表达能力，并认为卷积（也称为各向同性）模型非常适合利用输入图中的同态的问题。他们进一步认为，注意力或全消息传递模型适合处理异嗜性问题，但他们承认这些架构的资源消耗和可训练性可能会令人望而却步，尤其是在全消息传递的情况下。

### 扩展和部署`GNN`

虽然`GNN`在一系列领域取得了成功，但在扩展和部署方面仍存在挑战。图采样是对内存存储不下的大型图或模型进行缩放训练的一种方法。不是在整图上训练，而是在采样子图上运行每个迭代；采样方法根据是按节点、按层或按子图进行采样会有所不同。

另外，`GNN`分布式训练系统已经被提出，以将训练扩展到单个加速器的极限之外。一些工作提出了设计用于适应缩放的架构：图像增强`MLP`，例如`SIGN`被明确设计为浅层架构，因为所有图像操作都是作为预处理步骤完成的。

其他工作包括应用神经架构搜索`NAS`来安排现有`GNN`层，或为`GNN`构建量化技术。最后，最近的一项工作表明，对`GNN`使用内存高效的可逆残差使我们能够训练比以前更深更大的`GNN`模型，从而提高最先进的精度。

### 为什么现有方法不够

值得注意的是，这些方法中有许多都有很大的局限性，我们希望用我们的工作来解决。当应用于许多涉及模型泛化到看不见图的问题时，采样方法通常无效——这是`GNN`的常见用法。我们评估了各种采样方法，并观察到即使是对内存或延迟几乎没有好处的适度采样水平，也会导致模型性能显著下降。此外，这些方法不会加速底层`GNN`，因此它们可能不会对推断延迟提供任何总体好处。也没有证据表明我们知道，当推广到看不见的图时，图增强的`MLP`能够充分发挥作用；事实上，它们在理论上不如标准`GNN`表达。我们还调查了这一设置，发现这些方法与最先进的方法相比，没有提供具有竞争力的准确性。附录`B`提供了实验细节和结果，以及对现有工作局限性的进一步讨论。

总之，我们在高效`GNN`架构设计方面的工作引起了社区的兴趣，原因有两个：

- 它提出了关于常见假设的问题，以及我们如何设计和评估`GNN`模型；我们的工作可能使我们能进一步扩展模型，从而提高准确性。
- 此外，对于需要泛化到看不见的图形的任务，如代码分析或点云处理，我们减少了内存消耗和延迟，从而使我们能够将模型部署到比以前更加资源受限的设备上。我们注意到，有效的架构设计可以有效地与其他方法相结合，包括采样、量化和剪枝。



# 我们的架构

在本节中，我们描述了我们的方法，并将理论分析推迟到下一节。我们提出了两个版本：

- 使用**单个聚合器**的`EGC-S`(S for Single)
- 合并**多个聚合器**的`EGC-M`(M for Multi)

我们的方法如图`2`所示:

![image-20220718161115255](../../../../img/gnn/egcConv_paper/EGC-S.png){: width="1086" height="542"}图2: `EGC-S`层的可视化表示。在该可视化中，我们有$3$个基本过滤器（即$B=3$)，它们使用每个节点权重$w$进行组合。



## 架构描述

对于某一层，使用$B$个`basis weight` $\pmb{\Theta}_{b} \in \mathbb{R}^{F^{'} \times F}$，其中$F$表示输入维度和$F^{'}$表示输出维度。

通过计算每个节点的组合权重系数$\pmb{w}^{(i)} \in \mathbb{R}^{B}$来计算节点 $i$ 的输出，并使用不同的`basis weight` $\pmb{\Theta}_{b}$ 来对每个聚合的结果加权。

节点 $i$ 的输出通过3个步骤来计算：

- we perform the aggregation with each set of basis weights $\pmb{\Theta}_{b}$

- 计算每个节点 $i$ 的权重系数$\pmb{w}^{(i)}=\pmb{\Phi}\pmb{x}^{(i)} + \pmb{b} \in \mathbb{R}^{B}$,  $\pmb{\Phi} \in \mathbb{R}^{B \times F}$ 和 $\pmb{b} \in \mathbb{R}^{B}$分别是用于计算组合权重系数的权重和偏置。

- 节点 $i$ 的层输出是聚合输出的加权组合：
  $$
  \pmb{y}^{(i)} = \sum_{b=1}^{B} w_{b}^{(i)} \sum_{j \in N(i)} \alpha(i,j) \pmb{\Theta}_{b} \pmb{x}^{(j)}
  $$
  其中，$\alpha(i,j)$是节点 $i$ 和 $j$ 的某个函数，$N(i)$表示节点 $i$ 的输入邻居。

`GAT`开创的一种提高表征能力的方法是使用两个节点的学习函数来表示$\alpha$。虽然这可以实现邻居的各向异性处理，并提高性能，但由于需要显式物化消息，必然会导致$\Omicron(E)$的内存消耗，并使加速器的硬件实现复杂化。如果我们为$\alpha$选择的表示不是节点的函数，例如$\alpha=1$来复现`GIN`使用的加法聚合器或$\alpha=\sqrt{deg(i)deg(j)}$来复现`GCN`使用的对称归一化，那么我们可以使用稀疏矩阵乘法`SpMM`来实现消息传播阶段，并避免显式物化每个消息，即使是对于向后传递。在这项工作中，除非另有说明，否则我们假设$\alpha(i,j)$是`GCN`使用的对称归一化；我们使用这种规范化，因为它可以在各种任务中提供强大的结果；第`4.2`节提供了更正式的理由。

### 添加`head`作为正则化项

我们可以通过添加`head`来扩展我们的层，如在`GAT`或`Transformer`等架构中使用的。这些`head`共享基本权重，但每个`head`应用不同的加权系数。我们发现，当`head`数 $H$ 大于 $B$ 时，添加此自由度有助于正则化，因为不鼓励基专门化（见第`5.3`节），而无需将额外的损失项集成到优化中，因此不需要更改下游用户的代码。为了规范化输出维度，我们将基权重矩阵维度更改为$\frac{F^{'}}{H} \times F$。使用$\|$作为级联算子，并显式使用对称规范化，我们获得了`EGC-S`层：
$$
\pmb{y}^{(i)} =={\LARGE ||}_{h=1}^{H} \sum_{b=1}^{B} w_{h,b}^{(i)} \sum_{j \in N(i)\cup \{i\}} \frac{1}{\sqrt{deg(i)deg(j)}} \pmb{\Theta}_{b} \pmb{x}^{(j)}
$$
`EGC`通过组合基矩阵来工作。该思想在`R-GCN`中提出，用于处理多种边类型。在本工作中，我们正在解决与这些工作不同的问题：我们感兴趣的是设计高效的架构，而不是处理边信息的新方法。



## 提升表达能力

`Corso`等人最近的工作表明，仅使用一个聚合器是次优的，相反，最好将几个不同的聚合器组合在一起。在公式`3`中，我们对层仅使用对称归一化。为了提高性能，我们建议对$\pmb{\Theta}_{b} \pmb{x}^{(j)}$计算的表示应用不同的聚合器。聚合器的选择可以包括求和聚合器的不同变体，例如均值或未加权加法，与上一节中提出的对称归一化相反。或者，我们可以使用不基于总和的聚合器，例如`stddev`、`min`或`max`。也可以使用`Beaini`等人提出的定向聚合器，但这种增强与这项工作是正交的。如果我们有一组聚合器$\Alpha$，我们可以扩展公式`3`以获得`EGC-M`层：
$$
\pmb{y}^{(i)} ={\LARGE ||}_{h=1}^{H} \sum_{\oplus \in \Alpha} \sum_{b=1}^{B} w_{h,\oplus,b}^{{i}} \bigoplus_{j \in N(i)\cup \{i\}} \pmb{\Theta}_{b} \pmb{x}^{(j)}
$$
其中$\oplus$ 是一个聚合器。有了这个公式，我们将重用与之前计算的相同的消息，但我们将同时对其应用几个聚合函数。

### 聚合器融合

似乎添加更多聚合器会导致延迟和内存消耗呈线性增长。然而实际上并非如此。首先，由于稀疏操作在实践中通常是内存受限的，我们可以对已经从内存中到达的数据应用额外的聚合器，而延迟损失很小。`EGC`还可在推理时有效地内联节点加权操作，从而导致相对较小的内存消耗开销。等效优化更难成功应用于`PNA`，因为在所有结果串联和应用转换之前，每个节点在聚合期间必须执行更多的操作，这是由应用于每个聚合的缩放函数引起的。更多详细信息，包括评测和延迟测量，请参阅附录`D`。



# 解释和收益

本节将解释我们的设计选择，以及为什么它们更适合硬件。我们强调，我们的方法并不直接对应注意力。

## 空域解释：节点权重矩阵

在我们的方法中，每个节点具有自己的权重矩阵。我们可以通过将$\pmb{\Theta}_{b}$从内和项中分解出来重新排列公式`2`得出这一点：





### 与注意力机制关系

我们的方法与注意力机制没有直接关系，注意力依赖于成对相似机制，因此在使用常见公式时会产生$\Omicron(E)$成本。吴等人提出的基于注意力的`Transformers`的替代方案与我们的技术更接近，但依赖于每个时间步长权重矩阵的显式预测。这种方法对于图是不可行的，因为邻域大小不是恒定的。



## 谱域解释：局部谱滤波


$$
h_i^{(l+1)} = {\LARGE ||}_{h=1}^{H} \sum_{\oplus \in \mathcal{A}} \sum_{b=1}^{B} w_{h,\oplus,b}^{(l)} \bigoplus_{j \in \mathcal{N(i)}} W_{b}^{(l)} h_{j}^{(l)}
$$
