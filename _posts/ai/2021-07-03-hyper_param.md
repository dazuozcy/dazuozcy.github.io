---
layout: post
title: "超参"
author: dazuo
date: 2020-07-02 20:19:00 +0800
categories: [AI]
tags: [深度学习]
math: true
mermaid: true
---

> 深度学习的优化算法，说白了就是梯度下降。每次的参数更新有两种方式：
>
> - 遍历全部数据集计算一次损失函数，然后算函数对各个参数的梯度，更新梯度。
>
>   这种方法每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，计算速度慢，不支持在线学习，这称为Batch gradient descent，批梯度下降。
>
> - 每看一个数据就算一下损失函数，然后求梯度更新参数，这个称为随机梯度下降，stochastic gradient descent。
>
>   这个方法速度比较快，但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。
>
> 为了克服两种方法的缺点，现在一般采用的是一种折中手段，mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。
>
> 基本上现在的梯度下降都是基于mini-batch的。

# batch size





# epoch数目

epoch数目直接影响模型是欠拟合还是过拟合。

- epoch过小，模型未训练到最优解就停止了训练，容易欠拟合；
- epoch过大，模型训练时间过长，容易在训练集上过拟合，在测试集上达不到最优效果。

应根据训练过程中验证集上模型效果的变化情况，合理选择epoch数目。



# warm-up

## 什么是warm-up

Warmup是一种学习率预热的方法。是指在训练的一开始选择一个较小的学习率，训练一定的epoch或者step后，再将学习率改为预设值来进行训练。

## 为什么使用warm-up

由于刚开始训练时，模型的权重是随机初始化的，此时若选择较大的学习率，可能带来模型的不稳定（震荡）。选择warmup预热学习率的方式，可以使得开始训练的几个epoch或者step内学习率较小，在小学习率下，模型可以慢慢趋于稳定，等模型相对稳定后，再选择预先设置的学习率进行训练，使得模型收敛速度变得更快，模型效果更佳。

