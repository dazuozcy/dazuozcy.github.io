---
layout: post
title: "TVM"
author: dazuo
date: 2023-05-21 20:19:00 +0800
categories: [TVM]
tags: [TVM]
math: true
mermaid: true
---

# `TVM`工作流程

![image](https://img2022.cnblogs.com/blog/1059417/202207/1059417-20220716143918319-1023005685.png)

## 1、导入已有模型

从`Tensorflow`/`pytorch`或`ONNX`等框架导入模型。

```python
# 例，导入ONNX模型
onnx_model = onnx.load(model_path) 
```

## 2、转换到`Relay`

```python
shape_dict = {input_name: img_data.shape}
mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)
```

`Relay`是`TVM`的高级模型语言，导入到`TVM`的模型是用`Relay`表示的。Relay是一种函数式`IR`。

`Relay`应用图级(`graph-level`)优化`pass`来优化模型。

## 3、Lower to TE

`lower`是指较高层表示转换成较低层表示。在high-level经过优化后，Relay运行 `FuseOps`，将模型分割成许多小的子图，并将子图`lower` 到`TE`表示。

> TE 即Tensor Expression,张量表达，是描述张量计算的专属性语言

`TE`还提供了一些**`schedule`原语**来指定低级的循环优化，如`tiling`、`vectorization`、`parallelization`、`unrolling`和`fusion`。
为了帮助将`Relay`转换为`TE`，`TVM`包含张量算子清单`Tensor Operator Inventory`，`TOPI`，它有预先定义的常见张量算子的模板（如 `conv2d`、`transpose`）

## 4、自动调优

`TVM`中，一个计算任务`Workload`被分成两部分。

- 一是**计算表达式**(`Compute Expression`)

  它定义了一个算子的输入、输出和计算描述（如矩阵乘法）。

- 二是**`Schedule`**

  也就是如何完成这项计算任务，如何生成具体的低层代码。

比如一个矩阵乘法，实现的方式可以有很多种：简单循环、按行、列、分块合并等方式都可以完成任务。根据具体的硬件，可以抽象出各种实现参数：诸如如何进行`tiling`、`fusing`、并行度(`parallel`)、向量化(`vectorize`)等。

一个硬件上具体实现的代码最终由二者结合（计算表达式+Schedule）所定义。可能的Schedule参数组合可能有成千上万种，对于一个具体的硬件，哪一种实现是最好的呢？这就要用到自动调优了。

抽象而言，可以对某个具体的硬件进行特征提取，建立一个损失模型(`Cost Model`)，来引导优化过程，从各种可能的参数组合中尽可能选出最好的实现(Schedule)来。

`schedule`指定在`TE`中定义了算子或子图的低级循环优化。`auto-tuning`模块搜索最佳`schedule`并将其与cost model设备上的测量结果进行比较。`TVM`中，有两个`auto-tuning`模块：

- `AutoTVM`：基于模板的自动调优模块。 它运行搜索算法以在用户定义的模板中找到可调旋钮的最佳值。 `TOPI`中已经提供了普通算子的模板。
- `AutoScheduler`（又名`Ansor`）：一个无模板的自动调整模块。 它不需要预定义的计划模板。 相反，它通过分析计算表达式自动生成搜索空间。 然后它在生成的搜索空间中搜索最佳`Schedule`。



## 5、选出最佳schedule

`tuning`后，`auto-tuning`模块会生成`JSON`格式的`auto-tuning`b记录。这一步为每个子图挑选出最佳的`schedule`。

## 6、Lower to TIR

`TIR`是张量级的中间表示`Tensor Intermediate Representation`，`TVM`的低层次IR。

在tuning得到最佳配置后，每个TE子图被降低到`TIR`，并通过低级别的优化passes进行优化。

接下来，优化后的`TIR`被`lower`到硬件平台的目标编译器中。这是最后的代码生成阶段，产生可以部署到生产中的优化模型。

`TVM`支持几种不同的编译器后端，包括：

- `LLVM`，它可针对任意微处理器架构，包括标准`x86`和`ARM`、`AMD GPU`和`NVPTX`代码生成，以及`LLVM`支持的任何其他平台。
- 专门的编译器，例如`NVCC`，`NVIDIA`的编译器。
- 嵌入式和专用目标，通过`TVM`的`Bring Your Own Codegen`(`BYOC`) 框架实现。

最后的产出分为三部分：

- `Graph Json`是整个代码`Function`的调度顺序描述。
- `Complie Lib`是生成代码的`Function`库。
- `Param Dict`用于保存参数等常量。

## 7、Machine Code

最终的编译结果，由对应设备的`Runtime`运行，并获取到运行结果和性能信息。

`TVM`可以将模型编译为可链接的对象模块，然后可以使用轻量级`TVM`运行时运行，该运行时提供`C API`以动态加载模型，同时提供其他语言（如`Python`和`Rust`）的入口点。 `TVM`还可以构建捆绑部署，其中运行时与模型结合在一个包中。

