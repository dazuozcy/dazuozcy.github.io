---
layout: post
title: "读薄《Professional CUDA C Programming》"
author: dazuo
date: 2020-07-01 20:19:00 +0800
categories: [GPU]
tags: [GPU]
math: true
mermaid: true
---

# 一、基于CUDA的异构并行编程

## 并行计算

### 并行性

并行计算的软件和硬件层面是紧密联系的，传说中的**软硬件协同**。  并行计算通常涉及两个不同的计算技术领域：

- 计算机架构（硬件方面）
- 并行程序设计（软件方面）

在并行算法的实现中， 分析数据的相关性是最基本的内容， 因为**相关性**是限制并行性的一个主要因素。

在应用程序中有两种基本的并行类型：

- 任务并行。重点在于利用多核系统对任务进行分配。
- 数据并行。重点在于利用多核系统对数据进行分配。

数据并行程序设计的第一步是依据**线程**划分数据， 以使每个线程处理一部分数据。有两种方法可以对数据进行划分：

- 块划分：一组连续的数据被分到一个块内。 每个数据块以任意次序被安排给一个线程， 线程通常在同一时间只处理一个数据块
- 周期划分：更少的数据被分到一个块内。 相邻的线程处理相邻的数据块， 每个线程可以处理多个数据块。
  为一个待处理的线程选择一个新的块， 就意味着要跳过和现有线程一样多的数据块。

![image-20210830195708399](/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/data_partitions.png)

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/data_partitions2.png" alt="image-20210830200523656" title=" style=&quot;zoom:60%;" style="zoom:78%;" />

> 程序性能通常对块的大小比较敏感。 块划分与周期划分中划分方式的选择与计算机架构有密切关系。 

### 计算机架构

弗林分类法`(Flynn’s Taxonomy)` 根据指令和数据进入CPU的方式，将计算机架构分为4种不同的类型：

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/flynn_taxonomy.png" style="zoom:80%;" />



相关衡量指标：

* 延迟，一个操作从开始到完成所需要的时间，单位常用`us`表示。
* 带宽，单位时间内可处理的数据量， 通常表示为`MB/s`或`GB/s`。
* 吞吐量，单位时间内成功处理的运算数量， 通常表示为`gflops`（即每秒十亿次浮点运算）

根据内存组织形式，计算机架构可分为两种类型：

* 分布式内存的多节点系统

  如下图所示，每个处理器有自己的本地内存， 处理器之间可以通过网络进行通信。也被称为**集群**。

  <img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/cluster.png" alt="image-20210831143115327" style="zoom:80%;" />

* 共享内存的多处理器系统

  包含的处理器数量通常从两个到几十个或几百个处理器之间。 这些处理器要么是与同一个物理内存相关联（如下图所示） ， 要么共用一个低延迟的链路（比如PCIe）。

  <img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/multi-cores-system.png" alt="image-20210831143503074" style="zoom:80%;" />

**众核**`（many-core）` 通常是指有很多核心（几十或几百个） 的多核架构。`GPU`代表了一种众核架构， 几乎包括了前文描述的所有并行结构： **多线程**、**MIMD**（多指令多数据） 、 **SIMD**（单指令多数据） ， 以及**指令级并行**。 NVIDIA称这种架构为**SIMT**（单指令多线程）。



**CPU core VS GPU core**

- CPU核心较重， 用来处理非常复杂的控制逻辑， 以优化串行程序执行。
- GPU核心较轻， 用于优化具有简单控制逻辑的数据并行任务， 注重并行程序的吞吐量。



## 异构计算

CPU和GPU是两种独立的处理器， 它们通过单个计算节点中的PCI-Express总线相连。

尽管异构系统比传统的高性能计算系统有更大的优势， 但目前对这种系统的有效利用受限于应用程序设计复杂度的增加。

### 异构架构

GPU不是一个独立运行的平台而是CPU的协处理器。 因此， GPU必须通过PCIe总线与基于CPU的主机相连来进行操作。

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/cpu_gpu.png" style="zoom:70%;" />

一个异构应用包括两个部分：主机代码、设备代码。主机代码在CPU上运行， 设备代码在GPU上运行。 异构平台上执行的应用通常由CPU初始化。 在设备端加载计算密集型任务之前， CPU代码负责管理设备端的环境、 代码和数据。

因为CPU和GPU的功能互补性导致了CPU＋GPU的异构并行计算架构的发展， 这两种处理器的类型能使应用程序获得最佳的运行效果。 因此， 为获得最佳性能， 你可以同时使用CPU和GPU来执行你的应用程序， 在CPU上执行串行部分或任务并行部分， 在GPU上执行数据密集型并行部分。

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/cpu+gpu.png" style="zoom:75%;" />



**CPU 线程 VS. GPU线程**

Threads on a CPU are generally heavyweight entities. The operating system must swap threads on and off CPU execution channels to provide multithreading capability. Context switches are slow and expensive.
Threads on GPUs are extremely lightweight. In a typical system, thousands of threads are queued up for work. If the GPU must wait on one group of threads, it simply begins executing work on another.
CPU cores are designed to minimize latency for one or two threads at a time, whereas GPU cores are designed to handle a large number of concurrent, lightweight threads in order to maximize throughput.
Today, a CPU with four quad core processors can run only 16 threads concurrently, or 32 if the CPUs support hyper-threading.
Modern NVIDIA GPUs can support up to 1,536 active threads concurrently per multiprocessor. On GPUs with 16 multiprocessors, this leads to more than 24,000 concurrently active threads.

## CUDA C编程

一个典型的CUDA编程结构包括5个主要步骤：

- 分配GPU内存。
- 从CPU内存中拷贝数据到GPU内存。
- 调用CUDA内核函数来完成程序指定的运算。
- 将数据从GPU拷回CPU内存。
- 释放GPU内存空间。

```cpp
#include <stdio.h>

__global__ void helloFromGPU(void)
{
    // 通过内置变量threadIdx.x来实现只让thread 5调用printf
	if (threadIdx.x == 5) {
		printf("Hello World from GPU thread %d!\n", threadIdx.x);
	}
}

int main(void)
{
	printf("Hello World from CPU!\n");
	helloFromGPU<<<1, 10>>>();
	cudaDeviceReset();
	return 0;
}
```



> 修饰符`__global__`告诉编译器这个函数将会从CPU中调用， 然后在GPU上执行。

函数`cudaDeviceRest()` 用来显式地释放和清空当前进程中与当前设备有关的所有资源。 

GPU上调用`printf`, `cudaDeviceReset` 会强制将`printf`的内容从GPU flush 到host侧，然后在用户命令行里显示出来。如果不调用`cudaDeviceReset`或其他可以强制flush GPU输出的接口，比如`cudaDeviceSynchronize`，`printf`的内容不保证会显示出来。

`cudaDeviceSynchronize`会阻塞主机端线程的运行直到设备端所有的请求任务都结束。



### 数据局部性

数据局部性指的是数据重用， 以降低内存访问的延迟。 数据局部性有两种基本类型。 

- **时间局部性**是指在相对较短的时间段内数据和/或资源的重用。
-  **空间局部性**是指在相对较接近的存储空间内数据元素的重用。 





# 二、CUDA编程模型

## 概述

### CUDA编程结构

站在程序员的角度，可以从以下几个不同层面来看待并行计算：

- 领域层(Domain level)
- 逻辑层(Logic level)
- 硬件层(Hardware level)

这三个层面对应了并行计算编程的不同阶段：

- 算法设计阶段， 最关心的应是在**领域层**如何解析数据和函数， 以便在并行环境中能正确、 高效地解决问题。 
- 编码阶段， 关注点应转向如何组织并发线程。 在这个阶段， 需要从**逻辑层面**来思考， 以确保线程和计算能正确地解决问题。 在C语言并行编程中， 需要使用`pthreads`或`OpenMP`技术来显式地管理线程。 CUDA提出了一个线程层次结构抽象的概念， 以允许控制线程行为。 
- 在**硬件层**， 通过理解线程是如何映射到核心可以帮助提高其性能。



CUDA编程模型主要是异步的， 因此在GPU上进行的运算可以与主机-设备通信重叠。 内核一旦被启动， 管理权立刻返回给主机， 释放CPU来执行由设备上运行的并行代码实现的额外的任务。 

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap02/cuda_exe_flow.png" alt="image-20210902165645073" style="zoom:80%;"/>



### 内存管理

**CUDA运行时**负责分配与释放设备内存， 并且在主机内存和设备内存之间传输数据。

| 标准C函数 | CUDA C函数 |
| :------- | :--------- |
|  malloc   | cudaMalloc |
|   free    |   cudaFree |
|  memset   | cudaMemset |
|  memcpy   | cudaMemcpy |

cudaMemcpy的调用会导致主机运行阻塞。 

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap02/gpu_memory_structure.png" alt="image-20210902194627854" style="zoom:80%;" />

上图是一个简化的GPU内存结构， 它主要包含两部分： 全局内存和共享内存。全局内存类似于CPU的系统内存， 共享内存类似于CPU的缓存。 不过GPU的共享内存可以由CUDA C的kernel直接控制。



### 线程管理

当kernel函数在host侧启动时， 它的执行会移动到device侧， 此时设备中会产生大量的线程并且每个线程都执行由kernel函数指定的语句。

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap02/two-level-thread-hierarchy.png" alt="image-20210902200436676" style="zoom:80%;" />

一个kernel函数启动后产生的所有线程统称为一个Grid。 同一Grid的所有线程共享全局内存空间。 一个Grid由多个thread blocks构成， 一个thread block包含一组线程，同一线程块内的线程协作可以通过2种方式来实现：Block-local synchronization和Block-local shared memory。（不同block内的线程不能协作）



#### 内置变量

- gridDim, grid dimension, measured in blocks
- blockDim, block dimension, measured in threads
- blockIdx, block index within a grid
- threadIdx, thread index within a block



### kernel函数

```cpp
void kernel_name <<<grid, block>>>(argument_list);
```

执行配置的第一个值是网格维度， 也就是启动块的数目。 第二个值是块维度， 也就是每个块中线程的数目。

下图所示就是`void kernel_name <<<4, 8>>>(argument_list);`配置下的线程布局。

![image-20210904163022038](/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap02/thread-orgnization.png)



#### CUDA核函数的限制

- 只能访问设备内存
- 必须具有void返回类型
- 不支持可变数量的参数
- 不支持静态变量
- 显式异步行为



#### 验证kernel函数

除了使用调试工具外， 还有两个非常简单实用的方法可以验证核函数。

- 在`Fermi`及更高版本的设备端的核函数中使用`printf`函数。
- 可以将执行参数设置为`<<<1, 1>>>`， 因此强制用`1`个块和`1`个线程执行核函数， 这模拟了串行执行。



# 三、CUDA执行模型

## CUDA执行模型概述

Streaming Multiprocessors (SM)，流式多处理器。GPU实际上是一个SM的阵列，每个SM包含N个计算核，能支持数百个线程并发执行。

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap03/gpu-sm.png" alt="image-20210905210234116" style="zoom:80%;" />

当启动一个kernel网格时， 它的线程块被分布在了可用的SM上来执行。 一个线程块只能在一个SM上被调度。 一旦线程块在一个SM上被调度， 就会保存在该
SM上直到执行完成。 在同一时间， 一个SM可以容纳多个线程块，即多个线程块可能会被分配到同一SM上， 而且是根据SM资源的可用性进行调度的。

下图说明了Fermi SM的关键组件，寄存器和共享内存是SM中的稀缺资源。

- CUDA核心
- 共享内存/一级缓存
- 寄存器文件
- 加载/存储单元
- 特殊功能单元
- 线程束调度器

![image-20210905202343234](/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap03/fermi-sm.png)



CUDA采用单指令多线程（SIMT） 架构来管理和执行线程， 每32个线程为一组， 被称为**线程束**(warp)。

线程束中的所有线程同时执行相同的指令。 每个线程都有自己的指令地址计数器和寄存器状态， 利用本线程自己的数据执行当前的指令。 



#### SIMT vs. SMID

两者都是将相同的指令广播给多个执行单元来实现并行。 一个关键的区别是SIMD要求同一向量中的所有元素要在一个统一的同步组中一起执行，而SIMT允许同一线程束的多个线程独立执行。尽管一个线程束中的所有线程从相同的程序地址同时开始执行， 但是单独的线程仍可能有不同的行为。 SIMT让你可以为独立的标量线程编写线程级并行代码，甚至为互相协作的线程编写数据并行的代码。
SIMT模型包含3个SIMD所不具备的关键特征：

- 每个线程有自己的指令地址计数器
- 每个线程有自己的寄存器状态
- 每个线程可以有一个独立的执行路径



## 线程束执行的本质

启动内核时， 内核中所有的线程似乎都是并行地运行的，在逻辑上这是正确的， 但从硬件的角度来看， 不是所有线程在物理上都可以同时并行地执行。

### 线程束和线程块

线程束是SM中基本的执行单元。 当一个Grid被启动后， Grid中的线程块分布在SM中。 一旦线程块被调度到一个SM上， 线程块中的线程会被进一步划分为线程束。一个线程束由32个连续的线程组成， 一个线程束中所有的线程按照SIMT方式执行，即所有线程都执行相同的指令， 每个线程在**私有数据**上进行操作。

如果线程块的大小不是线程束大小的偶数倍， 那么在最后的线程束里有些线程就不会活跃，即使这些线程未被使用， 但它们仍然消耗SM的资源， 如寄存器。

比如，某个线程块有80个线程，那么硬件会为这个线程块配置80/32=3个线程束，最后一个线程束的最后16个线程不会被使用。

### 线程束分化

一个线程束中的所有线程在同一周期中必须执行相同的指令， 如果一个线程执行一条指令， 那么线程束中的所有线程都必须执行该指令。 如果在同一线程束中的线程使用不同的路径通过同一个应用程序， 这可能会产生问题。一半的线程束需要执行if语句块中的指令， 而另一半需要执行else语句块中的指令。 在同一线程束中的线程执行不同的指令， 被称为**线程束分化**。

### 资源分配

线程束的执行上下文主要由以下资源组成：

- 程序计数器
- 寄存器
- 共享内存

 由于计算资源是在线程束之间进行分配的， 而且在线程束的整个生存期中都保持在芯片内， 因此线程束上下文的切换是非常快的。

对于一个给定的内核， 同时存在于同一个SM中的线程块和线程束的数量取决于在SM中可用的且内核所需的寄存器和共享内存的数量。

资源可用性通常会限制SM中常驻线程块的数量。如果每个SM没有足够的寄存器或共享内存去处理至少一个块， 那么内核将无法启动。

当计算资源已分配给线程块时， 线程块被称为**活跃的块**。 它所包含的线程束被称为**活跃的线程束**。 活跃的线程束可以进一步被分为以下3种类型：

- 选定的线程束：活跃执行的线程束
- 阻塞的线程束：活跃的准备执行但尚未执行的线程束
- 符合条件的线程束：没有做好执行准备的线程束

如果同时满足以下两个条件则线程束符合执行条件：

- 32个CUDA核心可用于执行
- 当前指令中所有的参数都已就绪

计算资源限制了活跃的线程束的数量。 为了最大程度地利用GPU， 需要最大化活跃的线程束数量。

### 延迟隐藏

