---
layout: post
title: "读薄《Professional CUDA C Programming》"
author: dazuo
date: 2020-07-01 20:19:00 +0800
categories: [GPU]
tags: [GPU]
math: true
mermaid: true
---

# 一、基于CUDA的异构并行编程

## 并行计算

### 并行性

并行计算的软件和硬件层面是紧密联系的，传说中的**软硬件协同**。  并行计算通常涉及两个不同的计算技术领域：

- 计算机架构（硬件方面）
- 并行程序设计（软件方面）

在并行算法的实现中， 分析数据的相关性是最基本的内容， 因为**相关性**是限制并行性的一个主要因素。

在应用程序中有两种基本的并行类型：

- 任务并行。重点在于利用多核系统对任务进行分配。
- 数据并行。重点在于利用多核系统对数据进行分配。

数据并行程序设计的第一步是依据**线程**划分数据， 以使每个线程处理一部分数据。有两种方法可以对数据进行划分：

- 块划分：一组连续的数据被分到一个块内。 每个数据块以任意次序被安排给一个线程， 线程通常在同一时间只处理一个数据块
- 周期划分：更少的数据被分到一个块内。 相邻的线程处理相邻的数据块， 每个线程可以处理多个数据块。
  为一个待处理的线程选择一个新的块， 就意味着要跳过和现有线程一样多的数据块。

![image-20210830195708399](/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/data_partitions.png)

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/data_partitions2.png" alt="image-20210830200523656" title=" style=&quot;zoom:60%;" style="zoom:78%;" />

> 程序性能通常对块的大小比较敏感。 块划分与周期划分中划分方式的选择与计算机架构有密切关系。 

### 计算机架构

弗林分类法`(Flynn’s Taxonomy)` 根据指令和数据进入CPU的方式，将计算机架构分为4种不同的类型：

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/flynn_taxonomy.png" style="zoom:80%;" />



相关衡量指标：

* 延迟，一个操作从开始到完成所需要的时间，单位常用`us`表示。
* 带宽，单位时间内可处理的数据量， 通常表示为`MB/s`或`GB/s`。
* 吞吐量，单位时间内成功处理的运算数量， 通常表示为`gflops`（即每秒十亿次浮点运算）

根据内存组织形式，计算机架构可分为两种类型：

* 分布式内存的多节点系统

  如下图所示，每个处理器有自己的本地内存， 处理器之间可以通过网络进行通信。也被称为**集群**。

  <img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/cluster.png" alt="image-20210831143115327" style="zoom:80%;" />

* 共享内存的多处理器系统

  包含的处理器数量通常从两个到几十个或几百个处理器之间。 这些处理器要么是与同一个物理内存相关联（如下图所示） ， 要么共用一个低延迟的链路（比如PCIe）。

  <img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/multi-cores-system.png" alt="image-20210831143503074" style="zoom:80%;" />

**众核**`（many-core）` 通常是指有很多核心（几十或几百个） 的多核架构。`GPU`代表了一种众核架构， 几乎包括了前文描述的所有并行结构： **多线程**、**MIMD**（多指令多数据） 、 **SIMD**（单指令多数据） ， 以及**指令级并行**。 NVIDIA称这种架构为**SIMT**（单指令多线程）。



**CPU core VS GPU core**

- CPU核心较重， 用来处理非常复杂的控制逻辑， 以优化串行程序执行。
- GPU核心较轻， 用于优化具有简单控制逻辑的数据并行任务， 注重并行程序的吞吐量。



## 异构计算

CPU和GPU是两种独立的处理器， 它们通过单个计算节点中的PCI-Express总线相连。

尽管异构系统比传统的高性能计算系统有更大的优势， 但目前对这种系统的有效利用受限于应用程序设计复杂度的增加。

### 异构架构

GPU不是一个独立运行的平台而是CPU的协处理器。 因此， GPU必须通过PCIe总线与基于CPU的主机相连来进行操作。

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/cpu_gpu.png" style="zoom:70%;" />

一个异构应用包括两个部分：主机代码、设备代码。主机代码在CPU上运行， 设备代码在GPU上运行。 异构平台上执行的应用通常由CPU初始化。 在设备端加载计算密集型任务之前， CPU代码负责管理设备端的环境、 代码和数据。

因为CPU和GPU的功能互补性导致了CPU＋GPU的异构并行计算架构的发展， 这两种处理器的类型能使应用程序获得最佳的运行效果。 因此， 为获得最佳性能， 你可以同时使用CPU和GPU来执行你的应用程序， 在CPU上执行串行部分或任务并行部分， 在GPU上执行数据密集型并行部分。

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/cpu+gpu.png" style="zoom:75%;" />



**CPU 线程 VS. GPU线程**

Threads on a CPU are generally heavyweight entities. The operating system must swap threads on and off CPU execution channels to provide multithreading capability. Context switches are slow and expensive.
Threads on GPUs are extremely lightweight. In a typical system, thousands of threads are queued up for work. If the GPU must wait on one group of threads, it simply begins executing work on another.
CPU cores are designed to minimize latency for one or two threads at a time, whereas GPU cores are designed to handle a large number of concurrent, lightweight threads in order to maximize throughput.
Today, a CPU with four quad core processors can run only 16 threads concurrently, or 32 if the CPUs support hyper-threading.
Modern NVIDIA GPUs can support up to 1,536 active threads concurrently per multiprocessor. On GPUs with 16 multiprocessors, this leads to more than 24,000 concurrently active threads.

## CUDA C编程

一个典型的CUDA编程结构包括5个主要步骤：

- 分配GPU内存。
- 从CPU内存中拷贝数据到GPU内存。
- 调用CUDA内核函数来完成程序指定的运算。
- 将数据从GPU拷回CPU内存。
- 释放GPU内存空间。



```cpp
#include <stdio.h>

__global__ void helloFromGPU(void)
{
    // 通过内置变量threadIdx.x来实现只让thread 5调用printf
	if (threadIdx.x == 5) {
		printf("Hello World from GPU thread %d!\n", threadIdx.x);
	}
}

int main(void)
{
	printf("Hello World from CPU!\n");
	helloFromGPU<<<1, 10>>>();
	cudaDeviceReset();
	return 0;
}
```



> 修饰符`__global__`告诉编译器这个函数将会从CPU中调用， 然后在GPU上执行。

函数`cudaDeviceRest()` 用来显式地释放和清空当前进程中与当前设备有关的所有资源。 

GPU上调用`printf`, `cudaDeviceReset` 会强制将`printf`的内容从GPU flush 到host侧，然后在用户命令行里显示出来。如果不调用`cudaDeviceReset`或其他可以强制flush GPU输出的接口，比如`cudaDeviceSynchronize`，`printf`的内容不保证会显示出来。



### 数据局部性

数据局部性指的是数据重用， 以降低内存访问的延迟。 数据局部性有两种基本类型。 

- **时间局部性**是指在相对较短的时间段内数据和/或资源的重用。
-  **空间局部性**是指在相对较接近的存储空间内数据元素的重用。 





# 二、CUDA编程模型

## 概述

### CUDA编程结构

站在程序员的角度，可以从以下几个不同层面来看待并行计算：

- 领域层(Domain level)
- 逻辑层(Logic level)
- 硬件层(Hardware level)

这三个层面对应了并行计算编程的不同阶段：

- 算法设计阶段， 最关心的应是在**领域层**如何解析数据和函数， 以便在并行环境中能正确、 高效地解决问题。 
- 编码阶段， 关注点应转向如何组织并发线程。 在这个阶段， 需要从**逻辑层面**来思考， 以确保线程和计算能正确地解决问题。 在C语言并行编程中， 需要使用`pthreads`或`OpenMP`技术来显式地管理线程。 CUDA提出了一个线程层次结构抽象的概念， 以允许控制线程行为。 
- 在**硬件层**， 通过理解线程是如何映射到核心可以帮助提高其性能。



CUDA编程模型主要是异步的， 因此在GPU上进行的运算可以与主机-设备通信重叠。 内核一旦被启动， 管理权立刻返回给主机， 释放CPU来执行由设备上运行的并行代码实现的额外的任务。 

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap02/cuda_exe_flow.png" alt="image-20210902165645073" style="zoom:80%;"/>



### 内存管理

**CUDA运行时**负责分配与释放设备内存， 并且在主机内存和设备内存之间传输数据。

| 标准C函数 | CUDA C函数 |
| :------- | :--------- |
|  malloc   | cudaMalloc |
|   free    |   cudaFree |
|  memset   | cudaMemset |
|  memcpy   | cudaMemcpy |

cudaMemcpy的调用会导致主机运行阻塞。 

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap02/gpu_memory_structure.png" alt="image-20210902194627854" style="zoom:80%;" />

上图是一个简化的GPU内存结构， 它主要包含两部分： 全局内存和共享内存。全局内存类似于CPU的系统内存， 共享内存类似于CPU的缓存。 不过GPU的共享内存可以由CUDA C的kernel直接控制。



### 线程管理

当kernel函数在host侧启动时， 它的执行会移动到device侧， 此时设备中会产生大量的线程并且每个线程都执行由kernel函数指定的语句。

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap02/two-level-thread-hierarchy.png" alt="image-20210902200436676" style="zoom:80%;" />

一个kernel函数启动后产生的所有线程统称为一个Grid。 同一Grid的所有线程共享全局内存空间。 一个Grid由多个thread blocks构成， 一个thread block包含一组线程，同一线程块内的线程协作可以通过2种方式来实现：Block-local synchronization和Block-local shared memory。（不同block内的线程不能协作）



#### 内置变量

- gridDim, grid dimension, measured in blocks
- blockDim, block dimension, measured in threads
- blockIdx, block index within a grid
- threadIdx, thread index within a block

