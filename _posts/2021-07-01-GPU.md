---
layout: post
title: "读薄《Professional CUDA C Programming》"
author: dazuo
date: 2020-07-01 20:19:00 +0800
categories: [GPU]
tags: [GPU]
math: true
mermaid: true
---

# 一、基于CUDA的异构并行编程

## 并行计算

### 并行性

并行计算的软件和硬件层面是紧密联系的，传说中的**软硬件协同**。并行计算通常涉及两个不同的计算技术领域：

- 计算机架构（硬件方面）
- 并行程序设计（软件方面）

在并行算法的实现中， 分析数据的相关性是最基本的内容， 因为**相关性**是限制并行性的一个主要因素。

在应用程序中有两种基本的并行类型：

- **任务并行**。重点在于利用多核系统对任务进行分配。
- **数据并行**。重点在于利用多核系统对数据进行分配。

数据并行程序设计的第一步是依据**线程**划分数据， 以使每个线程处理一部分数据。有两种方法可以对数据进行划分：

- 块划分：一组连续的数据被分到一个块内。 每个数据块以任意次序被安排给一个线程， 线程通常在同一时间只处理一个数据块
- 周期划分：更少的数据被分到一个块内。 相邻的线程处理相邻的数据块， 每个线程可以处理多个数据块。
  为一个待处理的线程选择一个新的块， 就意味着要跳过和现有线程一样多的数据块。

![image-20210830195708399](/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/data_partitions.png)

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/data_partitions2.png" alt="image-20210830200523656" title=" style=&quot;zoom:60%;" style="zoom:78%;" />

> 程序性能通常对块的大小比较敏感。 块划分与周期划分中划分方式的选择与计算机架构有密切关系。 

### 计算机架构

弗林分类法`(Flynn’s Taxonomy)` 根据指令和数据进入CPU的方式，将计算机架构分为4种不同的类型：

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/flynn_taxonomy.png" style="zoom:80%;" />



相关衡量指标：

* 延迟，一个操作从开始到完成所需要的时间，单位常用`us`表示。
* 带宽，单位时间内可处理的数据量， 通常表示为`MB/s`或`GB/s`。
* 吞吐量，单位时间内成功处理的运算数量， 通常表示为`gflops`（即每秒十亿次浮点运算）

根据内存组织形式，计算机架构可分为两种类型：

* 分布式内存的多节点系统

  如下图所示，每个处理器有自己的本地内存， 处理器之间可以通过网络进行通信。也被称为**集群**。

  <img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/cluster.png" alt="image-20210831143115327" style="zoom:80%;" />

* 共享内存的多处理器系统

  包含的处理器数量通常从两个到几十个或几百个处理器之间。 这些处理器要么是与同一个物理内存相关联（如下图所示） ， 要么共用一个低延迟的链路（比如PCIe）。

  <img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/multi-cores-system.png" alt="image-20210831143503074" style="zoom:80%;" />

**众核**`（many-core）` 通常是指有很多核心（几十或几百个） 的多核架构。`GPU`代表了一种众核架构， 几乎包括了前文描述的所有并行结构： **多线程**、**MIMD**（多指令多数据） 、 **SIMD**（单指令多数据） ， 以及**指令级并行**。 NVIDIA称这种架构为**SIMT**（单指令多线程）。



**CPU core VS GPU core**

- CPU核心较重， 用来处理非常复杂的控制逻辑， 以优化串行程序执行。
- GPU核心较轻， 用于优化具有简单控制逻辑的数据并行任务， 注重并行程序的吞吐量。



## 异构计算

CPU和GPU是两种独立的处理器， 它们通过单个计算节点中的PCI-Express总线相连。

尽管异构系统比传统的高性能计算系统有更大的优势， 但目前对这种系统的有效利用受限于应用程序设计复杂度的增加。

### 异构架构

GPU不是一个独立运行的平台而是CPU的协处理器。 因此， GPU必须通过PCIe总线与基于CPU的主机相连来进行操作。

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/cpu_gpu.png" style="zoom:70%;" />

一个异构应用包括两个部分：主机代码、设备代码。主机代码在CPU上运行， 设备代码在GPU上运行。 异构平台上执行的应用通常由CPU初始化。 在设备端加载计算密集型任务之前， CPU代码负责管理设备端的环境、 代码和数据。

因为CPU和GPU的功能互补性导致了CPU＋GPU的异构并行计算架构的发展， 这两种处理器的类型能使应用程序获得最佳的运行效果。 因此， 为获得最佳性能， 你可以同时使用CPU和GPU来执行你的应用程序， 在CPU上执行串行部分或任务并行部分， 在GPU上执行数据密集型并行部分。

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap01/cpu+gpu.png" style="zoom:75%;" />



**CPU 线程 VS. GPU线程**

Threads on a CPU are generally heavyweight entities. The operating system must swap threads on and off CPU execution channels to provide multithreading capability. Context switches are slow and expensive.
Threads on GPUs are extremely lightweight. In a typical system, thousands of threads are queued up for work. If the GPU must wait on one group of threads, it simply begins executing work on another.
CPU cores are designed to minimize latency for one or two threads at a time, whereas GPU cores are designed to handle a large number of concurrent, lightweight threads in order to maximize throughput.
Today, a CPU with four quad core processors can run only 16 threads concurrently, or 32 if the CPUs support hyper-threading.
Modern NVIDIA GPUs can support up to 1,536 active threads concurrently per multiprocessor. On GPUs with 16 multiprocessors, this leads to more than 24,000 concurrently active threads.

## CUDA C编程

一个典型的CUDA编程结构包括5个主要步骤：

- 分配GPU内存。
- 从CPU内存中拷贝数据到GPU内存。
- 调用CUDA内核函数来完成程序指定的运算。
- 将数据从GPU拷回CPU内存。
- 释放GPU内存空间。

```cpp
#include <stdio.h>

__global__ void helloFromGPU(void)
{
    // 通过内置变量threadIdx.x来实现只让thread 5调用printf
	if (threadIdx.x == 5) {
		printf("Hello World from GPU thread %d!\n", threadIdx.x);
	}
}

int main(void)
{
	printf("Hello World from CPU!\n");
	helloFromGPU<<<1, 10>>>();
	cudaDeviceReset();
	return 0;
}
```

> 修饰符`__global__`告诉编译器这个函数将会从CPU中调用， 然后在GPU上执行。

函数`cudaDeviceRest()` 用来显式地释放和清空当前进程中与当前设备有关的所有资源。 

GPU上调用`printf`, `cudaDeviceReset` 会强制将`printf`的内容从GPU flush 到host侧，然后在用户命令行里显示出来。如果不调用`cudaDeviceReset`或其他可以强制flush GPU输出的接口，比如`cudaDeviceSynchronize`，`printf`的内容不保证会显示出来。

`cudaDeviceSynchronize`会阻塞主机端线程的运行直到设备端所有的请求任务都结束。



### 数据局部性

数据局部性指的是数据重用， 以降低内存访问的延迟。 数据局部性有两种基本类型。 

- **时间局部性**是指在相对较短的时间段内数据和/或资源的重用。
-  **空间局部性**是指在相对较接近的存储空间内数据元素的重用。 





# 二、CUDA编程模型

## 概述

### CUDA编程结构

站在程序员的角度，可以从以下几个不同层面来看待并行计算：

- 领域层(Domain level)
- 逻辑层(Logic level)
- 硬件层(Hardware level)

这三个层面对应了并行计算编程的不同阶段：

- 算法设计阶段， 最关心的应是在**领域层**如何解析数据和函数， 以便在并行环境中能正确、 高效地解决问题。 
- 编码阶段， 关注点应转向如何组织并发线程。 在这个阶段， 需要从**逻辑层面**来思考， 以确保线程和计算能正确地解决问题。 在C语言并行编程中， 需要使用`pthreads`或`OpenMP`技术来显式地管理线程。 CUDA提出了一个线程层次结构抽象的概念， 以允许控制线程行为。 
- 在**硬件层**， 通过理解线程是如何映射到核心可以帮助提高其性能。



CUDA编程模型主要是异步的， 因此在GPU上进行的运算可以与主机-设备通信重叠。 内核一旦被启动， 管理权立刻返回给主机， 释放CPU来执行由设备上运行的并行代码实现的额外的任务。 

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap02/cuda_exe_flow.png" alt="image-20210902165645073" style="zoom:80%;"/>



### 内存管理

**CUDA运行时**负责分配与释放设备内存， 并且在主机内存和设备内存之间传输数据。

| 标准C函数 | CUDA C函数 |
| :------- | :--------- |
|  malloc   | cudaMalloc |
|   free    |   cudaFree |
|  memset   | cudaMemset |
|  memcpy   | cudaMemcpy |

cudaMemcpy的调用会导致主机运行阻塞。 

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap02/gpu_memory_structure.png" alt="image-20210902194627854" style="zoom:80%;" />

上图是一个简化的GPU内存结构， 它主要包含两部分： 全局内存和共享内存。全局内存类似于CPU的系统内存， 共享内存类似于CPU的缓存。 不过GPU的共享内存可以由CUDA C的kernel直接控制。



### 线程管理

当kernel函数在host侧启动时， 它的执行会移动到device侧， 此时设备中会产生大量的线程并且每个线程都执行由kernel函数指定的语句。

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap02/two-level-thread-hierarchy.png" alt="image-20210902200436676" style="zoom:80%;" />

一个kernel函数启动后产生的所有线程统称为一个Grid。 同一Grid的所有线程共享全局内存空间。 一个Grid由多个thread blocks构成， 一个thread block包含一组线程，同一线程块内的线程协作可以通过2种方式来实现：Block-local synchronization和Block-local shared memory。（不同block内的线程不能协作）



#### 内置变量

- gridDim, grid dimension, measured in blocks
- blockDim, block dimension, measured in threads
- blockIdx, block index within a grid
- threadIdx, thread index within a block



### kernel函数

```cpp
void kernel_name <<<grid, block>>>(argument_list);
```

执行配置的第一个值是网格维度， 也就是启动块的数目。 第二个值是块维度， 也就是每个块中线程的数目。

下图所示就是`void kernel_name <<<4, 8>>>(argument_list);`配置下的线程布局。

![image-20210904163022038](/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap02/thread-orgnization.png)



#### CUDA核函数的限制

- 只能访问设备内存
- 必须具有void返回类型
- 不支持可变数量的参数
- 不支持静态变量
- 显式异步行为



#### 验证kernel函数

除了使用调试工具外， 还有两个非常简单实用的方法可以验证核函数。

- 在`Fermi`及更高版本的设备端的核函数中使用`printf`函数。
- 可以将执行参数设置为`<<<1, 1>>>`， 因此强制用`1`个块和`1`个线程执行核函数， 这模拟了串行执行。



# 三、CUDA执行模型

## CUDA执行模型概述

Streaming Multiprocessors (SM)，流式多处理器。GPU实际上是一个SM的阵列，每个SM包含N个计算核，能支持数百个线程并发执行。

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap03/gpu-sm.png" alt="image-20210905210234116" style="zoom:80%;" />

当启动一个kernel网格时， 它的线程块被分布在了可用的SM上来执行。 一个线程块只能在一个SM上被调度。 一旦线程块在一个SM上被调度， 就会保存在该
SM上直到执行完成。 在同一时间， 一个SM可以容纳多个线程块，即多个线程块可能会被分配到同一SM上， 而且是根据SM资源的可用性进行调度的。

下图说明了Fermi SM的关键组件，寄存器和共享内存是SM中的稀缺资源。

- CUDA核心
- 共享内存/一级缓存
- 寄存器文件
- 加载/存储单元
- 特殊功能单元
- 线程束调度器

![image-20210905202343234](/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap03/fermi-sm.png)



CUDA采用单指令多线程（SIMT） 架构来管理和执行线程， 每32个线程为一组， 被称为**线程束**(warp)。

线程束中的所有线程同时执行相同的指令。 每个线程都有自己的指令地址计数器和寄存器状态， 利用本线程自己的数据执行当前的指令。 



#### SIMT vs. SMID

两者都是将相同的指令广播给多个执行单元来实现并行。 一个关键的区别是SIMD要求同一向量中的所有元素要在一个统一的同步组中一起执行，而SIMT允许同一线程束的多个线程独立执行。尽管一个线程束中的所有线程从相同的程序地址同时开始执行， 但是单独的线程仍可能有不同的行为。 SIMT让你可以为独立的标量线程编写线程级并行代码，甚至为互相协作的线程编写数据并行的代码。
SIMT模型包含3个SIMD所不具备的关键特征：

- 每个线程有自己的指令地址计数器
- 每个线程有自己的寄存器状态
- 每个线程可以有一个独立的执行路径



## 线程束执行的本质

启动内核时， 内核中所有的线程似乎都是并行地运行的，在逻辑上这是正确的， 但从硬件的角度来看， 不是所有线程在物理上都可以同时并行地执行。

### 线程束和线程块

线程束是SM中基本的执行单元。 当一个Grid被启动后， Grid中的线程块分布在SM中。 一旦线程块被调度到一个SM上， 线程块中的线程会被进一步划分为线程束。一个线程束由32个连续的线程组成， 一个线程束中所有的线程按照SIMT方式执行，即所有线程都执行相同的指令， 每个线程在**私有数据**上进行操作。

如果线程块的大小不是线程束大小的偶数倍， 那么在最后的线程束里有些线程就不会活跃，即使这些线程未被使用， 但它们仍然消耗SM的资源， 如寄存器。

比如，某个线程块有80个线程，那么硬件会为这个线程块配置80/32=3个线程束，最后一个线程束的最后16个线程不会被使用。

### 线程束分化

一个线程束中的所有线程在同一周期中必须执行相同的指令， 如果一个线程执行一条指令， 那么线程束中的所有线程都必须执行该指令。 如果在同一线程束中的线程使用不同的路径通过同一个应用程序， 这可能会产生问题。一半的线程束需要执行if语句块中的指令， 而另一半需要执行else语句块中的指令。 

在同一线程束中的线程执行不同的指令， 被称为**线程束分化**。

### 资源分配

线程束的执行上下文主要由以下资源组成：

- 程序计数器
- 寄存器
- 共享内存

 由于计算资源是在线程束之间进行分配的， 而且在线程束的整个生存期中都保持在芯片内， 因此线程束上下文的切换是非常快的。

对于一个给定的内核， 同时存在于同一个SM中的线程块和线程束的数量取决于在SM中可用的且内核所需的寄存器和共享内存的数量。

资源可用性通常会限制SM中常驻线程块的数量。如果每个SM没有足够的寄存器或共享内存去处理至少一个块， 那么内核将无法启动。

当计算资源已分配给线程块时， 线程块被称为**活跃的块**。 它所包含的线程束被称为**活跃的线程束**。 活跃的线程束可以进一步被分为以下3种类型：

- 选定的线程束：活跃执行的线程束
- 阻塞的线程束：活跃的准备执行但尚未执行的线程束
- 符合条件的线程束：没有做好执行准备的线程束

如果同时满足以下两个条件则线程束符合执行条件：

- 32个CUDA核心可用于执行
- 当前指令中所有的参数都已就绪

计算资源限制了活跃的线程束的数量。 为了最大程度地利用GPU， 需要最大化活跃的线程束数量。

### 延迟隐藏

**利用率**与常驻线程束的数量直接相关。当每个时钟周期中所有的线程调度器都有一个符合条件的线程束时， 可以达到计算资源的完全利用。

指令可以被分为两种基本类型：

- 算术指令
- 内存指令

>  指令延迟： 在指令发出和完成之间的时钟周期

算术指令延迟是一个算术操作从开始到它产生输出之间的时间。 内存指令延迟是指发送出的加载或存储操作和数据到达目的地之间的时间。

### 占用率

占用率是每个SM中活跃的线程束占最大线程束数量的比值。

网格和线程块大小的准则：

- 保持每个块中线程数量是线程束大小（32） 的倍数
- 避免块太小： 每个块至少要有128或256个线程
- 根据内核资源的需求调整块大小
- 块的数量要远远多于SM的数量， 从而在设备中可以显示有足够的并行
- 通过实验得到最佳执行配置和资源使用情况

占用率唯一注重的是在每个SM中并发线程或线程束的数量。 然而， 充分的占用率不是性能优化的唯一目标。 内核一旦达到一定级别的占用率， 进一步增加占用率可能不会改进性能。 

### 同步

在CUDA中， 同步可以在两个级别执行：

- 系统级： 等待主机和设备完成所有的工作

  对于主机来说， 许多CUDA API调用和所有的内核启动不是同步的，cudaDeviceSynchronize()可用来阻塞主机应用程序， 直到所有的CUDA操作（复制、核函数等） 完成。

- 块级： 在设备执行过程中等待一个线程块中所有线程到达同一点

  当__syncthreads被调用时， 同一个线程块中的每个线程都必须等待直至该线程块中所有其他线程都已经达到这个同步点。在栅栏之前所有线程产生的所有全局内存和共享内存访问， 将会在栅栏后对线程块中所有其他的线程可见。 该函数可以协调同一个块中线程之间的通信， 但它强制线程束空闲， 从而可能对性能产生负面影响。

线程块中的线程可以通过共享内存和寄存器来共享数据。

在不同的块之间不允许线程同步。 块间同步， 唯一安全的方法是在每个内核执行结束端使用全局同步点； 也就是说， 在全局同步之后， 终止当前的核函数， 开始执行新的核函数。

### 可扩展性

一个可扩展的并行程序可以高效地使用所有的计算资源以提高性能。 可扩展性意味着增加的计算核心可以提高性能。  例如， 若一个CUDA程序在两个SM中是可扩展的， 则与在一个SM中运行相比， 在两个SM中运行会使运行时间减半。

能够在可变数量的计算核心上执行相同的应用程序代码的能力被称为**透明可扩展性**。

CUDA内核启动时， 线程块分布在多个SM中。 网格中的线程块以并行或连续或任意的顺序被执行。 线程块执行的独立性质使得CUDA编程在任意数量的核心中都是可扩展的。独立性是基于不允许跨线程块同步， 线程块可以以任何顺序、 并行、 串行的顺序在任何SM上执行。 

## 并行性的表现

### 用nvprof检测活跃的线程束

一个内核的可实现占用率被定义为： 每周期内活跃线程束的平均数量与一个SM支持的线程束最大数量的比值。 

可通过命令`nvprof --metrics achieved_occupancy xxx_exe`检测。

更高的占用率并不一定意味着有更高的性能。

### 用nvprof检测内存操作

可通过命令`nvprof --metrics gld_efficiency xxx_exe`检测。

可通过命令`nvprof --metrics gld_throughput xxx_exe`检测。

 更高的加载吞吐量并不一定意味着更高的性能。

值得注意的是， 最好的执行配置既不具有最高的可实现占用率， 也不具有最高的加载吞吐量。



## 避免分支分化

 通过重新组织数据的获取模式， 可以减少或避免线程束分化。

### 并行规约问题

在向量中执行满足交换律和结合律的运算， 被称为**归约问题**。 并行归约问题是这种运算的并行执行。 并行归约是一种常见的并行模式， 并且是许多并行算法中的一个关键运算。

根据每次迭代后**输出元素就地存储的位置**， 成对的并行求和实现可以被进一步分为以下两种类型：

- 相邻配对： 元素与它们直接相邻的元素配对。如下左图所示
- 交错配对： 根据给定的跨度配对元素。如下有图所示

<center class = "half">
<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap03/neighbored-pair.png" alt="neighbored-pair" style="zoom:85%;" /> <img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap03/Interleaved-pair.png" style="zoom:85%;" />
</center>



### 并行规约中的分化及改善

<center class = "half">
<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap03/neighbored-pair-impl1.png" alt="neighbored-pair" style="zoom:67%;" /> <img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap03/neighbored-pair-impl2.png" style="zoom:67%;" />
</center>

在上面左图的实现中，只有ID为偶数的线程执行这个条件语句的主体， 但是所有的线程都必须被调度。 在第二次迭代中， 只有四分之一的线程是活跃的， 但是所有的线程仍然都必须被调度。 通过重新组织每个线程的数组索引来强制ID相邻的线程执行求和操作， 线程束分化就能被归约了，如上面有图所示。

### 交错配对的规约

交错实现比第一个实现（上图中左图的实现）快了1.69倍， 比第二个实现（上图中右图的实现）快了1.34倍。 这种性能的提升主要是由kernel函数里的全局内存加载/存储模式导致的。



## 循环展开

循环体的复制数量被称为循环展开因子， 迭代次数就变为了原始循环迭代次数除以循环展开因子。

在CUDA中， 循环展开的意义非常重大。 目标仍然是相同的： 通过减少指令消耗和增加更多的独立调度指令来提高性能。 因此， 更多的并发操作被添加到流水线上， 以产生更高的指令和内存带宽。 这为线程束调度器提供更多符合条件的线程束， 它们可以帮助隐藏指令或内存延迟。



>  当在CUDA中展开循环、 数据块或线程束时， 可以提高性能的两个主要原因是什么？ 解释每种展开是如何提升指令吞吐量的。

Unrolling loops, data blocks, or warps can lead to less frequent branching from fewer loop conditionals. Additionally, unrolling can lead to an increase in the number of independent memory operations discoverable by the compiler. As a result, more concurrent read and write operations can be issued and memory bandwidth utilization will increase. 





## 动态并行

CUDA的动态并行允许在GPU端直接创建和同步新的GPU内核。这使得可以在一个核函数中任意点动态增加GPU应用程序的并行性。

动态并行提供了一个更有层次结构的方法， 在这个方法中， 并发性可以在一个GPU内核的多个级别中表现出来。

使用动态并行可以让递归算法更加清晰易懂， 也更容易理解。

有了动态并行， 可推迟到运行时决定要在GPU上创建多少个块和网格， 可以动态地利用GPU硬件调度器和加载平衡器， 并进行调整以适应数据驱动或工作负载。

在GPU端直接创建工作的能力可以减少在主机和设备之间传输执行控制和数据的需求。



# 四、全局内存

## CUDA内存模型概述

在现有的硬件存储子系统下， 必须依靠内存模型获得最佳的延迟和带宽。 CUDA内存模型结合了主机和设备的内存系统， 展现了完整的内存层次结构， 使你能显式地控制数据布局以优化性能。

### 内存层次结构的优点

一般来说， 应用程序不会在某一时间点访问任意数据或运行任意代码。 应用程序往往遵循局部性原则。有两种不同类型的局部性：

- 时间局部性，如果一个数据位置被引用， 那么该数据在较短的时间周期内很可能会再次被引用， 随着时间流逝， 该数据被引用的可能性逐渐降低。 
- 空间局部性，空间局部性认为如果一个内存位置被引用， 则附近的位置也可能会被引用。

一个内存层次结构由具有不同延迟、 带宽和容量的多级内存组成。

![](/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap04/memory-hierarchy.png)

GPU与CPU在内存层次结构设计中都使用相似的准则和模型。 GPU和CPU内存模型的主要区别是， CUDA编程模型能将内存层次结构更好地呈现给用户， 能让我们显式地控制它的行为。

### CUDA内存模型

CUDA内存模型提出了多种可编程内存的类型：

- 寄存器
- 共享内存
- 本地内存
- 常量内存
- 纹理内存
- 全局内存

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap04/cuda-memory-hierarchy.png" style="zoom:80%;" />

#### 寄存器

核函数中没有修饰符的变量， 通常存储在寄存器中。 在核函数声明的数组中， 如果用于该数组的索引是常量或能在编译时确定， 那么该数组也存储在寄存器中。寄存器变量对于每个线程来说都是私有的， 一个核函数通常使用寄存器来保存需要频繁访问的线程私有变量。 寄存器变量与核函数的生命周期相同。 一旦核函数执行完毕， 就不能对寄存器变量进行访问了。

通过下面的命令会输出`xxx.cu`中每个核函数里寄存器的数量、 共享内存的字节数以及每个线程所使用的常量内存的字节数。

```shell
nvcc -Xptxas -v xxx.cu
nvcc -maxrregcount=32 xxx.cu
```

如果一个核函数使用了超过硬件限制数量的寄存器， 则会用本地内存替代多占用的寄存器。 这种寄存器溢出会给性能带来不利影响。

`-maxrregcount`选项可以用来限制寄存器的使用数量。

#### 本地内存

编译器可能存放到本地内存中的变量有：

- 在编译时使用未知索引引用的本地数组
- 可能会占用大量寄存器空间的较大本地结构体或数组
- 任何不满足核函数寄存器限定条件的变量

溢出到本地内存中的变量本质上与全局内存在同一块存储区域，本地内存数据也是存储在每个SM的一级缓存和每个设备的二级缓存中。

#### 共享内存

在核函数中使用`__shared__`修饰的变量存放在共享内存中。因为共享内存是片上内存， 所以与本地内存或全局内存相比， 它具有更高的带宽和更低的延迟。 它的使用类似于CPU一级缓存， 但它是可编程的。

每个SM都有一定数量的由线程块分配的共享内存。 因此要注意不要过度使用共享内存， 否则将在不经意间限制活跃线程束的数量。

共享内存在核函数的范围内声明， 其生命周期伴随着整个线程块。 当一个线程块执行结束后， 其分配的共享内存将被释放并重新分配给其他线程块。

共享内存是线程之间相互通信的基本方式。访问共享内存必须使用`void __syncthreads()`来防止数据冲突。

#### 常量内存

常量内存驻留在设备内存中， 并在每个SM专用的常量缓存中缓存。 常量变量用`__const__`修饰符来修饰。

常量变量必须在全局空间内和所有核函数之外进行声明。常量内存是静态声明的， 并对同一编译单元中的所有核函数可见。

核函数从常量内存中只能读取数据。 因此， 常量内存必须在主机端使用`cudaError_t cudaMemcpyToSymbol(const void* symbol, const void* src, size_t count);`来初始化。这个函数将`count`个字节从`src`指向的内存复制到`symbol`指向的内存中， `symbol`指向的内存放在设备的全局内存或常量内存中。 在大多数情况下这个函数是同步的。

> 线程束中的所有线程从相同的内存地址中读取数据时， 常量内存表现最好。

举个例子， 数学公式中的系数就是一个很好的使用常量内存的例子， 因为一个线程束中所有的线程使用相同的系数来对不同数据进行相同的计算。 

如果线程束里每个线程都从不同的地址读取数据且只读一次， 那常量内存就不是最佳选择， 因为每从常量内存中读取一次数据， 都会广播给线程束的所有线程。

#### 纹理内存(Texture Memory)

纹理内存驻留在设备内存中， 并在每个SM的只读缓存中缓存。 纹理内存是一种通过指定的只读缓存访问的全局内存。 只读缓存包括硬件滤波的支持， 它可以将浮点插入作为读过程的一部分来执行。 纹理内存是对二维空间局部性的优化， 所以线程束里使用纹理内存访问二维数据的线程可以达到最优性能。 对于一些应用程序来说， 这是理想的内存， 并由于缓存和滤波硬件的支持所以有较好的性能优势。 然而对于另一些应用程序来说， 与全局内存相比， 使用纹理内存更慢。

#### 全局内存

全局内存是GPU中最大、 延迟最高且最常用的内存。 `global`指的是其作用域和生命周期。 它的声明可以在任何SM设备上被访问到， 并且贯穿应用程序的整个生命周期。

全局内存常驻于设备内存中，可通过`32`字节、`64`字节或`128`字节的内存事务进行访问。 这些内存事务必须自然对齐， 也就是说， 首地址必须是`32`字节、`64`字节或`128`字节的倍数。 优化内存事务对于获得最优性能来说是至关重要的。 当一个线程束执行内存加载/存储时， 需要满足的传输数量通常取决于以下两个因素：

- 跨线程的内存地址分布
- 每个事务内存地址的对齐方式

在一般情况下， 用来满足内存请求的事务越多， 未使用的字节被传输回的可能性就越高， 这就造成了数据吞吐率的降低。

#### GPU缓存

GPU缓存是不可编程的内存。 在GPU上有4种缓存：

- 一级缓存
- 二级缓存
- 只读常量缓存
- 只读纹理缓存

每个SM都有一个一级缓存、只读常量缓存和只读纹理缓存， 所有的SM共享一个二级缓存。 

一级和二级缓存都被用来存储本地内存和全局内存中的数据， 包括寄存器溢出的部分。 CUDA允许配置读的数据是使用一级和二级缓存，还是只使用二级缓存。

只读常量缓存和只读纹理缓存用于在设备内存中提高来自于各自内存空间内的读取性能。

在CPU上， 内存的加载和存储都可以被缓存。 但是， 在GPU上只有内存加载操作可以被缓存， 内存存储操作不能被缓存。

#### CUDA变量声明总结

​																										        **CUDA变量和类型修饰符**

|     修饰符     |             变量名称              |  存储器  | 作用域 | 生命周期 |
| :------------: | :-------------------------------: | :------: | :----: | :------: |
|       \        |           `float var;`            |  寄存器  |  线程  |   线程   |
|       \        |         `float var[100];`         | 本地内存 |  线程  |   线程   |
|  `__shared__`  | `float var;` or `float var[100];` | 共享内存 | 线程块 |  线程块  |
|  `__device__`  | `float var;` or `float var[100];` | 全局内存 |  全局  | 应用程序 |
| `__constant__` | `float var;` or `float var[100];` | 常量内存 |  全局  | 应用程序 |

​                                                            														**设备存储器主要特征**

|  存储器  | 片上/片外 | 缓存 | 存取 |     范围      | 生命周期 |
| :------: | :-------: | :--: | :--: | :-----------: | :------: |
|  寄存器  | **片上**  |  NA  | R/W  |   一个线程    |   线程   |
| 本地内存 |   片外    | Yes  | R/W  |   一个线程    |   线程   |
| 共享内存 | **片上**  |  NA  | R/W  | 块内所有线程  |  线程块  |
| 全局内存 |   片外    | Yes  | R/W  | 所有线程+主机 | 主机配置 |
| 常量内存 |   片外    | Yes  |  R   | 所有线程+主机 | 主机配置 |
| 纹理内存 |   片外    | Yes  |  R   | 所有线程+主机 | 主机配置 |



 一般情况下， 设备核函数不能访问主机变量， 并且主机函数也不能访问设备变量， 即使这些变量在同一文件作用域内被声明。
CUDA运行时API能够访问主机和设备变量， 但是这取决于你给正确的函数是否提供了正确的参数， 这样的话才能对正确的变量进行恰当的操作。 



## 内存管理

### 内存分配和释放

```cpp
cudaError_t cudaMalloc(void **devPtr, size_t count); 
// 分配出的全局内存中的值不会被清除。 你需要用cudaMemcpy从主机上传输的数据来填充所分配的全局内存， 或cudaMemset将其初始化
cudaError_t cudaMemcpy(void *dst, const void *src, size_t count, enum cudaMemcpyKind kind);
cudaError_t cudaMemset(void *devPtr, int value, size_t count);

cudaError_t cudaFree(void *devPtr);
```

设备内存的分配和释放操作成本较高， 所以应用程序应重利用设备内存， 以减少对整体性能的影响。

### 内存传输

从下面的内存传输图可以看出：

- GPU芯片和板载GDDR5 GPU内存之间的理论峰值带宽非常高， 对于Fermi C2050来说为144GB/s。
- CPU和GPU之间通过PCIe Gen2总线相连， 这种连接的理论带宽要低得多，为8GB/s（PCIeGen3总线最大理论限制值是16GB/s） 。

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap04/memory-transfer.png" style="zoom:80%;" />

这种差距意味着如果设计不当， 主机和设备间的数据传输会降低应用程序的整体性能。 因此，CUDA编程的一个基本原则是尽可能减少主机与设备之间的传输。

### 固定内存

GPU不能在可分页（`pageable`）主机内存上安全地访问数据， 因为当主机操作系统在物理位置上移动该数据时， 它无法控制。 当从可分页主机内存传输数据到设备内存时， CUDA驱动程序首先分配临时页面锁定的或固定的主机内存， 将主机源数据复制到固定内存中， 然后从固定内存传输数据给设备内存。如下图所示:

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap04/pinned-memory.png" style="zoom:80%;" />

CUDA运行时允许你使用`cudaError_t cudaMallocHost(void **devPtr, size_t count);`直接分配固定主机内存，配套`cudaError_t cudaFreeHost(void *ptr);`接口来释放固定主机内存：

这个函数分配了count字节的主机内存， 这些内存是页面锁定的并且对设备来说是可访问的。 由于固定内存能被设备直接访问， 所以**它能用比可分页内存高得多的带宽**进行读写。 然而，**分配过多的固定内存可能会降低主机系统的性能**， 因为它减少了用于存储虚拟内存数据的可分页内存的数量， 其中分页内存对主机系统是可用的。

### 零拷贝内存

通常来说， 主机不能直接访问设备变量， 同时设备也不能直接访问主机变量。 但有一个例外： 零拷贝内存。 主机和设备都可以访问零拷贝内存。

GPU线程可以直接访问零拷贝内存。 在CUDA核函数中使用零拷贝内存有以下几个优势：

- 当设备内存不足时可利用主机内存
- 避免主机和设备间的显式数据传输
- 提高PCIe传输率

当使用零拷贝内存来共享主机和设备间的数据时， 你必须**同步**主机和设备间的内存访问，需要额外的同步操作。

同时更改主机和设备的零拷贝内存中的数据将导致不可预知的后果。

零拷贝内存是一种特殊的固定（不可分页） 内存，它做了到设备地址空间的映射。你可以通过函数`cudaError_t cudaHostAlloc(void **pHost, size_t count, unsigned int flags);创建一个到固定内存的映射`。这个函数分配了count字节的主机内存， 该内存是页面锁定的且设备可访问的。 用这个函数分配的内存必须用`cudaFreeHost`函数释放。 `flags`参数可以对已分配内存的特殊属性进一步进行配置：

- cudaHostAllocDefault, 使cudaHostAlloc函数的行为与cudaMallocHost函数一致.
- cudaHostAllocPortable, 返回能被所有CUDA上下文使用的固定内存， 而不仅是执行内存分配的那一个。
- cudaHostAllocWriteCombined, 返回写结合内存，该内存可在某些系统配置上通过PCIe总线上更快地传输， 但它在大多数主机上不能被有效地读取。因此， 写结合内存对缓冲区来说是一个很好的选择，该内存通过设备使用映射的固定内存或主机到设备的传输。
- cudaHostAllocMapped, 该标志返回 可以实现主机写入和设备读取被映射到设备地址空间中的主机内存。

你可以使用`cudaError_t cudaHostGetDevicePointer(void **pDevice, void *pHost, unsigned int flags);`获取映射到固定内存的设备指针。该函数返回了一个在pDevice中的设备指针， 该指针可以在设备上被引用以访问映射得到的固定主机内存。 如果设备不支持映射得到的固定内存， 该函数将失效。

进行**频繁读写**操作时，使用零拷贝内存作为设备内存的补充将显著降低性能。 因为每次映射到内存的传输必须经过PCIe总线。比起全局内存， 延迟也显著增加。

如果想共享主机和设备端的**少量数据**， 零拷贝内存可能会是一个不错的选择， 因为它简化了编程且有较好的性能。 对于由PCIe总线连接的离散GPU上的更大数据集来说， 零拷贝内存不是一个好的选择， 它会导致性能的显著下降。

有两种常见的异构计算系统架构： 

- 集成架构，CPU和GPU集成在一个芯片上，且在物理地址上共享主存。这种架构中， 由于无须在PCIe总线上备份，零拷贝内存在性能和可编程性方面可能更佳。
- 离散架构，通过PCIe总线将设备连接到主机， 零拷贝内存只在特殊情况下有优势。

### 统一虚拟寻址

有了UVA(Unified Virtual Addressing)，主机内存和设备内存可以共享同一个虚拟地址空间，如下图所示：

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap04/uva.png" style="zoom:80%;" />

在UVA之前， 你需要管理哪些指针指向主机内存和哪些指针指向设备内存。

有了UVA， 由指针指向的内存空间对应用程序代码来说是透明的。

通过UVA， 由cudaHostAlloc分配的固定主机内存具有相同的主机和设备指针。 所以可以将返回的指针直接传递给核函数。

### 统一内存

在CUDA 6.0中， 引入了“统一内存”这一新特性。统一内存中创建了一个托管内存池（a pool of managed memory）， 内存池中已分配的空间可以用相同的内存地址（即指针） 在CPU和GPU上进行访问。 **底层系统在统一内存空间中自动在主机和设备之间进行数据传输**。 这种数据传输对应用程序是透明的， 这大大简化了程序代码。

> 统一内存 vs. 统一虚拟寻址

- 统一内存寻址依赖于UVA的支持， 但它们是完全不同的技术。 

- UVA为系统中的所有处理器提供了一个单一的虚拟内存地址空间。 

- 但UVA不会自动将数据从一个物理位置转移到另一个位置， 这是统一内存寻址的一个特有功能。

> 统一内存 vs. 零拷贝内存

- 统一内存提供了一个“单指针到数据”模型， 在概念上它类似于零拷贝内存。 

- 但零拷贝内存在主机内存中进行分配， 因此，由于受到在PCIe总线上访问零拷贝内存的影响， 核函数的性能将具有较高的延迟。
- 另一方面， 统一内存将内存和执行空间分离，因此可以根据需要将数据透明地传输到主机或设备上， 以提升局部性和性能。

托管内存指的是由底层系统自动分配的统一内存，它的创建也是使用cudaMalloc。你可以在核函数中使用两种类型的内存：由系统控制的托管内存， 以及由应用程序明确分配和调用的未托管内存。所有在设备内存上有效的CUDA操作也同样适用于托管内存。 其主要区别是主机也能够引用和访问托管内存。

托管内存可以被静态分配也可以被动态分配：

- 静态分配。通过添加`__managed__`修饰符， 静态声明一个设备变量作为托管变量。 但这个操作只能在文件范围和全局范围内进行。 该变量可以从主机或设备代码中直接被引用：

  ```cpp
  __device__ __managed__ int y;
  ```

- 动态分配

  `cudaError_t cudaMallocManaged(void **devPtr, size_t size, unsigned int flags=0);`

  这个函数分配size字节的托管内存，并用devPtr返回一个指针。该指针在所有设备和主机上都是有效的。使用托管内存的程序行为与使用未托管内存的程序副本行为在功能上是一致的。 但使用托管内存的程序可以利用自动数据传输和重复指针消除功能。

  在CUDA 6.0中， 设备代码不能调用`cudaMallocManaged`函数。 所有的托管内存必须在主机端动态声明或者在全局范围内静态声明。



## 内存访问模式

为了在读写数据时达到最佳性能， 内存访问操作必须满足一定的条件。CUDA执行模型的显著特征之一就是**指令必须以线程束为单位进行发布和执行**。存储操作也是同样。在执行内存指令时， 线程束中的每个线程都提供了一个正在加载或存储的内存地址。 在线程束的32个线程中， 每个线程都提出了一个包含请求地址的单一内存访问请求， 它并由一个或多个设备内存传输提供服务。根据**线程束中内存地址的分布**， 内存访问可以被分成不同的模式。

### 对齐与合并访问

如下图所示，全局内存通过缓存来实现加载/存储。

所有对全局内存的访问都会通过**二级缓存**， 也有许多访问会通过**一级缓存**， 这取决于访问类型和GPU架构。 如果这两级缓存都被用到， 那么内存访问是由一个**128字节的内存事务**实现的。 如果只使用了二级缓存， 那么这个内存访问是由一个**32字节的内存事务**实现的。 

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap04/global_mem_access.png" style="zoom:80%;" />

一行**一级缓存**是128个字节，它映射到设备内存中一个128字节的对齐段。如果线程束中的每个线程请求一个4字节的值，那么每次请求就会获取128字节的数据，这恰好与缓存行和设备内存段的大小相契合。

如果设备内存事务的首地址是被用到的缓存（32字节的L2 Cache或者128字节的L1 Cache）的粒度的偶数倍，就是**对齐内存访问**。非对齐的内存访问会浪费带宽。

当一个线程束中全部的32个线程访问一块连续内存时， 就会出现**合并内存访问**。

**对齐合并内存访问**的理想状态是线程束从对齐内存地址开始访问一个连续的内存块。

### 全局内存读取

在SM中，数据通过以下3种路径进行传输：

- 一级和二级缓存。默认路径
- 常量缓存
- 只读缓存

想要通过一级和二级缓存外的其他两种路径传递数据需要应用程序显式地说明。

一级缓存可以通过编译选项来启用或者禁用。

- 禁用（对应非缓存加载模式）。所有对全局内存的加载请求将直接进入到二级缓存； 如果二级缓存缺失， 则由DRAM完成请求。 每一次内存事务可由一个、 两个或四个部分执行， 每个部分有32个字节。 
- 启用（对应缓存加载模式）。全局内存加载请求首先尝试通过一级缓存。 如果一级缓存缺失， 该请求转向二级缓存。 如果二级缓存缺失， 则请求由DRAM完成。 在这种模式下， 一个内存加载请求由一个128字节的设备内存事务实现。

#### 缓存加载

缓存加载操作经过一级缓存， 在粒度为128字节的一级缓存行上由设备内存事务进行传输。 缓存加载可以分为对齐/非对齐及合并/非合并。 aligned/misaligned
and coalesced/uncoalesced

组合起来有4种情况：

- 对齐合并（aligned-coalesced）, 最理想的情况。

  线程束中所有线程请求的地址都在128字节的缓存行范围内。 完成内存加载操作只需要一个128字节的事务。 总线的使用率为100%， 在这个事务中没有未使用的数据

  <img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap04/aligned-coalesced.png" style="zoom:80%;" />

- 对齐非合并（aligned-uncoalesced）,

  访问是对齐的， 引用的地址不是连续的线程ID， 而是128字节范围内的随机值。 由于线程束中线程请求的地址仍然在一个缓存行范围内， 所以只需要一个128字节的事务来完成这一内存加载操作。 总线利用率仍然是100%，

  <img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap04/aligned-uncoalesced.png" style="zoom:80%;" />

- 非对齐合并（misaligned-coalesced）,

  线程束请求32个连续4个字节的非对齐数据元素。 在全局内存中线程束的线程请求的地址落在两个128字节段范围内。 因为当启用一级缓存时， 由SM执行的物理加载操作必须在128个字节的界线上对齐， 所以要求有两个128字节的事务来执行这段内存加载操作。 总线利用率为50%， 并且在这两个事务中加载的字节有一半是未使用的。

  <img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap04/misaligned-coalesced.png" style="zoom:80%;" />

  

  线程束中所有线程请求相同的地址。 因为被引用的字节落在一个缓存行范围内， 所以只需请求一个内存事务， 但总线利用率非常低。 如果加载的值是4字节的， 则总线利用率是4字节请求/128字节加载＝3.125%。

  ​       				<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap04/aligned-uncoalesced2.png" style="zoom:80%;" />

- 非对齐非合并（misaligned-uncoalesced）, 最差的情况。

  线程束中线程请求分散于全局内存中的32个4字节地址。尽管线程束请求的字节总数仅为128个字节， 但地址要占用N个缓存行（0＜N≤32） 。 完成一次内存加载操作需要申请N次内存事务。

  <img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap04/misaligned-uncoalesced.png" style="zoom:80%;" />



>  CPU一级缓存和GPU一级缓存之间的差异

CPU一级缓存优化了时间和空间局部性。GPU一级缓存专为空间局部性而非时间局部性设计。频繁访问一个一级缓存的内存位置不会增加数据留在缓存中的概率。

#### 没有缓存的加载

没有缓存的加载不经过一级缓存，它在内存段的粒度上（32个字节） 而非缓存池的粒度（128个字节） 执行。这是更细粒度的加载，可以为非对齐或非合并的内存访问带来更好的总线利用率。

#### 只读缓存

只读缓存最初是预留给纹理内存加载使用的。对计算能力为3.5及以上的GPU来说，只读缓存也支持使用全局内存加载代替一级缓存。只读缓存的加载粒度是32个字节。通常，对分散读取来说，这些更细粒度的加载要优于一级缓存。

有两种方式可以指导内存通过只读缓存进行读取：

- 使用函数__ldg

  ```cpp
  // 使用内部函数__ldg来通过只读缓存直接对数组进行读取访问
  __global__ void copyKernel(int *out, int *in) {
  	int idx = blockIdx.x * blockDim.x + threadIdx.x;
  	out[idx] = __ldg(&in[idx]);  // 原始方式：out[idx] = in[idx];
  }
  ```

- 在间接引用的指针上使用修饰符

  你也可以将`__restrict__`修饰符应用到指针上。该修饰符帮助nvcc识别无别名指针（即专门用来访问特定数组的指针）。nvcc将自动通过只读缓存指导无别名指针的加载。

  ```cpp
  __global__ void copyKernel(int * __restrict__ out, const int * __restrict__ in) {
  	int idx = blockIdx.x * blockDim.x + threadIdx.x;
  	out[idx] = in[idx];
  }
  ```

### 全局内存写入

GPU的内存的存储操作相对简单。一级缓存不能用在Fermi或Kepler GPU上进行存储操作，在发送到设备内存之前**存储操作只通过二级缓存**。 存储操作在32个字节段的粒度上被执行。 内存事务可以同时被分为**一段**、 **两段**或**四段**。 例如， 两个地址同属于一个128个字节的区域， 但不属于一个对齐的64个字节区域， 则会执行一个四段事务。也就是说， 执行一个四段事务比执行两个一段事务效果更好。

### 结构体数组与数组结构体

```cpp
struct innerStruct {
	float x;
	float y;
};
```

- 结构体数组(`AoS`)，an Array of Structures

  ```cpp
  struct innerStruct myAoS[N];
  ```

  它存储的是空间上相邻的数据(例如x和y)， 这在CPU上会**有良好的缓存局部性**。

- 数组结构体(`SoA`)，a Structure of Arrays

  ```cpp
  struct innerArray {
  	float x[N];
  	float y[N];
  };
  ```

  在原结构体中每个字段的所有值都被分到各自的数组中。 这不仅能将相邻数据点紧密存储起来， 也能将跨数组的独立数据点存储起来。



下图说明了AoS和SoA方法的内存布局

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap04/aos-soa.png" style="zoom:80%;" />

- AoS模式在GPU上存储示例数据并执行一个只有x字段的应用程序，将导致50%的带宽损失，因为y值在每32个字节段或128个字节缓存行上隐式地被加载。AoS格式也在不需要的y值上浪费了二级缓存空间。

- SoA模式存储数据充分利用了GPU内存带宽。由于没有相同字段元素的交叉存取，SoA布局提供了合并内存访问，且可对全局内存实现更高效的利用。

> AoS vs. SoA

许多并行编程范式，尤其是SIMD型范式，更**倾向于使用SoA**。在CUDA C编程中**也普遍倾向于使用SoA**，因为数据元素是为全局内存的有效合并访问而预先准备好的， 而被相同内存操作引用的同字段数据元素在存储时是彼此相邻的。

### 性能调整

优化设备内存带宽利用率有两个目标：

- 对齐及合并内存访问， 以减少带宽的浪费
- 足够的并发内存操作， 以隐藏内存延迟

#### 展开技术

内存操作的循环展开增加了更独立的内存操作。 展开并不影响执行内存操作的数量（只影响并发执行的数量）。

#### 增大并行性

为了充分体现并行性， 你应该用一个核函数启动的网格和线程块大小进行试验， 以找到该核函数最佳的执行配置。 



>  最大化带宽利用率

影响设备内存操作性能的因素主要有两个：

- 有效利用设备DRAM和SM片上内存之间的字节移动： 为了避免设备内存带宽的浪费， 内存访问模式应是对齐和合并的

- 当前的并发内存操作数： 可通过以下两点实现最大化当前存储器操作数。
  - 展开，每个线程产生更多的独立内存访问。
  - 修改核函数启动的执行配置来使每个SM有更多的并行性。



### 核函数可达到的带宽

#### 内存带宽

理论带宽是当前硬件可以实现的绝对最大带宽。

有效带宽是核函数实际达到的带宽， 是测量出来的。公式：`有效带宽(GB/s) = (读字节数+写字节数)×10^(−9) / 运行时间`

例如， 对于从设备上传入或传出数据的拷贝来说（包含4个字节整数的2048×2048矩阵）， 有效带宽根据公式计算如下：

`有效带宽(GB/s) = (2048*2048*4*2)×10^(−9) / 运行时间`

#### 矩阵转置问题

对角坐标



### 使用统一内存的矩阵加法

为了简化主机和设备内存空间的管理， 提高这个CUDA程序的可读性和易维护性， 可以使用统一内存。



# 五、共享内存和常量内存

共享内存是许多高性能计算应用程序的关键驱动力。

## CUDA共享内存概述

共享内存是较小的片上内存， 具有较低的延迟，且共享内存可提供比全局内存高得多的带宽。可以把它当作一个可编程管理的缓存。共享内存通常的用途有：

- 块内线程通信的通道。
- 用于全局内存数据的可编程管理的缓存。
- 高速暂存存储器，用于转换数据以优化全局内存访问模式。

### 共享内存

 每个SM都有一个小的低延迟内存池，这个内存池被当前该SM上正在执行的线程块中的所有线程所共享。共享内存使同一个线程块中的线程能够互相协作，便于重用片上数据，并可大大降低核函数所需的全局内存带宽。由于共享内存中的内容是由应用程序显式管理的，所以它通常被描述为可编程管理的缓存。

如果多个线程访问共享内存中的同一个字，一个线程读取该字后，通过多播把它发送给其他线程。共享内存被SM中的所有常驻线程块划分，因此，共享内存是限制设备并行性的关键资源。一个核函数使用的共享内存越多， 处于并发活跃状态的线程块就越少。

共享内存是一个可编程管理的缓存。当数据移动到共享内存中以及数据被释放时，我们对它有充分的控制权。由于在CUDA中允许手动管理共享内存，所以通过在数据布局上提供**更多的细粒度控制和改善片上数据的移动**，使得对应用程序代码进行优化变得更简单了。

### 共享内存分配

可以**静态**或**动态**地分配共享内存变量。在CUDA的源代码文件中，共享内存可以被声明为一个本地的CUDA核函数或是一个全局的CUDA核函数。CUDA支持一维、 二维和三维共享内存数组的声明。

如果在核函数中进行声明，那这个变量的作用域就局限在该内核中。如果在文件的任何核函数外进行声明，那这个变量的作用域对所有核函数来说都是全局的。

- 静态分配

  ```cpp
  __shared__ float tile[size_y][size_x];
  ```

- 动态分配

  ```cpp
  // 声明了共享内存中一个未知大小的一维整型数组。 这个声明可以在某个核函数的内部或所有核函数的外部进行
  extern __shared__ int tile[];
  
  // 因为tile数组的大小在编译时是未知的，所以在每个核函数被调用时，需要动态分配共享内存，将所需的大小按字节数作为三重括号内的第三个参数
  kernel<<<grid, block, isize * sizeof(int)>>>(...);
  ```

  注意，只能动态声明一维数组。

### 共享内存存储体和访问模式

第四章全局内存解释了由不同的全局内存访问模式引起的延迟和带宽对核函数性能的影响。 共享内存可以用来隐藏全局内存延迟和带宽对性能的影响。

#### 内存存储体

为了获得高内存带宽，共享内存被分为**32个**同样大小的内存模型，它们被称为**存储体**，它们可以被同时访问。有32个存储体是因为在一个线程束中有32个线程。 共享内存是一个一维地址空间。 根据GPU的计算能力，**共享内存的地址在不同模式下会映射到不同的存储体中** 。如果通过线程束发布共享内存加载或存储操作， 且在每个存储体上只访问不多于一个的内存地址，那该操作可由一个内存事务来完成。 否则，该操作由多个内存事务来完成， 这样就降低了内存带宽的利用率。

#### 存储体冲突

在共享内存中当多个地址请求落在相同的内存存储体中时，就会发生存储体冲突，这会导致请求被重复执行。硬件会将存储体冲突的请求分割到尽可能多的独立的无冲突事务中，有效带宽的降低是由一个等同于所需的独立内存事务数量的因素导致的。

当线程束发出共享内存请求时，有3种典型模式：

- 并行访问：多个地址**访问**多个存储体。

  它是被一个线程束访问的多个地址落在多个存储体中。这种模式意味着，如果不是所有的地址，那么至少有一些地址可在一个单一内存事务中被服务。最佳情况是，当每个地址都位于一个单独的存储体中时，执行无冲突的共享内存访问。

- 串行访问：多个地址**访问**同一个存储体。

  最坏的模式，当多个地址属于同一个存储体时，必须以串行的方式进行请求。如果线程束中32个线程全都访问同一存储体中不同的内存地址，那将需要32个内存事务，并且满足这些访问所消耗的时间是单一请求的32倍。

- 广播访问：单一地址**读取**单一存储体。

  线程束中所有的线程都读取同一存储体中相同的地址。若一个内存事务被执行，那么被访问的字就会被广播到所有请求的线程中。虽然一个单一的内存事务只需一个广播访问， 但是因为只有一小部分字节被读取， 所以带宽利用率很差。

#### 访问模式

共享内存存储体的宽度规定了共享内存地址与共享内存存储体的对应关系。

从共享内存地址到存储体索引的映射公式：

- 32 bits: 存储体索引＝（字节地址 ÷ 4）%32

  <img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap05/bank-idx-32bit.png" style="zoom:80%;" />

- 64 bits: 存储体索引＝（字节地址 ÷ 8）%32

  <img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap05/bank-idx-64bit.png" style="zoom:80%;" />



#### 内存填充

内存填充是避免存储体冲突的一种方法。下图为通过一个简单的例子来说明内存填充。

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap05/memory-padding.png" style="zoom: 80%;" />

假设有5个共享内存存储体。如果所有线程访问`bank 0`的不同地址，那么会发生一个五向的存储体冲突。

解决这种存储体冲突的一个方法是在每N个元素之后添加一个字，N是存储体的数量。这就改变了从字到存储体的映射，如上图右侧所示。由于填充， 之前所有属于`bank 0`的字， 现在被传播到了不同的存储体中。

填充的内存不能用于数据存储。其唯一作用是移动数据元素，以便将原来属于同一个存储体中的数据分散到不同存储体中。这样，线程块可用的总的共享内存的数量将减少。 填充之后， 还需要重新计算数组索引以确保能访问到正确的数据元素。

#### 访问模式配置

```cpp
// CUDA运行时API查询访问模式，pConfig为出参，
// 取值可能为cudaSharedMemBankSizeFourByte或cudaSharedMemBankSizeEightByte
cudaError_t cudaDeviceGetSharedMemConfig(cudaSharedMemConfig *pConfig);

// 在可配置共享内存存储体的设备上， 可以使用以下功能设置一个新的存储体大小
// 支持的配置值为cudaSharedMemBankSizeDefault、cudaSharedMemBankSizeFourByte、cudaSharedMemBankSizeEightByte
cudaError_t cudaDeviceSetSharedMemConfig(cudaSharedMemConfig config);
```

在不同的核函数启动之间更改共享内存配置可能需要一个隐式的设备同步点。更改共享内存存储体的大小不会增加共享内存的使用量，也不会影响核函数的占用率，但对性能可能有重大影响。一个大的存储体可能为共享内存访问产生更高的带宽，但可能会导致更多的存储体冲突，取决于应用程序中共享内存的访问模式。

### 配置共享内存量

每个SM都有`64 KB`的片上内存。**共享内存**和**一级缓存**共享该硬件资源。CUDA为配置一级缓存和共享内存的大小提供了两种方法：

- 按设备进行配置
- 按核函数进行配置

```cpp
// 使用下述的运行时函数，可为在设备上启动的核函数配置一级缓存和共享内存的大小
cudaError_t cudaDeviceSetCacheConfig(cudaFuncCache cacheConfig);

/*
 所支持的缓存配置参数如下
 cudaFuncCachePreferNone: no preference(default)
 cudaFuncCachePreferShared: prefer 48KB shared memory and 16 KB L1 cache
 cudaFuncCachePreferL1: prefer 48KB L1 cache and 16 KB shared memory
 cudaFuncCachePreferEqual: prefer 32KB L1 cache and 32 KB shared memory
*/
```

典型情况如下：

- 当核函数使用较多的共享内存时， 倾向于更多的共享内存
- 当核函数使用更多的寄存器时， 倾向于更多的一级缓存

指定`-Xptxasv`选项给`nvcc`， 可以知道核函数使用了多少寄存器。当内核用的寄存器数量超过硬件限制数量时， 应该为寄存器溢出配置一个更大的一级缓存。

CUDA运行时会尽可能使用请求设备的片上内存配置， 但如果需要执行一个核函数，它可自由地选择不同的配置。 每个核函数的配置可以覆盖设备范围的设置， 也可以使用以下运行时函数进行设置：

```cpp
cudaError_t cudaFuncSetCacheConfig(const void* func, enum cudaFuncCacheca cheConfig);
```

核函数使用的这种配置是由核函数指针`func`指定的。启动一个不同优先级的内核比启动有最近优先级设置的内核更可能会导致隐式设备同步。 对于每个核，只需调用一次这个函数。每个核函数启动时，片上内存中的配置不需要重新设定。

虽然一级缓存和共享内存位于相同的片上硬件中， 但在某些方面它们却不太相同。 共享内存是通过32个存储体进行访问的， 而一级缓存则是通过缓存行进行访问的。 使用共享内存， 对存储内容和存放位置有完全的控制权， 而使用一级缓存， 数据删除工作是由硬件完成的。

### 同步

 当不同步的多个线程修改同一个共享内存地址时，将导致线程内的冲突。CUDA提供了几个运行时函数来执行块内同步。 同步的两个基本方法如下所示：

- 障碍。所有调用的线程等待其余调用的线程到达障碍点。
- 内存栅栏。所有调用的线程必须等到全部内存修改对其余调用线程可见时才能继续执行。

#### 弱排序内存模型

现代的内存架构有一个宽松的内存模型。这意味着内存访问不一定按照它们在程序中出现的顺序执行。CUDA采用弱排序内存模型从而优化了更多激进的编译器。

GPU线程在不同内存（如共享内存、全局内存、锁页主机内存(`page-locked host memory`)或对等设备的内存）中写入数据的顺序，不一定和这些数据在源代码中访问的顺序相同。一个线程的写入顺序对其他线程可见时， 它可能和写操作被执行的实际顺序不一致。

为了显式地强制程序以一个确切的顺序执行，必须在应用程序代码中插入内存栅栏和障碍。

#### 显式障碍

在CUDA中，障碍只能在同一线程块的线程间执行。在核函数中，可以通过调用`void __syncthreads()`来指定一个障碍点。

`__syncthreads`作为一个障碍点来发挥作用，用于协调同一块中线程间的通信，它要求块中的线程必须等待，直到所有线程都到达该点。`__syncthreads`还确保在障碍点之前，被这些线程访问的所有全局和共享内存对同一块中的所有线程都可见。

在条件代码中使`__syncthreads`时， 必须要特别小心。 可以正常使用`__syncthreads`的充分必要条件是一个条件能保证对**整个线程块**进行同等评估。 否则执行很可能会挂起或产生意想不到的问题。 例如下面的代码可能会导致块中的线程无限期地等待对方， 因为块中的所有线程没有达到相同的障碍点：

```cpp
if (threadID % 2 == 0) {
	__syncthreads();
} else {
	__syncthreads();
}
```

#### 内存栅栏

内存栅栏的功能可确保栅栏前的任何内存写操作对栅栏后的其他线程都是可见的。 根据所需范围， 有3种内存栅栏： 块、 网格或系统。

- `void __threadfence_block();`

  `__threadfence_block`保证了栅栏前被调用线程中对共享内存和全局内存的所有写操作对栅栏后同一块中的其他线程都是可见的。

- `__void __threadfence()`;

  `__threadfence`挂起调用的线程， 直到全局内存中的所有写操作对相同网格内的所有线程都是可见的。

- `void __threadfence_system();`

  `__threadfence_system`挂起调用的线程，以确保该线程对全局内存、锁页主机内存和其他设备内存中所有写操作对全部设备中的线程和主机线程是可见的。

#### volatile修饰符

在全局或共享内存中使用`volatile`修饰符声明一个变量，可以防止编译器优化，编译器优化可能会将数据暂时缓存在寄存器或本地内存中。当使用`volatile`修饰符时，编译器假定任何其他线程在任何时间都可能更改或使用该变量的值。因此，这个变量的任何引用都会直接被编译到全局内存读指令或全局内存写指令中，它们都会忽略缓存。



## 共享内存的数据布局

当使用共享内存设计核函数时， 重点应放在以下两个概念上：

- 跨内存存储体映射数据元素
- 从线程索引到共享内存偏移的映射





## 减少全局内存访问

使用共享内存的主要原因之一是要缓存片上的数据， 从而减少核函数中全局内存访问的次数。



## 合并的全局内存访问

使用共享内存也能帮助避免对未合并的全局内存的访问。矩阵转置就是一个典型的例子：读操作被自然合并，但写操作是按照交叉访问的。第4章中已表明， 交叉访问是全局内存中最糟糕的访问模式，因为它浪费总线带宽。在共享内存的帮助下，可以先在共享内存中进行转置操作，然后再对全局内存进行合并写操作。



## 常量内存

常量内存是一种专用的内存， 它用于只读数据和统一访问线程束中线程的数据。 常量内存对内核代码而言是只读的， 但它对主机而言既是可读又是可写的。

常量内存位于设备的DRAM上（和全局内存一样） ， 并且有一个专用的片上缓存。 和一级缓存和共享内存一样， 从每个SM的常量缓存中读取的延迟， 比直接从常量内存中读取的低得多。 每个SM常量内存缓存大小的限制为64KB。

在常量内存中，如果线程束中的所有线程都访问相同的位置，那么这个访问模式就是最优的。 如果线程束中的线程访问不同的地址，则访问就需要串行。因此，一个常量内存读取的成本与线程束中线程读取唯一地址的数量呈线性关系。

### 使用常量内存实现一维模板

`f'(x) ≈ c0(f(x+4h)−f(x−4h)) + c1(f(x+3h)−f(x−3h)) - c2(f(x+2h)−f(x−2h)) + c3(f(x+h)−f(x−h))`

在上述模板公式的例子下，系数`c0`、`c1`、`c2`和`c3`在所有线程中都是相同的且不会被修改。这使它们成为常量内存最优的候选，因为它们是只读的，并将呈现一个广播式的访问模式：线程束中的每个线程同时引用相同的常量内存地址。

### 与只读缓存的比较

一般来说， 只读缓存在分散读取方面比一级缓存更好， 当线程束中的线程都读取相同地址时， 不应使用只读缓存。 只读缓存的粒度为32个字节。

只读缓存是独立的，且区别于常量缓存。通过常量缓存加载的数据必须是相对较小的，而且访问必须一致以获得良好的性能（一个线程束内的所有线程在任何给定时间内应该都访问相同的位置），而通过只读缓存加载的数据可以是比较大的， 而且能够在一个非统一的模式下进行访问。

> 常量缓存 vs. 只读缓存

- 在设备上常量缓存和只读缓存都是只读的。
- 每个SM资源都有限： 常量缓存是64 KB， 而只读缓存是48 KB。
- 常量缓存在统一读取中可以更好地执行（统一读取是线程束中的每一个线程都访问相同的地址） 。
- 只读缓存更适合于分散读取。



## 线程束洗牌指令

从Kepler系列GPU(计算能力为3.0或更高) 开始，洗牌指令(shuffle instruction) 作为一种机制被加入其中，只要两个线程在相同的线程束中，那么就允许这两个线程直接读取另一个线程的寄存器。

洗牌指令使得线程束中的线程彼此之间可以**直接交换数据**，**无需通过共享内存或全局内存**。洗牌指令比共享内存有更低的延迟，且该指令在执行数据交换时不消耗额外的内存。因此，洗牌指令为应用程序快速交换线程束中线程间的数据提供了一个有吸引力的方法。

### 线程束洗牌指令的不同形式

有两组洗牌指令： 一组用于整型变量， 另一组用于浮点型变量。 每组有4种形式的洗牌指令。 

- `int __shfl(int var, int srcLane, int width=warpSize)`

- `int __shfl_up(int var, unsigned int delta, int width=warpSize)`

- `int __shfl_down(int var, unsigned int delta, int width=warpSize)`

- `int __shfl_xor(int var, int laneMask, int width=warpSize)`

### 线程束内的共享数据

洗牌指令将被应用到以下3种整数变量类型中：

- 标量变量
- 数组
- 向量型变量

#### 跨线程束值的广播

#### 线程束内上移

#### 线程束内下移

#### 线程束内环绕移动

#### 跨线程束的蝴蝶交换

#### 跨线程束交换数组值

#### 跨线程束使用数组索引交换数值



### 使用线程束洗牌指令的并行归约



# 六、流和并发

前面几章介绍了**内核级的并发**，以及提升内核性能的几种方法，它们分别是从**编程模型**、**执行模型**和**内存模型**的角度进行介绍的。

本章将研究**网格级的并发**。在网格级并发中，多个内核在同一设备上同时执行，这往往会让设备利用率更好。

## 流和事件概述

CUDA流是一系列异步的CUDA操作，这些操作按照主机代码确定的顺序在设备上执行。流能封装这些操作，保持操作的顺序，允许操作在流中排队，并使它们在先前的所有操作之后执行，并且可以查询排队操作的状态。流中操作的执行相对于主机总是异步的。 使用多个流同时启动多个内核， 可以实现网格级并发。

CUDA的API函数一般可以分为同步或异步。具有同步行为的函数会阻塞主机端线程，直到它们完成。具有异步行为的函数被调用后，会立即将控制权归还给主机。 异步函数和流是在CUDA中构建网格级并发的两个基本支柱。

### CUDA流

所有的CUDA操作（包括内核和数据传输）都在一个流中显式或隐式地运行。流分为两种类型：

- 隐式声明的流（空流）
- 显式声明的流（非空流）

如果没有显式地指定一个流，那么内核启动和数据传输将默认使用空流。 

另一方面，非空流可以被显式创建和管理。如果想要重叠不同的CUDA操作， 必须使用非空流。基于流的异步的内核启动和数据传输支持以下类型的粗粒度并发:

- 重叠主机计算和设备计算
- 重叠主机计算和主机与设备间的数据传输
- 重叠主机与设备间的数据传输和设备计算
- 并发设备计算

`cudaError_t cudaStreamCreate(cudaStream_t* pStream)`接口可以创建一个可以显式管理的非空流。

当执行异步数据传输时， 必须使用固定（或非分页的） 主机内存。 在主机虚拟内存中固定分配， 可以确保其在CPU内存中的物理位置在应用程序的整个生命周期中保持不变。 否则， 操作系统可以随时自由改变主机虚拟内存的物理位置。 

在非默认流中启动内核， 必须在内核执行配置中提供一个流标识符作为第四个参数：

```cpp
kernel_name<<<grid, block, sharedMemSize, stream>>>(argument list);
```

`cudaError_t cudaStreamDestroy(cudaStream_t stream);`释放流中的资源。在一个流中，当`cudaStreamDestroy`函数被调用时， 如果该流中仍有未完成的工作，`cudaStreamDestroy`函数将立即返回，当流中所有的工作都已完成时，与流相关的资源将被自动释放。

因为所有的CUDA流操作都是异步的， 所以CUDA的API提供了两个函数来检查流中所有操作是否都已经完成:

- `cudaError_t cudaStreamSynchronize(cudaStream_t stream);`

  强制阻塞主机， 直到在给定流中所有的操作都完成了。

- `cudaError_t cudaStreamQuery(cudaStream_t stream);`

  检查流中所有操作是否都已经完成，但在它们完成前不会阻塞主机。当所有操作都完成时cudaStreamQuery函数会返回cudaSuccess，当一个或多个操作仍在执行或等待执行时返回cudaErrorNotReady。



<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap06/three-streams.png" style="zoom:80%;" />

在上图中，数据传输操作虽分布在不同的流，但并没有并发执行。这是由一个共享资源导致的: `PCIe`总线。虽然从编程模型的角度来看这些操作是独立的，但因为它们共享一个相同的硬件资源，所以它们的执行必须是串行的。具有双工`PCIe`总线的设备可以重叠两个数据传输，但它们必须在不同的流中以及不同的方向上。 

### 流调度

![](/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap06/single-hardware-work-queue.png)

网络中所有的流最终被多路复用到单一的硬件工作队列中的。当选择一个网格执行时，在队列前面的任务由CUDA运行时调度。运行时检查任务的依赖关系，如果仍有任务在执行，那么将等待该任务依赖的任务执行完。最后，当所有依赖关系都执行结束时，新任务被调度到可用的SM中。 这种单一流水线可能会导致虚假的依赖关系。 

![](/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap06/multiple-hardware-work-queue.png)

Kepler GPU使用多个硬件工作队列，从而减少了虚假的依赖关系。**Hyper-Q技术**通过在主机和设备之间维持多个硬件管理上的连接，允许多个CPU线程或进程在单一GPU上同时启动工作. Kepler GPU使用32个硬件工作队列，每个流分配一个工作队列。如果创建的流超过32个，多个流将共享一个硬件工作队列。这样做的结果是可实现全流级并发，并且其具有最小的虚假流间依赖关系。

### 流的优先级

对计算能力为3.5或更高的设备， 可以用接口`cudaError_t cudaStreamCreateWithPriority(cudaStream_t* pStream, unsigned int flags,
int priority);`给流分配优先级。

高优先级流的网格队列可以优先占有低优先级流已经执行的工作。 流优先级不会影响数据传输操作， 只对计算内核有影响。 如果指定的优先级超出了设备定义的范围， 它会被自动限制为定义范围内的最低值或最高值。

### CUDA事件

CUDA中事件本质上是CUDA流中的标记，它与该流内操作流中特定点相关联。可以使用事件来执行以下两个基本任务：

- 同步流的执行
- 监控设备的进展

CUDA的API提供了在流中任意点插入事件以及查询事件完成的函数。只有当一个给定CUDA流中先前的所有操作都执行结束后，记录在该流内的事件才会起作用（即完成）。在默认流中指定的事件， 适用于CUDA流中先前所有的操作。

#### 创建和销毁

```cpp
cudaEvent_t event; // 声明一个事件
cudaError_t cudaEventCreate(cudaEvent_t* event); // 创建一个事件
cudaError_t cudaEventDestroy(cudaEvent_t event); // 销毁一个事件
```

当`cudaEventDestroy`函数被调用时，如果事件尚未起作用，则调用立即返回，当事件被标记完成时自动释放与该事件相关的资源。

下面的代码片段演示如何将事件用于对设备操作计时。

```cpp
// create two events
cudaEvent_t start, stop;
cudaEventCreate(&start);
cudaEventCreate(&stop);
// record start event on the default stream
cudaEventRecord(start);
// execute kernel
kernel<<<grid, block>>>(arguments);
Introducing Streams and Events ❘ 275
// record stop event on the default stream
cudaEventRecord(stop);
// wait until the stop event completes
cudaEventSynchronize(stop);
// calculate the elapsed time between two events
float time;  //以ms为单位
cudaEventElapsedTime(&time, start, stop);
// clean up the two events
cudaEventDestroy(start);
cudaEventDestroy(stop);
```

上述示例中，启动和停止事件被默认放置到空流中。一个时间戳记录空流开始时的启动事件，另一个时间戳记录空流结束时的停止事件。然后，使用`cudaEventElapsedTime`函数得到两个事件之间的运行时间。

事件的启动和停止不必在同一个CUDA流中。 请注意，如果在非空流中记录启动事件或停止事件时，返回的时间可能比预期的要大。这是因为`cudaEventRecord`函数是异步的，并且不能保证计算的延迟正好处于两个事件之间。

### 流同步





# 七、调整指令级原语

针对计算密集型操作。

## CUDA指令概述

本节涵盖了显著影响CUDA内核生成指令的3大因素： 浮点运算、 内置和标准函数、 原子操作。 

### 浮点指令

双精度浮点数既有更好的细粒度又有比单精度值更大的数值范围。

### 内部函数和标准函数

标准函数用于支持可对主机和设备进行访问并标准化主机和设备的操作。 标准函数包含来自于C标准数学库的数学运算， 如sqrt、 exp和sin。 单指令运算如乘法和加法， 也包含在标准函数中。

CUDA内置函数只能对设备代码进行访问。在编程中， 如果一个函数是内部函数或是内置函数， 那么在编译时对它的行为会有特殊响应， 从而产生更积极的优化和更专业化的指令生成。 

在CUDA中，许多内部函数与标准函数是有关联的，这意味着存在与内部函数功能相同的标准函数。举个例子，标准函数中的双精度浮点平方根函数`sqrt` 有相同功能的内部函数是`__dsqrt_rn`。还有执行单精度浮点除法运算的内部函数：` __fdividef`。

内部函数分解成了比与它们等价的标准函数更少的指令。 这会导致内部函数比等价的标准函数更快， 但数值精确度却更低。

### 原子操作指令

一条原子操作指令用来执行一个数学运算， 此操作是一个独立不间断的操作， 且没有其他线程的干扰。 当一个线程在一个变量上成功完成一个原子操作时， 那么不管有多少线程正在访问这个变量， 这个变量的状态都已经发生了改变。

数据竞争的定义是两个或者多个独立的正在执行的线程访问同一个地址， 并且至少其中一个访问会修改该地址。

原子运算函数分为3种： 算术运算函数、 按位运算函数和替换函数。

```cpp
int atomicAdd(int *M, int V);
int atomicExch(int *M, int V); // 无条件地替换已有的值
int atomicCAS(int *address, int compare, int val);// 有条件地替换已有的值（比较并交换）
```

## 优化你的程序指令

### 单精度与双精度比较

双精度变量相较于单精度变量来说， 可以在一个更精细的粒度和更广泛的范围上表示不同的数值。双精度数值的精确性是以空间和性能消耗为代价的。 

首先，性能上，单精度和双精度浮点运算在通信和计算上的性能差异是不可忽略的。 

其次，精度上，单精度与双精度的结果有较大的数值差异，这些结果可能在迭代过程中不断被积累，即第一次迭代产生的不精确的结果作为下一次迭代的输入继续参与运算，导致最终结果偏差很大。因此，考虑到数值精确度，在迭代应用中可能更需要使用双精度变量。

然后，存储上，由于双精度数值所占空间是单精度数值的两倍， 所以当在寄存器中存储一个双精度数值（在内核中已被声明） 时， 一个线程块总的共享寄存器区域会比使用浮点数小得多。 

在声明单精度浮点数值时必须非常谨慎（例如，`pi＝3.14159f`）。任何不正确的省略尾数`f`的声明（`pi＝3.14159`） 都会自动地被NVCC编译器转换成双精度数.

### 标准函数与内部函数的比较

#### 标准函数和内部函数可视化

nvcc的`--ptx`标志能够让编译器在并行线程执行（PTX） 和指令集架构（ISA） 中生成程序的中间表达式。

#### 操纵指令生成

CUDA编译器中有两种方法可以控制指令级优化类型： 编译器标志、 内部或标准函数调用。

内部函数`__fdividef`与运算符`/`相比，在执行浮点数除法时速度更快但数值精确度相对较低。

### 了解原子指令

原子级比较并交换（CAS） 运算符`int atomicCAS(int *address, int compare, int val)`。

CAS将3个内容作为输入：目标内存地址`address`、存储在此地址中的期望值`compare`，以及实际想要存储在此位置的新值`val`，然后执行以下几步：

- 读取目标地址并将该处地址的存储值与预期值进行比较。
  - 如果存储值与预期值相等， 那么新值将存入目标位置。
  - 如果存储值与预期值不等， 那么目标位置不会发生变化。

- 不论发生什么情况， 一个CAS操作总是返回目标地址中的值。 注意， 使用返回值可以用来检查一个数值是否被替换成功。 如果返回值等于传入的预期值， 那么CAS操作一定成功了。

#### 内置的CUDA原子函数

| 操作 | 函数 | 支持的数值类型 |
| :--: | :--: | :------------: |
|      |      |                |
|      |      |                |
|      |      |                |
|      |      |                |
|      |      |                |
|      |      |                |
|      |      |                |
|      |      |                |
|      |      |                |
|      |      |                |
|      |      |                |



#### 原子操作的成本

原子函数在一些应用中很有帮助且很有必要，但可能要付出很高的性能代价。导致这种局面的原因有如下几个方面：

- 当在全局或共享内存中执行原子操作时，能保证所有的数值变化对所有线程都是立即可见的。因此，在最低限度下，一个原子操作指令将通过任何方式进入到全局或共享内存中读取当前存储的数值而不需要缓存。如果原子指令操作成功，那么必须把实际需要的值写入到全局或共享内存中。
- 共享地址冲突的原子访问可能要求发生冲突的线程不断地进行重试，类似于运行多个myAtomicAdd循环的迭代。尽管内置原子函数建立过程的可见性是有限的，但对你所实现的任何自定义原子操作来说都是真实的。 如果你的应用程序反复循环而致使I/O开销较大， 相应地性能会降低。
- 当线程在同一个线程束中时必须执行不同的指令，线程束执行是序列化的。如果一个线程束中的多个线程在相同的内存地址发出一个原子操作，就会产生类似于线程冲突的问题。因为只有一个线程的原子操作可以成功，所以所有其他的线程必须重试。如果一个原子指令需要n个循环，并且需要同一线程束中的t个线程在相同的内存地址上执行该原子指令，那么运行的时间将会是t×n，因为每次重试时只有一个线程会成功。记住，线程束中剩下的那些线程会等待所有原子操作的完成， 并且一个原子操作也意味着一个全局的读取和写入。



# 八、GPU加速库和OpenACC



# 九、多GPU编程

在一个计算节点内或跨多个GPU加速节点实现跨GPU扩展应用。CUDA提供了大量实现多GPU编程的功能，包括：在一个或多个进程中管理多设备，使用统一的虚拟寻址（Unified Virtual Addressing，UVA）直接访问其他设备内存，GPUDirect，以及使用流和异步函数实现的多设备计算通信重叠。 

## 从一个GPU到多GPU

在应用程序中添加对多GPU的支持， 其最常见的原因是以下几个方面：

- 问题域的大小： 现有的数据集太大， 单GPU内存大小与其不相符合
- 吞吐量和效率： 如果单GPU适合处理单任务， 那么可以通过使用多GPU并发地处理多任务来增加应用程序的吞吐量

当使用多GPU运行应用程序时，需要正确设计GPU间的通信。GPU间数据传输的效率取决于GPU是如何连接在一个节点上并跨集群的。在多GPU系统里有两种连接方式：

- 多GPU通过单个节点连接到PCIe总线上
- 多GPU连接到集群中的网络交换机上

这些连接拓扑结构不是互斥的。下图展示了一个集群的简化拓扑结构，其中有两个计算节点。GPU0和GPU1通过PCIe总线连接到node0上。 同样， GPU2和GPU3在通过PCIe总线连接到node1上。两个节点（node0和node1）通过Infiniband交换机互相连接。

<img src="/home/zuo/code/dazuozcy.github.io/img/professional_cuda_c_programming/chap09/simplified-topology-for-a-cluster-with-two-compute-nodes.png" style="zoom:80%;" />

为了设计一个利用多GPU的程序， 需要跨设备分配工作负载。 根据应用程序， 这种分配会导致两种常见的GPU间通信模式：

- 问题分区之间没有必要进行数据交换， 因此在各GPU间没有数据共享
- 问题分区之间有部分数据交换， 在各GPU间需要冗余数据存储

### 在多GPU上执行

CUDA运行时API支持在多GPU系统中管理设备和执行内核的多种方式。单个主机线程可以管理多个设备。 

```cpp
/*
  如何确定使能CUDA设备的数量， 对其进行遍历， 并查询性能
*/
int ngpus;
cudaGetDeviceCount(&ngpus);
for (int i = 0; i < ngpus; i++) {
	cudaDeviceProp devProp;
	cudaGetDeviceProperties(&devProp, i);
	printf("Device %d has compute capability %d.%d.\n", i, devProp.major, devProp.minor);
}    

/*
  在利用与多GPU一起工作的CUDA应用程序时， 必须显式地指定哪个GPU是当前所有CUDA运算的目标。 使用cudaSetDevice函数设置当前设备
  一旦选定了当前设备， 所有的CUDA运算将被应用到那个设备上：
  	a. 任何从主线程中分配来的设备内存将完全地常驻于该设备上
	b. 任何由CUDA运行时函数分配的主机内存都会有与该设备相关的生存时间
	c. 任何由主机线程创建的流或事件都会与该设备相关
	d. 任何由主机线程启动的内核都会在该设备上执行

  cudaSetDevice函数不会与其他设备同步，因此是一个低开销的调用。使用此函数，可以在任何时间从任何主机线程中选择任何设备。
  有效的设备标识符范围是[0,ngpus)。如果在首个CUDA API调用之前，没有显式调用cudaSetDevice，那当前设备会被自动设置设备0.
*/
for (int i = 0; i < ngpus; i++) {
    // set the current device
    cudaSetDevice(i);
    // execute kernel on current device
    kernel<<<grid, block>>>(...);
    // asynchronously transfer data between the host and current device
    cudaMemcpyAsync(...);
}
```



### 点对点通信

在64位应用程序上执行的内核， 可以直接访问连接到同一个PCIe根节点上的任一GPU的全局内存（32位应用程序不支持点对点访问）。点对点通信需要CUDA 4.0或更高版本，相应的GPU驱动器，以及一个具有两个或两个以上**连接到同一个PCIe根节点**上的Fermi或Kepler GPU系统。 有两个由CUDA P2P API支持的模式， 它们允许GPU之间直接通信：

- 点对点访问： 在CUDA内核和GPU间直接加载和存储地址
- 点对点传输： 在GPU间直接复制数据

在一个系统内，如果两个GPU连接到不同的PCIe根节点上，那么不允许直接进行点对点访问。 但仍可以使用CUDA P2P API在这些设备之间进行点对点传输，不过是通过主机内存透明地传输数据， 而不是通过PCIe总线直接传输数据。

#### 启用点对点访问

点对点访问允许各GPU连接到同一个PCIe根节点上， 使其直接引用存储在其他GPU设备内存上的数据。 对于透明的内核， 引用的数据将通过PCIe总线传输到请求的线程上。

因为不是所有的GPU都支持点对点访问， 所以需要使用下述函数显式地检查设备是否支持P2P：

```cpp
// 如果device能够直接访问对等设备peerDevice的全局内存，那么出参canAccessPeer返回值为整型1，否则返回0
cudaError_t cudaDeviceCanAccessPeer(int* canAccessPeer, int device, int peerDevice);
```

在两个设备间， 必须用以下函数显式地启用点对点内存访问：

```cpp
/*
  本函数允许从当前设备到peerDevice进行点对点访问。flag是保留参数，目前必须设为0。一旦成功，该对等设备的内存将立即由当前设备进行访问。
  本函数授权的访问是单向的，即这个函数允许从当前设备到peerDevice的访问，不允许从peerDevice到当前设备的访问。
  如果希望对等设备能直接访问当前设备的内存，则需要另一个方向单独的匹配调用。
*/
cudaError_t cudaDeviceEnablePeerAccess(int peerDevice, unsigned int flag);
```

点对点访问保持启用状态， 直到它被以下函数显式地禁用：

```cpp
cudaError_t cudaDeviceDisablePeerAccess(int peerDevice);
```

#### 点对点内存复制

两个设备之间启用对等访问之后， 使用下面的函数， 可以异步地复制设备上的数据：

```cpp
cudaError_t cudaMemcpyPeerAsync(void* dst, int dstDev, void* src, int srcDev, size_t nBytes, cudaStream_t stream);
```

这个函数将数据从设备的srcDev设备内存传输到设备dstDev的设备内存中。该函数对于主机和所有其他设备来说是异步的。如果srcDev和dstDev共享相同的PCIe根节点，那么数据传输是沿着PCIe最短路径执行的，不需要通过主机内存中转。

### 多GPU间的同步

在第6章中介绍的用于流和事件的CUDA API， 也适用于多GPU应用程序。 每一个流和事件与单一设备相关联。 在多GPU应用程序上可以使用和单GPU应用程序相同的同步函数， 但是必须指定适合的当前设备。 多GPU应用程序中使用流和事件的典型工作流程如下所示：

- 选择这个应用程序将使用的GPU集。
- 为每个设备创建流和事件。
- 为每个设备分配设备资源（如设备内存） 。
- 通过流在每个GPU上启动任务（例如， 数据传输或内核执行） 。
- 使用流和事件来查询和等待任务完成。
- 清空所有设备的资源。

只有与该流相关联的设备是当前设备时，在流中才能启动内核。只有与该流相关联的设备是当前设备时，才可以在流中记录事件。任何时间都可以在任何流中进行内存拷贝，无论该流与什么设备相关联或当前设备是什么。即使流或事件与当前设备不相关， 也可以查询或同步它们。



## 多GPU间细分计算

在本节中将会利用多GPU扩展第2章中的向量加法示例，向量加法是多GPU编程的典型案例， 在问题分区之间不需要交换数据。

### 在多设备上分配内存

```cpp
// 1. 确定在当前系统中有多少可用的GPU
int ngpus;
cudaGetDeviceCount(&ngpus);
printf(" CUDA-capable devices: %i\n", ngpus);

// 2.  接下来就需要为多个设备声明主机内存、设备内存、流和事件
float *d_A[NGPUS], *d_B[NGPUS], *d_C[NGPUS];
float *h_A[NGPUS], *h_B[NGPUS], *hostRef[NGPUS], *gpuRef[NGPUS];
cudaStream_t stream[NGPUS];

// 3. 在向量加法的例子中，元素的总输入大小为16M，所有设备平分，给每个设备isize个元素：
int size = 1 << 24;
int iSize = size / ngpus;
size_t iBytes = iSize * sizeof(float);

// 4. 可以分配主机和设备内存了，为每个设备创建CUDA流， 
for (int i = 0; i < ngpus; i++) {
    cudaSetDevice(i); // set current device
    // allocate device memory
    cudaMalloc((void **) &d_A[i], iBytes);
	cudaMalloc((void **) &d_B[i], iBytes);
    cudaMalloc((void **) &d_C[i], iBytes);
    // allocate page locked host memory for asynchronous data transfer
    cudaMallocHost((void **) &h_A[i], iBytes);
    cudaMallocHost((void **) &h_B[i], iBytes);
    cudaMallocHost((void **) &hostRef[i], iBytes);
    cudaMallocHost((void **) &gpuRef[i], iBytes);
    cudaStreamCreate(&stream[i]); // create streams for timing and synchronizing
}
```



### 单主机线程分配工作

```cpp
// 在设备间分配操作之前，需要为每个设备初始化主机数组的状态
for (int i = 0; i < ngpus; i++) {
    cudaSetDevice(i);
    initialData(h_A[i], iSize);
    initialData(h_B[i], iSize);
}

// distributing the workload across multiple devices
for (int i = 0; i < ngpus; i++) {
    cudaSetDevice(i);
    cudaMemcpyAsync(d_A[i], h_A[i], iBytes, cudaMemcpyHostToDevice, stream[i]);
    cudaMemcpyAsync(d_B[i], h_B[i], iBytes, cudaMemcpyHostToDevice, stream[i]);
    iKernel<<<grid, block, 0, stream[i]>>> (d_A[i], d_B[i], d_C[i], iSize);
    cudaMemcpyAsync(gpuRef[i], d_C[i], iBytes, cudaMemcpyDeviceToHost, stream[i]);
}
cudaDeviceSynchronize();
```

上面第二个for循环遍历多个GPU，为设备异步地复制输入数组。然后在相同的流中操作iSize个数据元素以便启动内核。最后，设备发出的异步拷贝命令，把结果从内核返回到主机。因为所有的函数都是异步的，所以控制会立即返回到主机线程。因此，当任务仍在当前设备上运行时，切换到下一个设备是安全的。

### 编译和执行

使用nvprof可以获得每个设备行为的更多细节：

```shell
nvprof --print-gpu-trace ./simpleMultiGPU
```



## 多GPU上的点对点通信

### 实现点对点访问

首先， 必须对所有设备启用双向点对点访问， 如以下代码所示：

```cpp
/*
* enable P2P memcopy between GPUs(all GPUs must be compute capability 2.0 or later (Fermi or later)).
*/
inline void enableP2P (int ngpus) {
    for( int i = 0; i < ngpus; i++ ) {
        cudaSetDevice(i);
        for(int j = 0; j < ngpus; j++) {
            if(i == j) continue;
            int peer_access_available = 0;
            cudaDeviceCanAccessPeer(&peer_access_available, i, j);
            if (peer_access_available) {
                cudaDeviceEnablePeerAccess(j, 0);
                printf("> GPU%d enabled direct access to GPU%d\n",i,j);
            } else {
            	printf("(%d, %d)\n", i, j);
            }
        }
    }
}
```

函数`enableP2P`遍历所有设备对`(i,j)` ，如果支持点对点访问，则使用`cudaDeviceEnablePeerAccess`函数启用双向点对点访问。

### 点对点的内存复制

启用点对点访问后， 可以在两个设备之间直接复制数据。

启用点对点访问后， 下面的代码在两个设备间执行ping-pong同步内存复制， 次数为一百次。 如果点对点访问在所有设备上都被成功启用了， 那么将直接通过PCIe总线进行数据传输而不用与主机交互。

```cpp
// ping pong unidirectional gmem copy
cudaEventRecord(start, 0);
for (int i = 0; i < 100; i++) {
    if (i % 2 == 0) {
    	cudaMemcpy(d_src[1], d_src[0], iBytes, cudaMemcpyDeviceToDevice);
    } else {
    	cudaMemcpy(d_src[0], d_src[1], iBytes, cudaMemcpyDeviceToDevice);
    }
}
```

### 统一虚拟寻址的点对点内存访问



## 多GPU上的有限查分

在本节中， 通过使用有限差分的方法求解二维波动方程， 将会学习到如何跨设备重叠计算和通信。 



## 跨GPU集群扩展应用程序

与同构系统相比， GPU加速集群被公认为极大地提升了性能效果和节省了计算密集型应用程序的功耗。 当处理超大数据集时， 通常需要多个计算节点来有效地解决问题。MPI（消息传递接口） 是一个标准化和便携式的用于数据通信的API， 它通过分布式进程之间的消息进行数据通信。

