---
layout: post
title: "读薄《Professional CUDA C Programming》——CUDA执行模型"
author: dazuo
date: 2020-07-01 20:19:00 +0800
categories: [CUDA]
tags: [GPU, CUDA]
math: true
mermaid: true
---

# 三、CUDA执行模型

通过第二章的练习，已经了解了如何**在网格和线程块中组织线程**以获得最佳的性能。尽管可通过反复试验找到最佳的执行配置，但可能仍然会感到疑惑，为什么选择这样的执行配置会更好。你可能想知道是否有一些选择网格和块配置的准则。本章将回答这些问题， 并从硬件方面深入介绍内核启动配置和性能分析的信息。

## CUDA执行模型概述

一般来说，执行模型会提供一个操作视图，说明如何在特定的计算架构上执行指令。CUDA执行模型揭示了GPU并行架构的抽象视图，使我们能够据此分析线程并发。CUDA执行模型能够提供有助于在指令吞吐量和内存访问方面编写高效代码的见解。

### GPU架构概述

`Streaming Multiprocessors (SM)`，流式多处理器。GPU实际上是一个SM的阵列，每个SM包含N个计算核，能支持数百个线程并发执行。

下图说明了`Fermi SM`的关键组件：

- CUDA核心
- 寄存器文件
- 共享内存/一级缓存
- 加载/存储单元
- 特殊功能单元
- 线程束调度器

![cuda_exe_flow](../../img/professional_cuda_c_programming/chap03/fermi-sm.png){: width="1086" height="542"}

当启动一个kernel网格时， 它的线程块被分布在了可用的SM上来执行。 一个线程块只能在一个SM上被调度。 一旦线程块在一个SM上被调度， 就会保存在该SM上直到执行完成。 在同一时间， 一个SM可以容纳多个线程块，即多个线程块可能会被分配到同一SM上， 而且是根据SM资源的可用性进行调度的。

CUDA采用**单指令多线程**(**SIMT**) 架构来管理和执行线程，每**32**个线程为一组， 被称为**线程束**(`warp`)。线程束中的所有线程同时执行相同的指令。 每个线程都有自己的指令地址计数器和寄存器状态， 利用本线程自己的数据执行当前的指令。

> SIMT vs. SMID

两者都是将相同的指令广播给多个执行单元来实现并行。一个关键的区别是`SIMD`要求同一向量中的所有元素要在一个统一的同步组中一起执行，而`SIMT`允许同一线程束的多个线程独立执行。尽管一个线程束中的所有线程从相同的程序地址同时开始执行，但是单独的线程仍可能有不同的行为。 `SIMT`让你可以为独立的标量线程编写线程级并行代码，甚至为互相协作的线程编写数据并行的代码。

`SIMT`模型包含3个`SIMD`所不具备的关键特征：

- 每个线程有自己的指令地址计数器

- 每个线程有自己的寄存器状态

- 每个线程可以有一个独立的执行路径



寄存器和共享内存是SM中的稀缺资源。这些有限的资源限制了在SM上活跃的线程束数量，活跃的线程束数量对应于SM上的并行量。

### Fermi架构

`Fermi`架构是第一个完整的GPU计算架构。下图所示为`Fermi`架构的逻辑框图。

![cuda_exe_flow](../../img/professional_cuda_c_programming/chap03/gpu-sm.png){: width="1086" height="542"}

CUDA核心被组织到`16`个SM中， 每一个SM含有`32`个CUDA核心。

每个CUDA核心都有一个全流水线的整数算术逻辑单元(ALU) 和一个浮点运算单元(FPU) ，在这里每个时钟周期执行一个整数或是浮点数指令。 

`Fermi`架构有6个384位的`GDDR5 DRAM`存储器接口，支持多达`6GB`的全局机载内存。

`GigaThread`引擎是一个全局调度器，用来分配线程块到SM线程束调度器上。

`Fermi`架构的一个关键特征是有一个`64KB`的片内可配置存储器，它在共享内存与一级缓存之间进行分配。



### Kepler架构

`Kepler` GPU架构是一种快速、高效、高性能的计算架构。`Kepler`的特点使得混合计算更容易。Kepler架构的3个重要的创新。

- **强化的SM**

   每个Kepler SM单元包含192个单精度CUDA核心，64个双精度单元，32个特殊功能单元（SFU） 以及32个加载/存储单元（LD/ST） 

- **动态并行**

  动态并行是Kepler GPU的一个新特性，它允许GPU动态启动新的网格。有了动态并行，任一内核都能启动其他的内核， 并且管理任何核间需要的依赖关系来正确地执行附加的工作，消除了与CPU通信的需求。动态并行拓宽了GPU在各种学科上的适用性。 

  ![cuda_exe_flow](../../img/professional_cuda_c_programming/chap03/dynamic-parallelism.png){: width="1086" height="542"}

- **`Hyper-Q`技术**

  Fermi GPU依赖一个单一的硬件工作队列来从CPU到GPU间传送任务，这可能会导致一个单独的任务阻塞队列中在该任务之后的所有其他任务。`Kepler `Hyper-Q`消除了这个限制。 如下图所示，Kepler GPU在主机与GPU之间提供了32个硬件工作队列。`Hyper-Q`保证了在GPU上有更多的并发执行，最大限度地提高了GPU的利用，也减少了CPU的闲置时间，提高了整体的性能。

  ![cuda_exe_flow](../../img/professional_cuda_c_programming/chap03/hyper-q.png){: width="1086" height="542"}



### Profile驱动的优化

性能分析工具深入洞察内核的性能， 检测核函数中影响性能的瓶颈。 CUDA提供了两个主要的性能分析工具： nvvp和nvprof。

- **`nvvp`**

  独立的可视化分析器，它可以可视化并优化CUDA程序的性能。该工具会显示CPU与GPU上程序活动的时间表，从而找到可以改善性能的点。此外nvvp可以分析应用程序潜在的性能瓶颈，并给出消除或减少这些瓶颈的建议。该工具既可作为一个独立的应用程序，也可作为`Nsight Eclipse Edition(nsight)`的一部分。

- **`nvprof`**

  在命令行上收集和显示分析数据。`nvprof`是和CUDA 5一起发布的，它是从一个旧的命令行CUDA分析工具进化而来的。跟`nvvp`一样，它可获得CPU与GPU上CUDA关联活动的时间表，其中包括内核执行、内存传输和CUDA的API调用。它也可以获得硬件计数器和CUDA内核的性能指标。

有3种常见的**限制内核性能**的因素：

- 存储带宽
- 计算资源
- 指令和内存延迟



## 线程束执行的本质

启动内核时， 内核中所有的线程似乎都是并行地运行的，在逻辑上这是正确的， 但从硬件的角度来看， 不是所有线程在物理上都可以同时并行地执行。

### 线程束和线程块

线程束是`SM`中的基本执行单元。当一个`Grid`被启动后，`Grid`中的线程块分布在`SM`中。一旦线程块被调度到一个`SM`上，线程块中的线程会被进一步划分为**线程束**。一个线程束由`32`个连续的线程组成，一个线程束中所有的线程按照`SIMT`方式执行，即所有线程都执行相同的指令，每个线程在**私有数据**上进行操作。

如果线程块的大小不是线程束大小的偶数倍，那么最后的线程束里有些线程就不会活跃，即这些线程未被使用，但它们仍然消耗`SM`的资源，如寄存器。比如，某个线程块有`80`个线程，那么硬件会为这个线程块配置`80/32=3`个线程束，最后一个线程束的最后`16`个线程不会被使用。

### 线程束分化

一个线程束中的所有线程在同一周期中必须执行相同的指令，如果一个线程执行一条指令，那么线程束中的所有线程都必须执行该指令。如果在同一线程束中的线程使用不同的路径通过同一个应用程序，这可能会产生问题。一半的线程束需要执行if语句块中的指令，而另一半需要执行else语句块中的指令。在同一线程束中的线程执行不同的指令， 被称为**线程束分化**。

 线程束分化会导致性能明显地下降。条件分支越多，并行性削弱越严重。

下面这个核函数是会产生线程束分化的例子，在同一个线程束里，不同线程有不同的路径。

```cpp
__global__ void mathKernel1(float *c) {
	int tid = blockIdx.x * blockDim.x + threadIdx.x;
	float a, b;
	a = b = 0.0f;
	if (tid % 2 == 0) {
		a = 100.0f;
    } else {
		b = 200.0f;
	}
	c[tid] = a + b;
}
```

下面这个核函数由于是按照线程束为粒度，所以不会产生线程束分化。

```cpp
__global__ void mathKernel2(void) {
	int tid = blockIdx.x * blockDim.x + threadIdx.x;
	float a, b;
	a = b = 0.0f;
	if ((tid / warpSize) % 2 == 0) {
		a = 100.0f;
	} else {
		b = 200.0f;
	}
	c[tid] = a + b;
}
```



### 资源分配

线程束的执行上下文主要由以下资源组成：

- 程序计数器

- 寄存器

- 共享内存



 由于计算资源是在线程束之间进行分配的， 而且在线程束的整个生存期中都保持在芯片内， 因此线程束上下文的切换是非常快的。



对于一个给定的内核， 同时存在于同一个SM中的线程块和线程束的数量取决于在SM中可用的且内核所需的寄存器和共享内存的数量。



资源可用性通常会限制SM中常驻线程块的数量。如果每个SM没有足够的寄存器或共享内存去处理至少一个块， 那么内核将无法启动。



当计算资源已分配给线程块时， 线程块被称为**活跃的块**。 它所包含的线程束被称为**活跃的线程束**。 活跃的线程束可以进一步被分为以下3种类型：



- 选定的线程束：活跃执行的线程束

- 阻塞的线程束：活跃的准备执行但尚未执行的线程束

- 符合条件的线程束：没有做好执行准备的线程束



如果同时满足以下两个条件则线程束符合执行条件：



- 32个CUDA核心可用于执行

- 当前指令中所有的参数都已就绪



计算资源限制了活跃的线程束的数量。 为了最大程度地利用GPU， 需要最大化活跃的线程束数量。



### 延迟隐藏





**利用率**与常驻线程束的数量直接相关。当每个时钟周期中所有的线程调度器都有一个符合条件的线程束时， 可以达到计算资源的完全利用。



指令可以被分为两种基本类型：

- 算术指令

- 内存指令



>  指令延迟： 在指令发出和完成之间的时钟周期



算术指令延迟是一个算术操作从开始到它产生输出之间的时间。 内存指令延迟是指发送出的加载或存储操作和数据到达目的地之间的时间。



### 占用率

占用率是每个SM中活跃的线程束占最大线程束数量的比值。

网格和线程块大小的准则：

- 保持每个块中线程数量是线程束大小（32） 的倍数

- 避免块太小： 每个块至少要有128或256个线程

- 根据内核资源的需求调整块大小

- 块的数量要远远多于SM的数量， 从而在设备中可以显示有足够的并行

- 通过实验得到最佳执行配置和资源使用情况



占用率唯一注重的是在每个SM中并发线程或线程束的数量。 然而， 充分的占用率不是性能优化的唯一目标。 内核一旦达到一定级别的占用率， 进一步增加占用率可能不会改进性能。 



### 同步

在CUDA中， 同步可以在两个级别执行：



- 系统级： 等待主机和设备完成所有的工作

对于主机来说， 许多CUDA API调用和所有的内核启动不是同步的，cudaDeviceSynchronize()可用来阻塞主机应用程序， 直到所有的CUDA操作（复制、核函数等） 完成。



- 块级： 在设备执行过程中等待一个线程块中所有线程到达同一点

当__syncthreads被调用时， 同一个线程块中的每个线程都必须等待直至该线程块中所有其他线程都已经达到这个同步点。在栅栏之前所有线程产生的所有全局内存和共享内存访问， 将会在栅栏后对线程块中所有其他的线程可见。 该函数可以协调同一个块中线程之间的通信， 但它强制线程束空闲， 从而可能对性能产生负面影响。



线程块中的线程可以通过共享内存和寄存器来共享数据。



在不同的块之间不允许线程同步。 块间同步， 唯一安全的方法是在每个内核执行结束端使用全局同步点； 也就是说， 在全局同步之后， 终止当前的核函数， 开始执行新的核函数。



### 可扩展性

一个可扩展的并行程序可以高效地使用所有的计算资源以提高性能。 可扩展性意味着增加的计算核心可以提高性能。  例如， 若一个CUDA程序在两个SM中是可扩展的， 则与在一个SM中运行相比， 在两个SM中运行会使运行时间减半。



能够在可变数量的计算核心上执行相同的应用程序代码的能力被称为**透明可扩展性**。



CUDA内核启动时， 线程块分布在多个SM中。 网格中的线程块以并行或连续或任意的顺序被执行。 线程块执行的独立性质使得CUDA编程在任意数量的核心中都是可扩展的。独立性是基于不允许跨线程块同步， 线程块可以以任何顺序、 并行、 串行的顺序在任何SM上执行。 



## 并行性的表现

### 用nvprof检测活跃的线程束

一个内核的可实现占用率被定义为： 每周期内活跃线程束的平均数量与一个SM支持的线程束最大数量的比值。 



可通过命令`nvprof --metrics achieved_occupancy xxx_exe`检测。



更高的占用率并不一定意味着有更高的性能。



### 用nvprof检测内存操作





可通过命令`nvprof --metrics gld_efficiency xxx_exe`检测。



可通过命令`nvprof --metrics gld_throughput xxx_exe`检测。



 更高的加载吞吐量并不一定意味着更高的性能。



值得注意的是， 最好的执行配置既不具有最高的可实现占用率， 也不具有最高的加载吞吐量。





## 避免分支分化





 通过重新组织数据的获取模式， 可以减少或避免线程束分化。



### 并行规约问题





在向量中执行满足交换律和结合律的运算， 被称为**归约问题**。 并行归约问题是这种运算的并行执行。 并行归约是一种常见的并行模式， 并且是许多并行算法中的一个关键运算。



根据每次迭代后**输出元素就地存储的位置**， 成对的并行求和实现可以被进一步分为以下两种类型：



\- 相邻配对： 元素与它们直接相邻的元素配对。如下左图所示

\- 交错配对： 根据给定的跨度配对元素。如下有图所示



<!-- <center class = "half">

<img src="https://github.com/dazuozcy/dazuozcy.github.io/blob/master/img/professional_cuda_c_programming/chap03/neighbored-pair.png" alt="neighbored-pair" style="zoom:85%;" /> <img src="https://github.com/dazuozcy/dazuozcy.github.io/blob/master/img/professional_cuda_c_programming/chap03/Interleaved-pair.png" style="zoom:85%;" />

</center> -->





### 并行规约中的分化及改善





<!-- <center class = "half">

<img src="https://github.com/dazuozcy/dazuozcy.github.io/blob/master/img/professional_cuda_c_programming/chap03/neighbored-pair-impl1.png" alt="neighbored-pair" style="zoom:67%;" /> <img src="https://github.com/dazuozcy/dazuozcy.github.io/blob/master/img/professional_cuda_c_programming/chap03/neighbored-pair-impl2.png" style="zoom:67%;" />

</center> -->



在上面左图的实现中，只有ID为偶数的线程执行这个条件语句的主体， 但是所有的线程都必须被调度。 在第二次迭代中， 只有四分之一的线程是活跃的， 但是所有的线程仍然都必须被调度。 通过重新组织每个线程的数组索引来强制ID相邻的线程执行求和操作， 线程束分化就能被归约了，如上面有图所示。



### 交错配对的规约





交错实现比第一个实现（上图中左图的实现）快了1.69倍， 比第二个实现（上图中右图的实现）快了1.34倍。 这种性能的提升主要是由kernel函数里的全局内存加载/存储模式导致的。





## 循环展开



循环体的复制数量被称为循环展开因子， 迭代次数就变为了原始循环迭代次数除以循环展开因子。



在CUDA中， 循环展开的意义非常重大。 目标仍然是相同的： 通过减少指令消耗和增加更多的独立调度指令来提高性能。 因此， 更多的并发操作被添加到流水线上， 以产生更高的指令和内存带宽。 这为线程束调度器提供更多符合条件的线程束， 它们可以帮助隐藏指令或内存延迟。





>  当在CUDA中展开循环、 数据块或线程束时， 可以提高性能的两个主要原因是什么？ 解释每种展开是如何提升指令吞吐量的。



Unrolling loops, data blocks, or warps can lead to less frequent branching from fewer loop conditionals. Additionally, unrolling can lead to an increase in the number of independent memory operations discoverable by the compiler. As a result, more concurrent read and write operations can be issued and memory bandwidth utilization will increase. 



## 动态并行

CUDA的动态并行允许在GPU端直接创建和同步新的GPU内核。这使得可以在一个核函数中任意点动态增加GPU应用程序的并行性。



动态并行提供了一个更有层次结构的方法， 在这个方法中， 并发性可以在一个GPU内核的多个级别中表现出来。



使用动态并行可以让递归算法更加清晰易懂， 也更容易理解。



有了动态并行， 可推迟到运行时决定要在GPU上创建多少个块和网格， 可以动态地利用GPU硬件调度器和加载平衡器， 并进行调整以适应数据驱动或工作负载。



在GPU端直接创建工作的能力可以减少在主机和设备之间传输执行控制和数据的需求。
