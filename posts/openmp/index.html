<!DOCTYPE html><html lang="en-US" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="openMP" /><meta name="author" content="dazuo" /><meta property="og:locale" content="en_US" /><meta name="description" content="Intro" /><meta property="og:description" content="Intro" /><link rel="canonical" href="https://dazuozcy.github.io/posts/openmp/" /><meta property="og:url" content="https://dazuozcy.github.io/posts/openmp/" /><meta property="og:site_name" content="zuo" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-07-03T20:19:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="openMP" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"dazuo"},"dateModified":"2022-10-31T22:37:31+08:00","datePublished":"2020-07-03T20:19:00+08:00","description":"Intro","headline":"openMP","mainEntityOfPage":{"@type":"WebPage","@id":"https://dazuozcy.github.io/posts/openmp/"},"url":"https://dazuozcy.github.io/posts/openmp/"}</script><title>openMP | zuo</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="zuo"><meta name="application-name" content="zuo"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" as="script"> <script async src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://cdn.jsdelivr.net/gh/cotes2020/chirpy-images/commons/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">zuo</a></div><div class="site-subtitle font-italic">感谢永远有歌把心境道破</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/dazuozcy" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['dazuozcy','163.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>openMP</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>openMP</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> dazuo </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Fri, Jul 3, 2020, 8:19 PM +0800" prep="on" > Jul 3, 2020 <i class="unloaded">2020-07-03T20:19:00+08:00</i> </span></div><div> <span> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Oct 31, 2022, 10:37 PM +0800" prefix="Updated " > Oct 31, 2022 <i class="unloaded">2022-10-31T22:37:31+08:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2225 words">12 min</span></div></div><div class="post-content"><h1 id="intro">Intro</h1><p>OpenMP是共享内存体系结构上的一个基于<strong>多线程</strong>的<strong>并行</strong>编程模型，适用于SMP共享内存多处理系统和多核处理器体系结构。支持C/C++/Fortran.</p><p>OpenMP由三部分组成</p><ul><li>编译器指令(compiler directives)<li>运行时库程序(runtime routines)<li>环境变量(environment variables)</ul><h1 id="openmps-machine-model">OpenMP’s machine model</h1><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/openmp-machine-model.png?raw=true" alt="OpenMP's machine model" width="1086" height="542" /></p><h3 id="openmp-shared-memory-parallel-programming-model">OpenMP: Shared-Memory Parallel Programming Model</h3><h3 id="all-processorscores-access-a-shared-main-memory">All processors/cores access a shared main memory.</h3><h3 id="parallelization-in-opemmp-employs-multiple-threads">Parallelization in OpemMP employs multiple threads.</h3><h1 id="openmps-memory-model">OpenMP’s memory model</h1><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/openmp-memory-model.png?raw=true" alt="OpenMP's memory model" width="1086" height="542" /></p><ul><li>All threads have access to the same, globally shared memory.<li>Data in private memory is only accessible by the thread owning the memory.<li>No other thread sees the changes in private memory.<li>Data Transfer is through shared memory and 100% transparent to the application.</ul><h1 id="openmps-execution-model">OpenMP’s Execution model</h1><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 686 362'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/openmp-execution-model.png?raw=true" alt="OpenMP's Execution model" width="686" height="362" /></p><ul><li><p>OpenMP programs starts with just one thread: The Master Thread. It’s used as an Initial Thread.</p><li>Worker threads are spawned at Paralllel Region, together with the Master they form the team of threads.<li>In between Parallel Regions the Worker Threads are put to sleep. The OpenMP Runtime takes care of all thead management work.</ul><p>Concept: fork-join model. The fundamental model behind OpenMP. It Allows for an incremental parallelization.</p><h1 id="parallel-region-and-structured-blocks">Parallel Region and Structured Blocks</h1><h3 id="the-parallelism-has-to-be-expressed-explictly">The parallelism has to be expressed explictly</h3><div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="cp">#pragma omp parallel
</span><span class="p">{</span>
    <span class="p">...</span>
    <span class="n">structured</span> <span class="n">block</span>
    <span class="p">...</span>
<span class="p">}</span>
</pre></table></code></div></div><h3 id="structured-block">structured block</h3><ul><li>exactly one entry point at the top<li>exactly one exit point at the bottom<li>Branching in or out is not allowed<li>terminating the program is allowed(abort/exit)</ul><h3 id="specification-if-num-of-threads">specification if num of threads</h3><ul><li>Environment variable:<div class="language-bash highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>  <span class="nb">export </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>...
</pre></table></code></div></div><li><p>via num_threads clause</p><p>add <code class="language-plaintext highlighter-rouge">num_threads</code> to the parallel construct</p></ul><h1 id="worksharing"><code class="language-plaintext highlighter-rouge">Worksharing</code></h1><ul><li>Loop construct<li>Sections/Section construct<li>Single Construct<li>Task Construct</ul><h3 id="if-only-the-parallel-construct-is-used-each-thread-executes-the-structured-block">If only the <code class="language-plaintext highlighter-rouge">parallel</code> construct is used, each thread executes the structured block.</h3><h3 id="program-speedup-worksharing">Program speedup: <code class="language-plaintext highlighter-rouge">Worksharing</code></h3><h3 id="openmps-most-common-worksharing-construct-for">OpenMP’s most common <code class="language-plaintext highlighter-rouge">Worksharing</code> construct: for</h3><div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
</pre><td class="rouge-code"><pre><span class="kt">int</span> <span class="n">i</span><span class="p">;</span>

<span class="c1">// Sequential code</span>
<span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>

<span class="c1">// OpenMP parallel region</span>
<span class="cp">#pragma omp parallel
</span><span class="p">{</span>
    <span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">Nthrds</span><span class="p">,</span> <span class="n">istart</span><span class="p">,</span> <span class="n">iend</span><span class="p">;</span>
    <span class="n">id</span> <span class="o">=</span> <span class="n">omp_get_thread_num</span><span class="p">();</span>
    <span class="n">Nthrds</span> <span class="o">=</span> <span class="n">omp_get_num_threads</span><span class="p">();</span>
    <span class="n">istart</span> <span class="o">=</span> <span class="n">id</span> <span class="o">*</span> <span class="n">N</span> <span class="o">/</span> <span class="n">Nthrds</span><span class="p">;</span>
    <span class="n">iend</span> <span class="o">=</span> <span class="p">(</span><span class="n">id</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span> <span class="o">/</span> <span class="n">Nthrds</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">id</span> <span class="o">==</span> <span class="n">Nthrds</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">iend</span> <span class="o">=</span> <span class="n">N</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="n">istart</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">iend</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="c1">// OpenMp parallel region and </span>
<span class="c1">// a worksharing for construct</span>
<span class="cp">#pragma omp parallel
#pragma omp for
</span><span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
</pre></table></code></div></div><h4 id="distrubution-of-loop-iterations-over-all-threads-in-a-team">distrubution of loop iterations over all threads in a Team.</h4><h4 id="scheduling-of-the-distrubution-can-be-influenced">Scheduling of the distrubution can be influenced.</h4><h3 id="loops-often-account-for-most-of-a-programs-runtime">Loops often account for most of a program’s runtime.</h3><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/workingsharing.png?raw=true" alt="Workingsharing illustration" width="1086" height="542" /></p><h3 id="influencing-the-for-loop-scheduling">influencing the for loop scheduling</h3><h4 id="for-construct-openmp-allows-to-influence-how-the-iterations-are-scheduled-among-the-threads-of-the-team-via-the-schedule-clause">for-construct: OpenMP allows to influence how the iterations are scheduled among the threads of the team, via the schedule clause.</h4><ul><li>schedule(static [, chunk]): Iteration space divided into blocks of chunk size, blocks are assigned to threads in a round-robin fasion. If chunk is not specified: #threads blocks.<li>schedule(dynamic [, chunk]): Iteration space divided into blocks of chunk(not specified: 1) size, blocks are scheduled to threads in the order in which threads finish previous blocks.<li>schedule(guided [, chunk]):Similar to dynamic, but block size starts with implementation-defined value, then is decreased exponentially down to chunk.<li>schedule(runtime): Schedule and chunk size taken from the OMP_SCHEDULE environment variable(or the runtime library)<li>schedule(auto) schedule is left up to the runtime to choose(does not have to be any of the above)</ul><h4 id="when-to-use-the-schedule-clause">When to use the schedule clause</h4><ul><li><p>static</p><p>Pre-determined and predictable by the programmer. Least work at runtime, Scheduling done at compile time.</p><li><p>dynamic</p><p>Unpredictable, highly variable work per iteration. Most work at runtime. Complex scheduling logic used at runtime.</p></ul><p>Default on most implementation is shcedule(static).</p><h3 id="critical-region">Critical Region</h3><p>A Critical Region is executed by all threads, but by only one thread simultaneously(Mutual Exclusion)</p><div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="kt">int</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="cp">#pragma omp parallel for
</span><span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">100</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="cp">#pragma omp critical
</span>    <span class="p">{</span> <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">];</span> <span class="p">}</span>
<span class="p">}</span>
</pre></table></code></div></div><h3 id="the-barrier-construct">The Barrier Construct</h3><p>Threads wait until all the threads of the current Team have reached the barrier.</p><div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="cp">#pragma omp barrier
</span></pre></table></code></div></div><p>All worksharing constructs contain an implict barrier at the end.</p><h3 id="the-single-construct">The Single Construct</h3><div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="cp">#pragma omp single [clause]
</span><span class="p">...</span> <span class="n">structured</span> <span class="n">block</span> <span class="p">...</span>
</pre></table></code></div></div><ul><li><p>The single construct specifies that the enclosed structured block is executed by only one thread of them.(It’s up to the runtime which thread that is)</p><li><p>Useful for:</p><ul><li>I/O<li>Memory allocation and deallocation, etc.</ul></ul><h3 id="the-master-construct">The master Construct</h3><div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="cp">#pragma omp master[clause]
</span><span class="p">...</span> <span class="n">structured</span> <span class="n">block</span> <span class="p">...</span>
</pre></table></code></div></div><ul><li><p>The master construct specifies that the enclosed structured block is executed only by the master thread of team.</p><li><p>Note: The master construct is no worksharing construct ans does not contain an implicit barrier at the end.</p></ul><h3 id="the-sectionssection-construct">The sections/section Construct</h3><div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="cp">#pragma omp parallel
</span><span class="p">{</span>
    <span class="cp">#pragma omp sections
</span>    <span class="p">{</span>
    <span class="cp">#pragma omp section
</span>    <span class="n">x_calculation</span><span class="p">();</span>
    <span class="cp">#pragma omp section
</span>    <span class="n">y_calculation</span><span class="p">();</span>
    <span class="cp">#pragma omp section
</span>    <span class="n">z_calculation</span><span class="p">();</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></table></code></div></div><h1 id="false-sharing">False Sharing</h1><h2 id="caches">Caches</h2><h3 id="cpu-is-fast">CPU is fast</h3><ul><li>Order of 3.0 GHz<h3 id="caches-1">Caches</h3><li>Fast, but expensive<li>small, order of MB</ul><h3 id="memory-is-low">Memory is low</h3><ul><li>Order of 0.3 GHz<li>Large, order of GB</ul><h3 id="thus-a-good-utilization-if-caches-is-crutial-for-good-performance-of-hpc-applications">Thus, a good utilization if caches is crutial for good performance of HPC applications!</h3><h2 id="data-in-caches">Data in Caches</h2><h3 id="when-data-is-used-it-is-copied-into-caches">when data is used, it is copied into caches.</h3><h3 id="the-hardware-always-copies-chunks-into-the-cache-so-called-cache-lines">The hardware always copies chunks into the cache, so called cache-lines.</h3><h3 id="this-is-useful-when">This is useful, when:</h3><ul><li>the data isc used frequently(temporal locality)<li>consecutive data is used which is on the same cache-line(spatial locality)</ul><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/caches.png?raw=true" alt="caches" width="1086" height="542" /></p><h2 id="false-sharing-1">False Sharing</h2><h3 id="false-sharing-occurs-when">False Sharing occurs when</h3><ul><li>different threads use elements of the same cache-line<li>one of the threads writes to the cache-line each update will cause the cache lines to “slosh back and forth” between threads.</ul><h3 id="as-a-result-the-cache-line-is-moved-between-the-threads-although-there-is-no-real-data-dependency">As a result, the cache line is moved between the threads, although there is no real data dependency.</h3><h3 id="note-false-sharing-is-a-performance-problem-not-a-correctness-issue">Note: False sharing is a performance problem, not a correctness issue.</h3><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/false-sharing.png?raw=true" alt="False Sharing" width="1086" height="542" /></p><div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="kt">double</span> <span class="n">s_priv</span><span class="p">[</span><span class="n">nthreads</span><span class="p">];</span>
<span class="cp">#pragma omp parallel num_threads(nthreads)
</span><span class="p">{</span>
    <span class="kt">int</span> <span class="n">t</span> <span class="o">=</span> <span class="n">omp_get_thread_num</span><span class="p">();</span>
    <span class="cp">#pragame omp for
</span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">99</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">s_priv</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="p">}</span>
<span class="p">}</span> <span class="c1">// end parallel</span>

<span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">nthreads</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">s_priv</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
</pre></table></code></div></div><h3 id="no-performance-benefit-for-more-threads">No performance benefit for more threads!</h3><ul><li>Reason: false sharing of s_priv<li>Solution: padding, so that only one variable per cache line is used.</ul><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/pad-cacheline.png?raw=true" alt="False Sharing" width="1086" height="542" /></p><div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre><span class="cm">/* Fasle sharing avoided */</span>
<span class="c1">// *8是为了保证不同线程不访问同一cacheline</span>
<span class="kt">double</span> <span class="n">s_priv</span><span class="p">[</span><span class="n">nthreads</span> <span class="o">*</span> <span class="mi">8</span><span class="p">];</span>
<span class="cp">#pragma omp parallel num_threads(nthreads)
</span><span class="p">{</span>
    <span class="kt">int</span> <span class="n">t</span> <span class="o">=</span> <span class="n">omp_get_thread_num</span><span class="p">();</span>
    <span class="cp">#pragame omp for
</span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">99</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">s_priv</span><span class="p">[</span><span class="n">t</span> <span class="o">*</span> <span class="mi">8</span><span class="p">]</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="p">}</span>
<span class="p">}</span> <span class="c1">// end parallel</span>

<span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">nthreads</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">s_priv</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="mi">8</span><span class="p">];</span>
<span class="p">}</span>
</pre></table></code></div></div><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/perf-on-threads.png?raw=true" alt="False Sharing" width="1086" height="542" /></p><h2 id="race-condition">Race Condition</h2><p>threads communicate by sharing variables, unintended sharing of data causes race conditions.</p><h3 id="data-race-a-typical-openmp-programming-error-when">Data Race: a typical OpenMP programming error, when:</h3><ul><li>two or more threads access the same memory location, and<li>at least one of the accessed is a write, and<li>the accesses are not protected by locks or critical regions, and<li>the accesses are not synchronized, e.g. by a barrier.</ul><h3 id="non-deterministic-occurrence-eg-the-sequence-of-the-execution-of-parallel-loop-iteratins-is-non-deterministic-and-may-change-from-run-to-run">Non-deterministic occurrence: e.g. the sequence of the execution of parallel loop iteratins is non-deterministic and may change from run to run.</h3><h3 id="in-many-cases-private-clauses-barriers-or-critical-regions-are-missing">In many cases private clauses, barriers or critical regions are missing.</h3><h3 id="data-races-are-hard-to-find-using-a-traditional-debugger">Data races are hard to find using a traditional debugger.</h3><ul><li>Use tools like Intel Inspector XE ThreadSanitizer, Archer. <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/inspector-XE.png?raw=true" alt="inspector XE" width="1086" height="542" /></ul><div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre><span class="cm">/* Example of Calc Pi */</span>
<span class="kt">double</span> <span class="nf">f</span><span class="p">(</span><span class="n">dpuble</span> <span class="n">x</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">4.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span><span class="p">));</span>
<span class="p">}</span>

<span class="kt">double</span> <span class="nf">CalcPi</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">const</span> <span class="kt">double</span> <span class="n">fH</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="kt">double</span><span class="p">)</span> <span class="n">n</span><span class="p">;</span>
    <span class="kt">double</span> <span class="n">fSum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
    <span class="kt">double</span> <span class="n">fX</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">i</span><span class="p">;</span>

<span class="cp">#pragma omp parallel for private(fX, i, fSum)
</span>    <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">fX</span> <span class="o">=</span> <span class="n">fH</span> <span class="o">*</span> <span class="p">((</span><span class="kt">double</span><span class="p">)</span><span class="n">i</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">);</span>
        <span class="n">fSum</span> <span class="o">+=</span> <span class="n">f</span><span class="p">(</span><span class="n">fX</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">fH</span> <span class="o">*</span> <span class="n">fSum</span><span class="p">;</span>
<span class="p">}</span>
</pre></table></code></div></div><h2 id="atomic-directive"><code class="language-plaintext highlighter-rouge">atomic</code> directive</h2><p>The statements inside the atomic must be one of the following forms:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre>x binop = expr
   x++
   ++x
   x--
   --x

x is an lvalue of scalar type and binop is a non-overloaded built in operator.
</pre></table></code></div></div><div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="cp">#pragma omp parallel
</span><span class="p">{</span>
    <span class="kt">double</span> <span class="n">tmp</span><span class="p">,</span> <span class="n">B</span><span class="p">;</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">DOIT</span><span class="p">();</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">big_ugly</span><span class="p">(</span><span class="n">B</span><span class="p">);</span>

<span class="cp">#pragma omp atomic
</span>    <span class="n">X</span> <span class="o">+=</span> <span class="n">tmp</span><span class="p">;</span>
<span class="p">}</span>
</pre></table></code></div></div><h2 id="the-barrier-directive">The <code class="language-plaintext highlighter-rouge">barrier</code> directive</h2><h3 id="all-tasks-created-by-any-thread-of-the-current-team-are-guaranteed-to-be-completed-at-barrier-exit">All tasks created by any thread of the current <code class="language-plaintext highlighter-rouge">Team</code> are guaranteed to be completed at barrier exit.</h3><h2 id="taskwait-directive"><code class="language-plaintext highlighter-rouge">taskwait</code> directive</h2><h3 id="a-stand-alone-directive">A stand-alone directive</h3><div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="cp">#pragma omp taskwait
</span></pre></table></code></div></div><h3 id="wait-on-the-completion-of-child-tasks-of-the-current-task-just-direct-children-not-all-descendant-task-includes-an-implicit-task-scheduling-pointtsp">wait on the completion of child tasks of the current task; just direct children, not all descendant task; includes an implicit task scheduling point(TSP)</h3><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/taskwait.png?raw=true" alt="taskwait" width="1086" height="542" /></p><h2 id="taskgroup-directive"><code class="language-plaintext highlighter-rouge">taskgroup</code> directive</h2><h3 id="attached-to-a-structured-block-completion-of-all-descendants-of-the-current-task-tsp-at-the-end">attached to a structured block; completion of all descendants of the current task; TSP at the end</h3><div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="cp">#pragma omp taskgroup [clause[[,] clause]...]
</span><span class="p">{</span><span class="n">structured</span><span class="o">-</span><span class="n">block</span><span class="p">}</span>
</pre></table></code></div></div><h3 id="where-clausecould-only-be-reductionreduction-identifierlist-items">where clause(could only be): reduction(reduction-identifier:list-items)</h3><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/taskgroup.png?raw=true" alt="taskgroup" width="1086" height="542" /></p><h2 id="task-synchronization-explained">Task Synchronization explained</h2><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/task-sync-explaination.png?raw=true" alt="task-sync-explaination" width="1086" height="542" /></p><h1 id="loops-with-tasks">Loops with Tasks</h1><h2 id="the-taskloop-construct">The taskloop Construct</h2><h3 id="task-generating-construct-decompose-a-loop-into-chunks-create-a-task-for-each-loop-chunk">Task generating construct: decompose a loop into chunks, create a task for each loop chunk</h3><div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="cp">#pragma omp taskloop [clause[[,] clause]...]
</span><span class="p">{</span><span class="n">structured</span><span class="o">-</span><span class="k">for</span><span class="o">-</span><span class="n">loops</span><span class="p">}</span>
</pre></table></code></div></div><p>clause is one of: <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/taskloop-construct.png?raw=true" alt="taskloop-construct" width="1086" height="542" /></p><h2 id="taskloop-decomposition-approaches">Taskloop decomposition approaches</h2><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/taskloop-decom-approaches.png?raw=true" alt="taskloop-decom-approaches" width="1086" height="542" /></p><h3 id="if-none-of-previous-clauses-is-present-the-number-of-chunks-and-the-number-of-iterations-per-chunk-is-implementation-defined">if none of previous clauses is present, the number of chunks and the number of iterations per chunk is implementation defined.</h3><h3 id="additional-considerations">Additional considerations:</h3><ul><li>The order of the creation of the loop tasks is unspecified<li>Taskloop creates an implicit taskgroup region; <code class="language-plaintext highlighter-rouge">nogroup</code> -&gt; no implicit taskgroup region is created.</ul><h1 id="task-scheduling">Task scheduling</h1><h2 id="default-tasks-are-tied-to-the-thread-that-first-executes-them---not-neccessarily-the-creator">Default: Tasks are <code class="language-plaintext highlighter-rouge">tied</code> to the thread that first executes them -&gt; not neccessarily the creator.</h2><h3 id="scheduling-constraints">Scheduling Constraints:</h3><ul><li>Only the thread a task is tied to can execute it<li>A task can only be suspended at task scheduling points: task creation, task finish, taskwait, barrier, taskyield<li>If task is not suspended in a barrier, executing thread can only switch to a direct descendant of all tasks tied to hte thread.</ul><h3 id="tasks-created-with-the-utied-clause-are-never-tied">Tasks created with the <code class="language-plaintext highlighter-rouge">utied</code> clause are never tied</h3><ul><li>Resume at task scheduling points possibly by different thread<li>But: More freedom to the implementation, e.g., load balancing.</ul><h2 id="unsafe-use-of-untied-tasks">Unsafe use of <code class="language-plaintext highlighter-rouge">untied</code> Tasks</h2><h3 id="problem-because-untied-tasks-may-migrate-between-threads-at-any-point-thread-centric-constructs-can-yield-unexpected-results">problem: because untied tasks may migrate between threads at any point, thread-centric constructs can yield unexpected results.</h3><h3 id="remember-when-using-untied-tasks">Remember when using <code class="language-plaintext highlighter-rouge">untied</code> tasks:</h3><ul><li>Avoid <code class="language-plaintext highlighter-rouge">threadprivate</code> variable<li>Avoid any use of thread-ids(e.g., <code class="language-plaintext highlighter-rouge">omp_get_thread_num()</code>)<li>Be careful with <code class="language-plaintext highlighter-rouge">critical region</code> and locks</ul><h3 id="simple-solution">Simple Solution</h3><ul><li>Create a tied task region with <code class="language-plaintext highlighter-rouge">#pragma omp task if (0)</code></ul><h2 id="the-taskyield-directive">The taskyield Directive</h2><h3 id="the-taskyield-directive-specifies-that-the-current-task-can-be-suspended-in-favor-of-execution-of-a-different--task">The taskyield directive specifies that the current task can be suspended in favor of execution of a different- task.</h3><ul><li>Hint to the runtime for optimization and/or deadlock prevention.<div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="cp">#pragma omp taskyield
</span></pre></table></code></div></div><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/taskyield.png?raw=true" alt="taskyield" width="1086" height="542" /></p></ul><h1 id="tasks-and-dependencies">Tasks and Dependencies</h1><p>Task dependencies constrain execution order and times for tasks <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/task-dependency.png?raw=true" alt="task-dependency" width="1086" height="542" /></p><h1 id="numa">NUMA</h1><p>Non-Uniform Memory Architecture <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/serial-NUMA.png?raw=true" alt="serial-NUMA" width="1086" height="542" /> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/parallel-NUMA.png?raw=true" alt="parallel-NUMA" width="1086" height="542" /></p><h2 id="get-info-on-the-system-topology">Get info on the system topology</h2><p>Before you design a strategy for thread binding, you should have a basic understanding of the system topology. Please use one of the following options on a target machine</p><h3 id="intel-mpis-cpuinfo-tool">Intel MPI’s cpuinfo tool</h3><ul><li>module switch openmpi intelmpi<li>cpuinfo<li>Delivers information about the number of sockets(=packages) and the mapping pf processor ids used by the operating system to cpu cores.</ul><h3 id="hwlocs-tool">hwloc’s tool</h3><ul><li>lstopo(command line: <code class="language-plaintext highlighter-rouge">hwloc-ls</code>)<li>Display a graphical representation of the system topology, seperated into NUMA nodes, along with the mapping of processor ids used by the operating system to cpu cores and additional info on caches.</ul><h2 id="decide-for-binding-strategy">Decide for Binding Strategy</h2><p>Select the right binding strategy depends not only on the topology, but also on the characteristics of your application.</p><h3 id="putting-threads-far-apart-ie-on-different-sockets">putting threads far apart, i.e. on different sockets</h3><ul><li>May improve the aggregated memory bandwidth available to your application<li>May improve the combined cache size available to your application<li>May decrease performance of synchronization constructs</ul><h3 id="putting-threads-close-together-ie-on-two-adjacent-cores-which-possibly-shared-some-caches">putting threads close together, i.e. on two adjacent cores which possibly shared some caches</h3><ul><li>May improve performance of synchronization constructs<li>May decrease the available memory bandwidth and cache size</ul><h2 id="openmp-40-places--binding-policies12">OpenMP 4.0: Places + Binding Policies(1/2)</h2><h3 id="define-openmp-places">Define OpenMP Places</h3><ul><li>Set of OpenMP threads running on one or more processors<li>can be defined by the user, i.e. <code class="language-plaintext highlighter-rouge">OMP_PLACES=cores</code></ul><h3 id="define-a-set-of-openmp-thread-affinity-policies">Define a set of OpenMP Thread Affinity Policies</h3><ul><li>SPREAD: spread OpenMP threads evenly among the places<li>CLOSE: pack OpenMP threads near master thread<li>MASTER: collocate OpenMP thread with master thread</ul><h3 id="goals">Goals</h3><ul><li>user has a way to specify where to execute OpenMP threads for<li>locality between OpenMP threads/less false sharing/memory bandwidth</ul><h3 id="abstract-names-for-omp_places">Abstract names for <code class="language-plaintext highlighter-rouge">OMP_PLACES</code>:</h3><ul><li>threads: each place corresponds to a single hardware thread on the target machine.<li>cores: each place corresponds to a single core (having one or more hardware threads) on the target machine.<li>sockets: each place corresponds to a single socket(consisting of one or more cores) on the target machine.<li>ll_caches(5.1): each place corresponds to a set of cores that share the last level cache.<li>numa_domains(5.1): each place corresponds to a set of cores for which their closest memory is: the same memory; and at a similar distance from the cores.</ul><h2 id="openmp-40-places--binding-policies22">OpenMP 4.0: Places + Binding Policies(2/2)</h2><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/openMP/placing-binding-policy2.png?raw=true" alt="placing-binding-policy2" width="1086" height="542" /></p><h1 id="reference">Reference</h1><ul><li><a href="https://hpc-wiki.info/hpc/OpenMP_in_Small_Bites">OpenMP_in_Small_Bites</a></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/openmp/'>openMP</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/openmp/" class="post-tag no-text-decoration" >openMP</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=openMP - zuo&url=https://dazuozcy.github.io/posts/openmp/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=openMP - zuo&u=https://dazuozcy.github.io/posts/openmp/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=openMP - zuo&url=https://dazuozcy.github.io/posts/openmp/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/simd/">SIMD</a><li><a href="/posts/new-from-cpp11/">C++11开始的新特性</a><li><a href="/posts/keywords/">关键字</a><li><a href="/posts/empty-class/">class的内存模型</a><li><a href="/posts/diff-between-c-cpp/">C++ vs C</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/gpu/">GPU</a> <a class="post-tag" href="/tags/cuda/">CUDA</a> <a class="post-tag" href="/tags/openmp/">openMP</a> <a class="post-tag" href="/tags/c/">C++</a> <a class="post-tag" href="/tags/parallel-computing/">Parallel Computing</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/forecasting-at-scale/">Forecasting at Scale</a> <a class="post-tag" href="/tags/onednn/">oneDNN</a> <a class="post-tag" href="/tags/%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88/">智能指针</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/introdution-to-openmp-intel/"><div class="card-body"> <span class="timeago small" > Jul 3, 2020 <i class="unloaded">2020-07-03T20:19:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Introduction to openMP</h3><div class="text-muted small"><p> 这是一门非常优秀的OpenMP入门教程，讲解人Tim Mattson也曾参与过OpenMP的开发。 1. 概述 课程是简洁的lectures + 简短的execises的方式，讲究边学边练边掌握。 课程由五大模块组成，每个模块由一系列单元和讨论组成： Getting Started with OpenMP The Core Features of OpenMP Workin...</p></div></div></a></div><div class="card"> <a href="/posts/openmp-in-onednn/"><div class="card-body"> <span class="timeago small" > Jul 3, 2020 <i class="unloaded">2020-07-03T20:19:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>OpenMP in oneDNN</h3><div class="text-muted small"><p> 本文介绍下OpenMP在oneDNN库中的使用。 parallel() oneDNN中CPU上多线程并行的基础是parallel()函数，将与OpenMP无关的内容删除以后，精简如下所示： // src/common/dnnl_thread_parallel_nd.hpp inline int adjust_num_threads(int nthr, size_t work_amou...</p></div></div></a></div><div class="card"> <a href="/posts/openmp-increasing-mem-use/"><div class="card-body"> <span class="timeago small" > Jul 3, 2020 <i class="unloaded">2020-07-03T20:19:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>OpenMP库持续增加的内存占用</h3><div class="text-muted small"><p> 背景 在长稳执行某网络时，出现了内存占用持续上升的问题。经过初步分析隔离，发现是某些算子引入的，因为把那几个算子打桩掉后，内存占用就稳定了。那几个算子有个共性，底层是调用的oneDNN库实现的。oneDNN库的算子有内存泄漏问题？ 目标 分析oneDNN库的那几个算子为什么会内存泄漏。 定位过程 选择其中一个算子，梳理出调用流程，发现几处显式申请释放内存的操作，但都是严格匹配的，不...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/openmp-increasing-mem-use/" class="btn btn-outline-primary" prompt="Older"><p>OpenMP库持续增加的内存占用</p></a> <a href="/posts/cProfile/" class="btn btn-outline-primary" prompt="Newer"><p>Python性能分析工具cProfile</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/dazuozcy">zuo</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/gpu/">GPU</a> <a class="post-tag" href="/tags/cuda/">CUDA</a> <a class="post-tag" href="/tags/openmp/">openMP</a> <a class="post-tag" href="/tags/c/">C++</a> <a class="post-tag" href="/tags/parallel-computing/">Parallel Computing</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/forecasting-at-scale/">Forecasting at Scale</a> <a class="post-tag" href="/tags/onednn/">oneDNN</a> <a class="post-tag" href="/tags/%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88/">智能指针</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { let initTheme = "default"; if ($("html[mode=dark]").length > 0 || ($("html[mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://dazuozcy.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script async src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script>
