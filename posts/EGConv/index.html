<!DOCTYPE html><html lang="en-US" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="EGConv-DO WE NEED ANISOTROPIC GRAPH NEURAL NETWORKS" /><meta name="author" content="dazuo" /><meta property="og:locale" content="en_US" /><meta name="description" content="The Efficient Graph Convolution from the “Adaptive Filters and Aggregator Fusion for Efficient Graph Convolutions” paper." /><meta property="og:description" content="The Efficient Graph Convolution from the “Adaptive Filters and Aggregator Fusion for Efficient Graph Convolutions” paper." /><link rel="canonical" href="https://dazuozcy.github.io/posts/EGConv/" /><meta property="og:url" content="https://dazuozcy.github.io/posts/EGConv/" /><meta property="og:site_name" content="zuo" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-07-02T20:19:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="EGConv-DO WE NEED ANISOTROPIC GRAPH NEURAL NETWORKS" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"dazuo"},"dateModified":"2022-08-08T19:24:06+08:00","datePublished":"2020-07-02T20:19:00+08:00","description":"The Efficient Graph Convolution from the “Adaptive Filters and Aggregator Fusion for Efficient Graph Convolutions” paper.","headline":"EGConv-DO WE NEED ANISOTROPIC GRAPH NEURAL NETWORKS","mainEntityOfPage":{"@type":"WebPage","@id":"https://dazuozcy.github.io/posts/EGConv/"},"url":"https://dazuozcy.github.io/posts/EGConv/"}</script><title>EGConv-DO WE NEED ANISOTROPIC GRAPH NEURAL NETWORKS | zuo</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="zuo"><meta name="application-name" content="zuo"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" as="script"> <script async src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://cdn.jsdelivr.net/gh/cotes2020/chirpy-images/commons/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">zuo</a></div><div class="site-subtitle font-italic">感谢永远有歌把心境道破</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/dazuozcy" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['dazuozcy','163.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>EGConv-DO WE NEED ANISOTROPIC GRAPH NEURAL NETWORKS</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>EGConv-DO WE NEED ANISOTROPIC GRAPH NEURAL NETWORKS</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> dazuo </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Thu, Jul 2, 2020, 8:19 PM +0800" prep="on" > Jul 2, 2020 <i class="unloaded">2020-07-02T20:19:00+08:00</i> </span></div><div> <span> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Aug 8, 2022, 7:24 PM +0800" prefix="Updated " > Aug 8, 2022 <i class="unloaded">2022-08-08T19:24:06+08:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="12619 words">70 min</span></div></div><div class="post-content"><blockquote><p>The Efficient Graph Convolution from the <a href="https://arxiv.org/abs/2104.01481">“Adaptive Filters and Aggregator Fusion for Efficient Graph Convolutions”</a> paper.</p></blockquote><h1 id="摘要">摘要</h1><p><code class="language-plaintext highlighter-rouge">GNN</code>社区的常识表明：<strong>各向异性</strong>模型(模型中节点间发送的消息是源节点和目标节点的函数)用来实现最先进的性能。目前的<code class="language-plaintext highlighter-rouge">benchmarks</code>表明，这类模型的性能优于类似的<strong>各向同性</strong>模型(模型中消息仅是源节点的函数)。</p><p>在本文中，我们提供了挑战这一常识的实践证据：提出了一种各向同性<code class="language-plaintext highlighter-rouge">GNN</code>，高效图卷积( <code class="language-plaintext highlighter-rouge">EGC</code>)，它通过使用<strong>空间自适应滤波器</strong><code class="language-plaintext highlighter-rouge"> spatially-varying adaptive filters</code>，表现始终优于类似的各向异性模型，包括流行的<code class="language-plaintext highlighter-rouge">GAT</code>或<code class="language-plaintext highlighter-rouge">PNA</code>架构。</p><p>我们的工作除了向<code class="language-plaintext highlighter-rouge">GNN</code>社区提出了重要的问题外，还包括对效率的重大实际影响。<code class="language-plaintext highlighter-rouge">EGC</code>具有更高的模型精度，更低的内存消耗和延迟，以及适合加速器实现的特性，同时也是现有架构即插即用的替代品。</p><p>作为各向同性模型，它的复杂度与图中的顶点数成正比，即 \(\mathcal{O}(V)\)，相反，各向异性的杂度与图中的边数成正比，即 \(\mathcal{O}(E)\)。</p><p>我们证明了<code class="language-plaintext highlighter-rouge">EGC在</code>6个大型和不同的基准数据集上表现优于现有方法。<a href="https://github.com/shyam196/egc">我们实验的代码和预训练模型</a>。</p><h1 id="引言">引言</h1><p>图神经网络(<code class="language-plaintext highlighter-rouge">GNN</code>)已成为一种有效的在任意结构的数据上建立模型的方法。例如，它们已被成功应用于计算机视觉任务：<code class="language-plaintext highlighter-rouge">GNN</code>可在点云数据上提供高性能，并用于跨图像的特征匹配。最近的工作还表明，<code class="language-plaintext highlighter-rouge">GNN</code>可应用于物理模拟。代码分析是<code class="language-plaintext highlighter-rouge">GNN</code>获得成功的另一个应用领域。</p><p>近年来，研究界对构建更具表达力、性能更好的图像处理模型给予了极大关注。对<code class="language-plaintext highlighter-rouge">GNN</code>模型进行基准测试的努力，如开放图基准测试(<code class="language-plaintext highlighter-rouge">Open Graph Benchmark</code>)的工作，试图更严格地量化不同架构的相对性能。</p><p>一个常见的结论是，各向异性模型（其中节点之间发送的消息是源节点和目标节点的函数）是性能最好的模型。相比之下，各向同性模型（其中消息仅是源节点的函数）的精度较低，即使它们比可比较的各向异性模型具有效率优势。直觉上，这个结论是令人满意的：各向异性模型本质上更具表达力，因此我们期望它们在大多数情况下表现更好。我们的工作提供了一种各向同性模型，称为高效图卷积(<code class="language-plaintext highlighter-rouge">EGC</code>)，其性能优于类似的各向异性方法，包括流行的<code class="language-plaintext highlighter-rouge">GAT</code>和<code class="language-plaintext highlighter-rouge">PNA</code>架构，从而对这种智慧提出了令人惊讶的挑战。</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../../../img/gnn/egcConv_paper/EGC-message.png" alt="image-20220228235349197" width="1086" height="542" /> <em>图<code class="language-plaintext highlighter-rouge">1</code>：许多<code class="language-plaintext highlighter-rouge">GNN</code>架构(例如<code class="language-plaintext highlighter-rouge">GAT</code>、<code class="language-plaintext highlighter-rouge">PNA</code>)中包含复杂的消息功能以提高准确性(左)。这是有问题的，因为我们必须具体化消息，导致 \(\mathcal{O}(E)\) 内存消耗和操作数来计算消息；这些数据流模式也很难在硬件级别进行优化。这项工作表明，我们可以使用简单的消息函数，只需要 \(\mathcal{O}(V)\) 内存消耗(右)，并提高现有<code class="language-plaintext highlighter-rouge">GNN</code>的性能。</em></p><p>除了为社区提供了实证结果外，我们的工作还对效率的实际影响具有重要意义，如上面图1所示。由于<code class="language-plaintext highlighter-rouge">EGC</code>是一种各向同性模型，实现了高精度，我们可以利用各向同性模型提供的效率优势，而无需在模型精度上妥协。我们已经看到了近年来由于最先进的模型利用各向异性机制来提高精度，使得<code class="language-plaintext highlighter-rouge">GNN</code>架构复杂度增加到 \(\mathcal{O}(E)\)。</p><p><code class="language-plaintext highlighter-rouge">EGC</code>将复杂性降低到 \(\mathcal{O}(V)\)，提供了实质性的收益，尽管具体的收益取决于模型作用的图的拓扑结构。读者应该注意，我们的方法也可与其他方法相结合，以提高<code class="language-plaintext highlighter-rouge">GNN</code>的效率。例如，常见的软硬件协同设计技术（包括量化和剪枝）可与这项工作相结合，这项工作提出了一种通过改进底层架构设计来提高模型效率的正交方法。我们还注意到，我们的方法可以与图采样技术相结合，以进一步提高在具有数百万或数十亿节点的图上进行训练时的可伸缩性。</p><h2 id="贡献">贡献</h2><ul><li>我们提出了一种新的<code class="language-plaintext highlighter-rouge">GNN</code>架构，高效图卷积<code class="language-plaintext highlighter-rouge">EGC</code>，并为其提供了空间和谱解释。<li>我们在6个大型图数据集上对我们的架构进行了严格的评估，涵盖了转换和归纳用例，并证明了<code class="language-plaintext highlighter-rouge">EGC</code>始终比强基线取得更好的结果。<li>我们提供了几项消融研究，以激发我们模型中超参数的选择。<li>我们证明，与竞争方法相比，我们的模型同时实现了更好的参数效率、延迟和内存消耗。<a href="https://github.com/shyam196/egc">我们实验的代码和预训练模型</a>. 在出版时，<code class="language-plaintext highlighter-rouge">EGC</code>被贡献至了<code class="language-plaintext highlighter-rouge">PyTorch Geometric</code>库。</ul><h1 id="背景">背景</h1><h2 id="深度学习的软硬件协同设计">深度学习的软硬件协同设计</h2><p>引言中已经描述了几种常用的软硬件协同设计方法：量化、修剪和细致的架构设计都是<code class="language-plaintext highlighter-rouge">CNN</code>和<code class="language-plaintext highlighter-rouge">Transformers</code>的常见方法。除了能够从通用处理器（如<code class="language-plaintext highlighter-rouge">CPU</code>和<code class="language-plaintext highlighter-rouge">GPU</code>）获得更好的性能外，这些技术对于最大化专用加速器的回报也是必不可少的；虽然随着时间的推移，由于<code class="language-plaintext highlighter-rouge">CMOS</code>技术的改进，性能可能会提高，但进一步的改进在算法层面上没有创新。作为神经网络架构设计师，我们不能简单地依靠硬件的改进来使我们的建议在实际部署中可行。</p><h2 id="图神经网络">图神经网络</h2><p>许多<code class="language-plaintext highlighter-rouge">GNN</code>架构被视为<code class="language-plaintext highlighter-rouge">CNN</code>架构在非规则域的推广：在<code class="language-plaintext highlighter-rouge">CNN</code>中，每个节点的表示都是基于局部邻域，使用整图共享的参数来构建的。<code class="language-plaintext highlighter-rouge">GNN</code>不同，因为我们无法对邻域的大小或顺序进行假设。用于定义<code class="language-plaintext highlighter-rouge">GNN</code>的一个常见框架是消息传递神经网络<code class="language-plaintext highlighter-rouge">MPNN</code>范式。</p><p>假设图 \(G=(V,E)\) 有<strong>节点特征</strong>矩阵 \(\mathbf{X} \in \mathbb{R}^{N\times F}\)、<strong>邻接</strong>矩阵 \(\mathbf{A} \in \mathbb{R}^{N\times N}\)和可选的$D$维<strong>边特征</strong>矩阵 \(\mathbf{E} \in \mathbb{R}^{E\times D}\)。我们定义一个函数 \(\phi\)，用于计算从节点 \(u\) 到节点 \(v\) 的消息，这是一个可微且具有排列不变性<code class="language-plaintext highlighter-rouge">permutation-invariant </code>(是指特征之间没有空间位置关系)的<strong>聚合器 \(\oplus\)</strong>, 和一个**更新函数 \(\gamma\) **来计算 \(l+1\) 层的表示</p><p>\(\mathbf{h}_{l+1}^{(i)} = \gamma(\mathbf{h}_{l}^{(i)}, \oplus_{j \in \mathcal{N}(i)}[\phi(\mathbf{h}_{l}^{(i)}, \mathbf{h}_{l}^{(j)}, \mathbf{e}_{ij})])\).</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../../../img/gnn/egcConv_paper/results_for_models.png" alt="image-20220805142730872" width="1086" height="542" /> <em>表1: 参数归一化模型在5个数据集上的运行结果(平均值±标准偏差)。聚合器的选择和详细信息及进一步的实验细节可在补充材料中找到。结果标记为<code class="language-plaintext highlighter-rouge">∗</code>表示在11 GB <code class="language-plaintext highlighter-rouge">1080Ti</code>和<code class="language-plaintext highlighter-rouge">2080Ti</code> GPU上出现内存不足。<code class="language-plaintext highlighter-rouge">EGC</code>在其中4项任务中获得最佳性能，并始终保持较大的优势。</em></p><p>表1中提供了基准架构的传播规则，表5提供了更多详细信息。</p><div class="table-wrapper"><table><thead><tr><th>方法<th>传播规则<th>内存<th>备注<tbody><tr><td><code class="language-plaintext highlighter-rouge">GCN</code><td>$\mathbf{y}^{(i)}=\mathbf{\Theta} \sum_{j \in \mathcal{N}(i)\cup{i}} \frac{1}{\sqrt{deg(i)deg(j)}} \mathbf{x}^{(i)}$<td>$\mathcal{O}(V)$<td>带自环的无向图的形式化定义；motivated by graph signal processing.<tr><td><code class="language-plaintext highlighter-rouge">GIN</code><td>$\mathbf{y}^{(i)}=f_{\Theta}[(1+\epsilon)\mathbf{x}^{(i)}+\sum_{j \in \mathcal{N}(i)}\mathbf{x}^{(j)}]$<td>$\mathcal{O}(V)$<td>$f$ 可学习函数, 通常参数化为一个<code class="language-plaintext highlighter-rouge">MLP</code>或线性层 ; $\epsilon$ 可以是固定值或可学习的.<tr><td><code class="language-plaintext highlighter-rouge">GraphSAGE</code><td>$\mathbf{y}^{(i)}=\mathbf{\Theta_{1}}\mathbf{x}^{(i)}+ \bigoplus_{j \in \mathcal{N}(i)}\mathbf{\Theta_{2}}\mathbf{x}^{(j)}$<td>$\mathcal{O}(V)$<td>$\bigoplus$ 通常参数化为<code class="language-plaintext highlighter-rouge">mean</code> 或<code class="language-plaintext highlighter-rouge">max</code><tr><td><code class="language-plaintext highlighter-rouge">GAT</code><td>$\mathbf{y}^{(i)}=\alpha_{i,i}\mathbf{\Theta}\mathbf{x}^{(i)}+\sum_{j \in \mathcal{N}(i)}\alpha_{i,j}\mathbf{\Theta}\mathbf{x}^{j}$<td>$\mathcal{O}(E)$<td>注意力系数 \(\alpha_{i,j}=\frac{exp\big(LeakyReLU\big(\mathbf{a}^{\top}[\mathbf{\Theta}\mathbf{x}^{(i)} \Vert \mathbf{\Theta}\mathbf{x}^{(j)}]\big)\big)}{\sum_{k \in \mathcal{N}(i)\cup\{i\}}exp\big(LeakyReLU\big(\mathbf{a}^{\top}[\mathbf{\Theta}\mathbf{x}^{(i)} \Vert \mathbf{\Theta}\mathbf{x}^{(k)}]\big)\big)}\) <br />Common to define multiple attention heads and concatenate.<tr><td><code class="language-plaintext highlighter-rouge">GATv2</code><td>$\mathbf{y}^{(i)}=\alpha_{i,i}\mathbf{\Theta}\mathbf{x}^{(i)}+\sum_{j \in \mathcal{N}(i)}\alpha_{i,j}\mathbf{\Theta}\mathbf{x}^{j}$<td>$\mathcal{O}(E)$<td>与<code class="language-plaintext highlighter-rouge">GAT</code>类似，但重新定义了 $\alpha_{i,j}$ 来提高表达力 \(\alpha_{i,j}=\frac{exp\big(\mathbf{a}^{\top}LeakyReLU(\mathbf{\Theta}[\mathbf{x}_{i} \Vert \mathbf{x}_{j}])\big)}{ \sum_{k \in \mathcal{N}(i)\cup\{i\}} exp\big(\mathbf{a}^{\top}LeakyReLU(\mathbf{\Theta}[\mathbf{x}_{i} \Vert \mathbf{x}_{k})\big)}\)<tr><td><code class="language-plaintext highlighter-rouge">MPNN</code><td>$\mathbf{y}^{(i)}=U(\mathbf{x}^{(i)},\bigoplus_{j \in \mathcal{N}(i)}M(\mathbf{x}^{(i)},\mathbf{x}^{(j)},\mathbf{e}_{i,j}))$<td>$\mathcal{O}(E)$<td>$U,M$ typically defined as linear layers acting on concatenated features; $\bigoplus$ may be any valid aggregator, typically <code class="language-plaintext highlighter-rouge">sum</code> or <code class="language-plaintext highlighter-rouge">max</code>.<tr><td><code class="language-plaintext highlighter-rouge">PNA</code><td>$\mathbf{y}^{(i)}=U(\mathbf{x}^{(i)},\bigoplus_{j \in \mathcal{N}(i)}M(\mathbf{x}^{(i)},\mathbf{x}^{(j)},\mathbf{e}_{i,j}))$<td>$\mathcal{O}(E)$<td>与<code class="language-plaintext highlighter-rouge">MPNN</code>类似, 但 $\bigoplus$ 定义为使用4个聚合器(<code class="language-plaintext highlighter-rouge">mean</code>, <code class="language-plaintext highlighter-rouge">standard deviation</code>, <code class="language-plaintext highlighter-rouge">max</code>, <code class="language-plaintext highlighter-rouge">min</code>)，按节点度的3个不同函数进行缩放，默认情况下产生12种不同的聚合器</table></div><p>​ <em>表5：与我们的工作做对比的通用<code class="language-plaintext highlighter-rouge">GNN</code>架构的传播规则，使用节点公式。根据流行的体系结构和最近比较先进的方案<code class="language-plaintext highlighter-rouge">PNA</code>进行评估。</em></p><h3 id="gnn的相对表达能力"><code class="language-plaintext highlighter-rouge">GNN</code>的相对表达能力</h3><p>研究界的共识是，各向同性<code class="language-plaintext highlighter-rouge">GNN</code>比各向异性<code class="language-plaintext highlighter-rouge">GNN</code>表达能力差；从经验上看，这得到了基准的充分支持。<code class="language-plaintext highlighter-rouge">Brody</code>等人证明，<code class="language-plaintext highlighter-rouge">GAT</code>模型可以比各向同性模型更严格地表达。<code class="language-plaintext highlighter-rouge">Bronstein</code>等人还讨论了不同类别<code class="language-plaintext highlighter-rouge">GNN</code>层的相对表达能力，并认为卷积（也称为各向同性）模型非常适合利用输入图中的同态的问题。他们进一步认为，注意力或全消息传递模型适合处理异嗜性问题，但他们承认这些架构的资源消耗和可训练性可能会令人望而却步，尤其是在全消息传递的情况下。</p><h3 id="扩展和部署gnn">扩展和部署<code class="language-plaintext highlighter-rouge">GNN</code></h3><p>虽然<code class="language-plaintext highlighter-rouge">GNN</code>在一系列领域取得了成功，但在扩展和部署方面仍存在挑战。图采样是对内存存储不下的大型图或模型进行缩放训练的一种方法。不是在整图上训练，而是在采样子图上运行每个迭代；采样方法根据是按节点、按层或按子图进行采样会有所不同。</p><p>另外，<code class="language-plaintext highlighter-rouge">GNN</code>分布式训练系统已经被提出，以将训练扩展到单个加速器的极限之外。一些工作提出了设计用于适应缩放的架构：图像增强<code class="language-plaintext highlighter-rouge">MLP</code>，例如<code class="language-plaintext highlighter-rouge">SIGN</code>被明确设计为浅层架构，因为所有图像操作都是作为预处理步骤完成的。</p><p>其他工作包括应用神经架构搜索<code class="language-plaintext highlighter-rouge">NAS</code>来安排现有<code class="language-plaintext highlighter-rouge">GNN</code>层，或为<code class="language-plaintext highlighter-rouge">GNN</code>构建量化技术。最后，最近的一项工作表明，对<code class="language-plaintext highlighter-rouge">GNN</code>使用内存高效的可逆残差使我们能够训练比以前更深更大的<code class="language-plaintext highlighter-rouge">GNN</code>模型，从而提高最先进的精度。</p><h3 id="为什么现有方法不够">为什么现有方法不够</h3><p>值得注意的是，这些方法中有许多都有很大的局限性，我们希望用我们的工作来解决。当应用于许多涉及模型泛化到看不见图的问题时，采样方法通常无效——这是<code class="language-plaintext highlighter-rouge">GNN</code>的常见用法。我们评估了各种采样方法，并观察到即使是对内存或延迟几乎没有好处的适度采样水平，也会导致模型性能显著下降。此外，这些方法不会加速底层<code class="language-plaintext highlighter-rouge">GNN</code>，因此它们可能不会对推断延迟提供任何总体好处。也没有证据表明我们知道，当推广到看不见的图时，图增强的<code class="language-plaintext highlighter-rouge">MLP</code>能够充分发挥作用；事实上，它们在理论上不如标准<code class="language-plaintext highlighter-rouge">GNN</code>表达。我们还调查了这一设置，发现这些方法与最先进的方法相比，没有提供具有竞争力的准确性。附录<code class="language-plaintext highlighter-rouge">B</code>提供了实验细节和结果，以及对现有工作局限性的进一步讨论。</p><p>总之，我们在高效<code class="language-plaintext highlighter-rouge">GNN</code>架构设计方面的工作引起了社区的兴趣，原因有两个：</p><ul><li>它提出了关于常见假设的问题，以及我们如何设计和评估<code class="language-plaintext highlighter-rouge">GNN</code>模型；我们的工作可能使我们能进一步扩展模型，从而提高准确性。<li>此外，对于需要泛化到看不见的图形的任务，如代码分析或点云处理，我们减少了内存消耗和延迟，从而使我们能够将模型部署到比以前更加资源受限的设备上。我们注意到，有效的架构设计可以有效地与其他方法相结合，包括采样、量化和剪枝。</ul><h1 id="我们的架构">我们的架构</h1><p>在本节中，我们描述了我们的方法，并将理论分析推迟到下一节。我们提出了两个版本：</p><ul><li>使用<strong>单个聚合器</strong>的<code class="language-plaintext highlighter-rouge">EGC-S</code>(S for Single)<li>合并<strong>多个聚合器</strong>的<code class="language-plaintext highlighter-rouge">EGC-M</code>(M for Multi)</ul><p>我们的方法如图<code class="language-plaintext highlighter-rouge">2</code>所示:</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../../../img/gnn/egcConv_paper/EGC-S.png" alt="image-20220718161115255" width="1086" height="542" /> <em>图2: <code class="language-plaintext highlighter-rouge">EGC-S</code>层的可视化表示。在该可视化中，我们有$3$个基本过滤器（即$B=3$)，它们使用每个节点权重$w$进行组合。</em></p><h2 id="架构描述">架构描述</h2><p>对于某一层，使用 $B$ 个<code class="language-plaintext highlighter-rouge">basis weight</code> \(\mathbf{\Theta}_{b} \in \mathbb{R}^{F^{'} \times F}\)，其中 $F$ 表示输入维度和 \(F^{'}\) 表示输出维度。</p><p>通过计算每个节点的组合权重系数 \(\mathbf{w}^{(i)} \in \mathbb{R}^{B}\) 来计算节点 \(i\) 的输出，并使用不同的<code class="language-plaintext highlighter-rouge">basis weight</code> \(\mathbf{\Theta}_{b}\) 来对每个聚合的结果加权。</p><p>节点 \(i\) 的输出通过3个步骤来计算：</p><ul><li><p>we perform the aggregation with each set of basis weights \(\mathbf{\Theta}_{b}\)</p><li><p>计算每个节点 $i$ 的权重系数 \(\mathbf{w}^{(i)}=\mathbf{\Phi}\mathbf{x}^{(i)} + \mathbf{b} \in \mathbb{R}^{B}\), \(\pmb{\Phi} \in \mathbb{R}^{B \times F}\) 和 \(\pmb{b} \in \mathbb{R}^{B}\) 分别是用于计算组合权重系数的权重和偏置。</p><li><p>节点 $i$ 的层输出是聚合输出的加权组合：</p>\[\mathbf{y}^{(i)} = \sum_{b=1}^{B} w_{b}^{(i)} \sum_{j \in \mathcal{N}(i)} \alpha(i,j) \mathbf{\Theta}_{b} \mathbf{x}^{(j)}\]<p>其中，\(\alpha(i,j)\) 是节点 \(i\) 和 \(j\) 的某个函数，\(\mathcal{N}(i)\) 表示节点 \(i\) 的输入邻居。</p></ul><p><code class="language-plaintext highlighter-rouge">GAT</code>开创的一种提高表征能力的方法是使用两个节点的学习函数来表示 \(\alpha\)。虽然这可以实现邻居的各向异性处理，并提高性能，但由于需要显式物化消息，必然会导致 \(\mathcal{O}(E)\) 的内存消耗，并使加速器的硬件实现复杂化。如果我们为 \(\alpha\) 选择的表示不是节点的函数，例如\(\alpha=1\) 来复现<code class="language-plaintext highlighter-rouge">GIN</code>使用的加法聚合器或 \(\alpha=\sqrt{deg(i)deg(j)}\) 来复现<code class="language-plaintext highlighter-rouge">GCN</code>使用的对称归一化，那么我们可以使用稀疏矩阵乘法<code class="language-plaintext highlighter-rouge">SpMM</code>来实现消息传播阶段，并避免显式物化每个消息，即使是对于向后传递。在这项工作中，除非另有说明，否则我们假设 \(\alpha(i,j)\) 是<code class="language-plaintext highlighter-rouge">GCN</code>使用的对称归一化；我们使用这种规范化，因为它可以在各种任务中提供强大的结果；第<code class="language-plaintext highlighter-rouge">4.2</code>节提供了更正式的理由。</p><h3 id="添加heads作为正则化项">添加<code class="language-plaintext highlighter-rouge">Heads</code>作为正则化项</h3><p>我们可以通过添加<code class="language-plaintext highlighter-rouge">head</code>来扩展我们的层，如在<code class="language-plaintext highlighter-rouge">GAT</code>或<code class="language-plaintext highlighter-rouge">Transformer</code>等架构中使用的。这些<code class="language-plaintext highlighter-rouge">head</code>共享基本权重，但每个<code class="language-plaintext highlighter-rouge">head</code>应用不同的加权系数。我们发现，当<code class="language-plaintext highlighter-rouge">head</code>数 $H$ 大于 $B$ 时，添加此自由度有助于正则化，因为不鼓励基专门化（见第<code class="language-plaintext highlighter-rouge">5.3</code>节），而无需将额外的损失项集成到优化中，因此不需要更改下游用户的代码。为了规范化输出维度，我们将基权重矩阵维度更改为 \(\frac{F^{'}}{H} \times F\)。使用\(\|\)作为级联算子，并显式使用对称规范化，我们获得了<code class="language-plaintext highlighter-rouge">EGC-S</code>层：</p>\[\mathbf{y}^{(i)} =={\LARGE \lVert}_{h=1}^{H} \sum_{b=1}^{B} w_{h,b}^{(i)} \sum_{j \in \mathcal{N}(i)\cup \{i\}} \frac{1}{\sqrt{deg(i)deg(j)}} \mathbf{\Theta}_{b} \mathbf{x}^{(j)}\]<p><code class="language-plaintext highlighter-rouge">EGC</code>通过组合基矩阵来工作。该思想在<code class="language-plaintext highlighter-rouge">R-GCN</code>中提出，用于处理多种边类型。在本工作中，我们正在解决与这些工作不同的问题：我们感兴趣的是设计高效的架构，而不是处理边信息的新方法。</p><h2 id="提升表达能力">提升表达能力</h2><p><code class="language-plaintext highlighter-rouge">Corso</code>等人最近的工作表明，仅使用一个聚合器是次优的，相反，最好将几个不同的聚合器组合在一起。在公式<code class="language-plaintext highlighter-rouge">3</code>中，我们对层仅使用对称归一化。为了提高性能，我们建议对$\mathbf{\Theta}_{b} \mathbf{x}^{(j)}$计算的表示应用不同的聚合器。聚合器的选择可以包括求和聚合器的不同变体，例如均值或未加权加法，与上一节中提出的对称归一化相反。或者，我们可以使用不基于总和的聚合器，例如<code class="language-plaintext highlighter-rouge">stddev</code>、<code class="language-plaintext highlighter-rouge">min</code>或<code class="language-plaintext highlighter-rouge">max</code>。也可以使用<code class="language-plaintext highlighter-rouge">Beaini</code>等人提出的定向聚合器，但这种增强与这项工作是正交的。如果我们有一组聚合器 \(\mathcal{A}\)，我们可以扩展公式<code class="language-plaintext highlighter-rouge">3</code>以获得<code class="language-plaintext highlighter-rouge">EGC-M</code>层：</p>\[\mathbf{y}^{(i)} ={\LARGE \lVert}_{h=1}^{H} \sum_{\oplus \in \mathcal{A}} \sum_{b=1}^{B} w_{h,\oplus,b}^{(i)} \bigoplus_{j \in \mathcal{N}(i)\cup \{i\}} \mathbf{\Theta}_{b} \mathbf{x}^{(j)}\]<p>其中 \(\oplus\) 是一个聚合器。有了这个公式，我们将重用与之前计算的相同的消息，但我们将同时对其应用几个聚合函数。</p><h3 id="聚合器融合">聚合器融合</h3><p>似乎添加更多聚合器会导致延迟和内存消耗呈线性增长。然而实际上并非如此。首先，由于稀疏操作在实践中通常是内存受限的，我们可以对已经从内存中到达的数据应用额外的聚合器，而延迟损失很小。<code class="language-plaintext highlighter-rouge">EGC</code>还可在推理时有效地内联节点加权操作，从而导致相对较小的内存消耗开销。等效优化更难成功应用于<code class="language-plaintext highlighter-rouge">PNA</code>，因为在所有结果串联和应用转换之前，每个节点在聚合期间必须执行更多的操作，这是由应用于每个聚合的缩放函数引起的。更多详细信息，包括评测和延迟测量，请参阅附录<code class="language-plaintext highlighter-rouge">D</code>。</p><h1 id="解释和收益">解释和收益</h1><p>本节将解释我们的设计选择，以及为什么它们更适合硬件。我们强调，我们的方法并不直接对应注意力。</p><h2 id="空域解释节点权重矩阵">空域解释：节点权重矩阵</h2><p>在我们的方法中，每个节点具有自己的权重矩阵。我们可以通过将 \(\mathbf{\Theta}_{b}\) 从内和项中分解出来重新排列公式<code class="language-plaintext highlighter-rouge">2</code>得出这一点：</p>\[\mathbf{y}^{(i)} = {\LARGE \lVert}_{h=1}^{H} \underbrace{\mathbf{\Theta}_{h}^{(i)}}_{\text{Varying per Node}} \quad \underbrace{\Bigg( \sum_{j \in \mathcal{N}(i)\cup \{i\}} \frac{1}{\sqrt{deg(i)deg(j)}} \mathbf{x}^{(j)} \Bigg)}_{\text{Computable via SpMM}}\]<p>相反，<code class="language-plaintext highlighter-rouge">GAT</code>共享权重，并通过计算每条消息的权重将复杂性推到消息计算阶段。<code class="language-plaintext highlighter-rouge">MPNN</code>和<code class="language-plaintext highlighter-rouge">PNA</code>通过显式计算每条消息进一步增加了复杂性——由于<code class="language-plaintext highlighter-rouge">dense</code>操作大致按照 \(\frac{\vert E \vert}{\vert V \vert}\) 级别增加，从而导致大量延迟开销。具体而言，我们有：</p>\[\mathbf{y}_{\text{GAT}}^{(i)} = {\LARGE \lVert}_{h=1}^{H} \underbrace{\mathbf{\Theta}}_{\text{Shared Weights}} \quad \underbrace{\Bigg( \sum_{j \in \mathcal{N}(i)\cup \{i\}} \alpha_{h,i,j} \mathbf{x}^{(j)} \Bigg)}_{\text{Computable via SpMM}}\] \[\mathbf{y}_{\text{PNA}}^{(i)} = U(\mathbf{x}^{(i)}, \ \bigoplus_{j \in \mathcal{N(i)}} \underbrace{M(\mathbf{x}^{(i)},\mathbf{x}^{(j)})}_{\text{Explicit Message} \text{Calculation}})\]<p>从效率的角度来看，我们观察到，使用<code class="language-plaintext highlighter-rouge">SpMM</code>方法具有更好的特性，因为它只需要 $\mathcal{O}(V)$ 内存消耗。使用<code class="language-plaintext highlighter-rouge">SpMM</code>不需要显式实现消息。我们注意到，在<code class="language-plaintext highlighter-rouge">GAT</code>中虽然可以使用<code class="language-plaintext highlighter-rouge">SpMM</code>传递消息，但无法避免在训练期间存储权重，因为反向传播需要，从而导致 $\mathcal{O}(E)$ 内存消耗。我们还注意到，对于某些架构，在推理阶段可能可以融合消息传递和聚合步骤，但这对硬件加速器来说是一个复杂的模式。</p><h3 id="与注意力机制关系">与注意力机制关系</h3><p>我们的方法与注意力机制没有直接关系，注意力依赖于成对相似机制，因此在使用常见公式时会产生 $\mathcal{O}(E)$ 成本。吴等人提出的基于注意力的<code class="language-plaintext highlighter-rouge">Transformers</code>的替代方案与我们的技术更接近，但依赖于每个时间步长权重矩阵的显式预测。这种方法对于图是不可行的，因为邻域大小不是恒定的。</p><h2 id="谱域解释局部谱滤波">谱域解释：局部谱滤波</h2><p>我们还可以通过图信号处理理论来解释<code class="language-plaintext highlighter-rouge">EGC-S</code>。许多现代图神经网络建立在这种观察之上： 欧氏空间上的卷积运算在推广到图域时具有强烈的归纳影响，它关注域的结构，并通过作为空间中的局部操作来保持特征的局部性。我们的方法可以看作是为图域构建自适应滤波器。当信号或噪声的特性随时间或空间变化时，自适应滤波器是常用的方法；例如，它们通常用于自适应噪声抵消。我们的方法可以看作是通过线性组合具有空间可变系数的可学习滤波器组来构造自适应滤波器。</p><p>图卷积运算通常在谱域上定义为使用带 \(θ\) 参数的滤波器 \(g_{\theta}\) 对具有 \(N\) 个节点的图的输入信号 \(\mathbf{x} \in \mathbb{R}^{N}\) 进行滤波。这需要使用傅里叶变换在谱域和空域间进行转换。与欧几里德域一样，图域上的傅里叶变换被定义为拉普拉斯算子正交特征基的基分解，对于具有邻接矩阵 \(\mathbf{A} \in \mathbb{R}^{N \times N}\) 的图，该基分解被定义为 \(\mathbf{L} = \mathbf{D} − \mathbf{A}\)、 其中 $\mathbf{D}$ 是 \(D_{ii} = \sum_{j=1}^{N}A_{ij}\) 的度矩阵。信号\(\mathbf{x} \in \mathbb{R}^{N}\)的傅里叶变换为 \(\mathcal{F}(\mathbf{x}) = \mathbf{U}^{\top} \mathbf{x}\)，其中 \(\mathbf{L} = \mathbf{U} \mathbf{\Lambda} \mathbf{U}^{\top}\)，具有正交特征向量矩阵 \(\mathbf{U} \in \mathbb{R}^{N \times N}\) 和对角特征值矩阵\(\mathbf{\Lambda} \in \mathbb{R}^{N \times N}\)。</p><p>信号 $\mathbf{x}$ 经过 \(g_{\theta}\) 滤波后的结果是 \(\mathbf{y} = g_{θ}(\mathbf{L}) \mathbf{x} = \mathbf{U} g_{θ} (\mathbf{\Lambda}) \mathbf{U}^{\top} \mathbf{x}\)，其中如果 $g_{\theta}$ 存在泰勒展开，则第二个等号成立。</p><p>我们的方法相当于学习多个滤波器并计算这些滤波器的线性组合，其权重取决于每个节点的局部属性。因此，该模型允许为每个节点应用多个滤波器，使我们能够获得空间变化的频率响应，同时计算复杂度远低于 $\mathcal{O}(E)$。使用线性组合的滤波器，滤波后的信号变为 \(\mathbf{y} = \sum_{b=1}^{B} \mathbf{w}_{b} \odot g_{\theta_{b}} (\mathbf{L}) \mathbf{x}\)，其中 $\mathbf{w}_{b} \in \mathbb{R}^{N}$ 是图中 $N$ 个节点中每个节点的滤波器 $b$ 的权重。</p><p>如果我们使用<code class="language-plaintext highlighter-rouge">Kipf&amp;Welling</code>使用的一阶切比雪夫多项式对滤波器进行参数化，则滤波信号的最终表达式为 \(\mathbf{Y} = \sum_{b=1}^{B} \mathbf{w}_{b} \odot (\tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}})\)，</p><p>其中 $\tilde{\mathbf{A}} = \mathbf{A} + \mathbf{I}_{N}$ 是带自环的邻接矩阵，$\tilde{\mathbf{D}}$ 是前面定义过的 $\tilde{\mathbf{A}}$ 的度矩阵。这解释了我们在式(2)中选择的对称归一化聚合器。</p><p>程等人提出了一种局部滤波方法。然而，他们的方法不能推广到看不见的拓扑，也不能扩展到大型图，因为它需要学习几个系数大小为 $N×N$ 的滤波器矩阵 $\mathbf{S}_{k}$ 。我们的方法不受这些约束。</p><h1 id="验证">验证</h1><h2 id="说明">说明</h2><p>我们主要在5个数据集上评估我们的方法，这些数据集取自<code class="language-plaintext highlighter-rouge">GNN</code>基准测试的最新工作。我们使用<code class="language-plaintext highlighter-rouge">Dwivedi</code>等人的<code class="language-plaintext highlighter-rouge">ZINC</code>和<code class="language-plaintext highlighter-rouge">CIFAR-10 Superpixels</code>，以及<code class="language-plaintext highlighter-rouge">Open Graph Benchmark</code>中的<code class="language-plaintext highlighter-rouge">Arxiv</code>、<code class="language-plaintext highlighter-rouge">MolHIV</code>和<code class="language-plaintext highlighter-rouge">Code</code>。</p><p>这些数据集涵盖了广泛的领域，涵盖了<strong>转换</strong>和<strong>归纳</strong>任务，并且比通常<code class="language-plaintext highlighter-rouge">GNN</code>工程中使用的数据集更大。我们使用这些论文指定的评估指标和分割。基准架构的选择反映了流行的通用选择，以及最先进的<code class="language-plaintext highlighter-rouge">PNA</code>和<code class="language-plaintext highlighter-rouge">GATv2</code>架构。</p><p>为了提供公平的比较，我们在实验中标准化了所有参数计数、架构和优化器。所有实验均使用<code class="language-plaintext highlighter-rouge">Adam</code>进行。有关我们如何确保公平评估的更多详细信息，请参见附录。</p><p>实验中我们没有使用<strong>边特征</strong>，因为对于大多数基准架构，没有标准的方法来合并它们。我们不使用抽样，如第2.2节所述，抽样对4个数据集无效；对于剩余的数据集<code class="language-plaintext highlighter-rouge">Arxiv</code>，我们认为引入额外变量不符合科学目的。这也适用于<code class="language-plaintext highlighter-rouge">GraphSAGE</code>，它不使用常用的邻域采样。所有实验都进行了10次。</p><h2 id="主要结果">主要结果</h2><p>我们在5项任务中的结果如表1所示。我们注意到以下观察结果：</p><ul><li><code class="language-plaintext highlighter-rouge">EGC-S</code>与各向异性方法相当。尽管我们的资源效率很高，但我们在所有基准上都优于<code class="language-plaintext highlighter-rouge">GAT(v1)</code>和<code class="language-plaintext highlighter-rouge">MPNN-Sum</code>。最明显的例外是<code class="language-plaintext highlighter-rouge">CIFAR</code> 和<code class="language-plaintext highlighter-rouge">Code</code>数据集上的<code class="language-plaintext highlighter-rouge">MPNN-Max</code>，其中<code class="language-plaintext highlighter-rouge">Max</code>聚合器提供了更强的归纳偏差。我们观察到<code class="language-plaintext highlighter-rouge">GATv2</code>改进了<code class="language-plaintext highlighter-rouge">GAT</code>，但没有明显优于<code class="language-plaintext highlighter-rouge">EGC</code>。<li><code class="language-plaintext highlighter-rouge">EGC-M</code>优于<code class="language-plaintext highlighter-rouge">PNA</code>。添加多个聚合器函数在<code class="language-plaintext highlighter-rouge">EGC</code>上获得的效果提升超过在<code class="language-plaintext highlighter-rouge">PNA</code>上的。我们假设，我们对<code class="language-plaintext highlighter-rouge">PNA</code>的效果提升与<code class="language-plaintext highlighter-rouge">PNA</code>对多度缩放变换的依赖有关。虽然这种方法可以增强体系结构的代表性，但我们认为它可能导致过度适应训练集的趋势。<li><code class="language-plaintext highlighter-rouge">EGC</code>表现强势且不会耗尽内存。我们观察到，在数据集<code class="language-plaintext highlighter-rouge">Arxiv</code>上，<code class="language-plaintext highlighter-rouge">EGC</code>是仅有的三种没有耗尽<code class="language-plaintext highlighter-rouge">Nvidia 1080/2080Ti</code> GPU的VRAM（具有11 GB VRAM）的架构之一：我们必须使用具有48 GB VRAM的<code class="language-plaintext highlighter-rouge">RTX 8000 GPU</code>来运行这些实验。<code class="language-plaintext highlighter-rouge">PNA</code>是我们在精确度方面最具竞争力的技术，它也耗尽了代码基准测试的内存。表4提供了详细的内存消耗数据。</ul><p>总体而言，<code class="language-plaintext highlighter-rouge">EGC</code>在5个主要数据集中的4个上获得了最佳性能；在剩下的数据集<code class="language-plaintext highlighter-rouge">MolHIV</code>上，<code class="language-plaintext highlighter-rouge">EGC</code>是第二好的架构。这代表了一项有意义的成果：我们的架构不需要在性能和精度之间做出选择。</p><h2 id="附加研究">附加研究</h2><h3 id="如何选择heads和bases">如何选择<code class="language-plaintext highlighter-rouge">Heads</code>和<code class="language-plaintext highlighter-rouge">Bases</code>？</h3><p>为了了解Heads($H$)和Bases($B$)之间的权衡，我们使用<code class="language-plaintext highlighter-rouge">EGC-S</code>对<code class="language-plaintext highlighter-rouge">ZINC</code>进行了<code class="language-plaintext highlighter-rouge">ablation</code>研究，如图3所示。</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../../../img/gnn/egcConv_paper/study_over_H_B.png" alt="image-20220804201339851" width="1086" height="542" /> <em>图3：<code class="language-plaintext highlighter-rouge">heads</code>数量$H$和<code class="language-plaintext highlighter-rouge">bases</code>数量$B$的研究。在<code class="language-plaintext highlighter-rouge">ZINC</code>数据集上运行<code class="language-plaintext highlighter-rouge">EGC-S</code>的研究。度量标准是<code class="language-plaintext highlighter-rouge">MAE</code>(平均值±标准偏差)：越低越好。我们研究保持总数参数量不变，调整隐藏层数。每个实验单独微调。设置$B&gt;H$不一定会提高效果，因为存在过拟合的风险，使用较小的隐藏层数来保持恒定的参数量。</em></p><p>这些参数之间的关系非常重要。有几个方面需要考虑：</p><ul><li>增加 $H$ 和 $B$ 意味着我们花费更多的参数预算来创建组合，这减少了隐藏层数，如图3所示。如果我们使用多个聚合器，情况会更糟：我们的组合维度必须是 $HB \vert \mathcal{A} \vert$。<li>增加 $B$ 意味着我们必须大幅减少隐藏层大小，因为它相当于于增加更多大小为 $\frac{F_{‘}}{H} \times F$ 的权重。<li>增加 $H$ 允许我们增加隐藏层大小，这是因为每个基权重都变小了。我们在图3中看到，将 $B$ 增加到超过 $H$ 不会产生显著的性能改进：我们推测<code class="language-plaintext highlighter-rouge">bases begin specializing for individual heads</code>；通过共享，有一种正则化效应，如<code class="language-plaintext highlighter-rouge">Schlichtkrull</code>等人所观察到的。这种正则化有助于稳定优化器，我们观察到较小的 $B$ 的试验方差也较低。</ul><p>我们建议 $B=H$ 或 $B=\frac{H}{2}$。我们发现 $H=8$ 对<code class="language-plaintext highlighter-rouge">EGC-S</code>有效；对于<code class="language-plaintext highlighter-rouge">EGC-M</code>，这个配置会使得在组合权重上花费了更多的参数，我们建议设置 $H=4$。表1中始终适用此约定；附录和规范中提供了完整的详细信息。</p><h3 id="是否应激活组合权重w">是否应激活组合权重($w$)？</h3><p>任何激活函数都会缩小每个节点权重 $\Theta_{h}^{(i)}$ 所处的空间，因此我们预计它会损害性能，表2中证实了这一点。激活 $w$ 可以提高训练稳定性，但我们在实验中没有发现这是一个问题。另一个问题是，不同的聚合器会导致输出具有不同的平均值和方差，因此需要通过不同的因子进行缩放，以进行组合。</p><div class="table-wrapper"><table><thead><tr><th>Activation<th style="text-align: left">EGC-S<th>EGC-M<tbody><tr><td>Identity<td style="text-align: left">0.364 ± 0.020<td>0.281 ± 0.007<tr><td>Hardtanh <br />Sigmoid <br />Softmax<td style="text-align: left">0.435 ± 0.010<br />0.366 ± 0.008<br />0.404 ± 0.010<td>0.293 ± 0.013<br />0.303 ± 0.016<br />0.307 ± 0.013</table></div><p>​ <em>表2：激活组合权重 $w$ 会损害性能。在<code class="language-plaintext highlighter-rouge">ZINC</code>上运行，越低越好。</em></p><h3 id="egc在大规模异构图中的应用">EGC在大规模异构图中的应用</h3><p>我们在异构数据集<code class="language-plaintext highlighter-rouge">OGB-MAG</code>上评估了<code class="language-plaintext highlighter-rouge">EGC</code>，该数据集包含2M个节点和21M条边。在图的同构版本上，我们的性能超过基线1.5-2%；我们对对称归一化(<code class="language-plaintext highlighter-rouge">EGC-S</code>)和聚合器<code class="language-plaintext highlighter-rouge">mean</code>进行了实验，以证明<code class="language-plaintext highlighter-rouge">EGC</code>的机制是有效的，无论哪个聚合器为数据集提供了更强的<code class="language-plaintext highlighter-rouge">inductive bias</code>。我们的架构可以扩展以处理不同的边类型，从而产生<code class="language-plaintext highlighter-rouge">R-EGC</code>架构，其性能比<code class="language-plaintext highlighter-rouge">R-GCN</code>提高了0.9%。如下表所示：</p><div class="table-wrapper"><table><thead><tr><th>Methed<th>Test Accuracy % $\uparrow$<tbody><tr><td><code class="language-plaintext highlighter-rouge">MLP</code><td>26.92 ± 0.26<tr><td><code class="language-plaintext highlighter-rouge">GCN</code><td>30.43 ± 0.25<tr><td><code class="language-plaintext highlighter-rouge">GraphSAGE-Mean</code><td>31.53 ± 0.15<tr><td><code class="language-plaintext highlighter-rouge">EGC-S</code><td>32.13 ± 0.73<tr><td><code class="language-plaintext highlighter-rouge">EGC</code> ($\odot$ = <code class="language-plaintext highlighter-rouge">Mean</code>)<td>33.22 ± 0.50<tr><td><code class="language-plaintext highlighter-rouge">R-GCN (Full Batch)</code><td>46.29 ± 0.45<tr><td><code class="language-plaintext highlighter-rouge">R-EGC (Full Batch)</code><td>47.21 ± 0.40</table></div><p>我们期望通过采样技术来正则化优化，或使用预训练的<code class="language-plaintext highlighter-rouge">embeddings</code>，来进一步提高精度；然而，添加这些技术会使比较结果更加困难，因为众所周知，采样技术会对每种体系结构的性能产生不同的影响。</p><h2 id="内存和时延基准">内存和时延基准</h2><p>现在我们评估我们模型的资源效率。对于<code class="language-plaintext highlighter-rouge">CPU</code>，我们使用<code class="language-plaintext highlighter-rouge">Intel Xeon Gold 5218</code>，对于<code class="language-plaintext highlighter-rouge">GPU</code>，我们使用<code class="language-plaintext highlighter-rouge">Nvidia RTX 8000</code>。</p><h3 id="聚合器融合-1">聚合器融合</h3><p>我们评估了<code class="language-plaintext highlighter-rouge">CPU</code>和<code class="language-plaintext highlighter-rouge">GPU</code>上几种拓扑结构的聚合器融合。出于篇幅原因，我们将全部细节留给附录D。总之，我们观察到，在标准<code class="language-plaintext highlighter-rouge">SpMM</code>上使用3个聚合器只会产生14%的额外开销，使我们能够在推理时提高模型性能，而不会产生过多的计算开销。</p><h3 id="端到端延迟">端到端延迟</h3><p>我们在表4中提供了参数规范化模型的延迟和内存统计信息。我们需要注意对于训练和推理，\(\mathcal{O}(E)\) 的模型的速度是多么的慢，内存操作是多么的密集。读者应该注意到，<code class="language-plaintext highlighter-rouge">MPNN</code>和<code class="language-plaintext highlighter-rouge">PNA</code>的推理延迟和内存消耗相对于<code class="language-plaintext highlighter-rouge">EGC</code>增加了6-7倍，对应于<code class="language-plaintext highlighter-rouge">Arxiv</code>数据集上的 \(\frac{\lvert E \rvert}{\lvert V \rvert }\)。与最相近的竞品<code class="language-plaintext highlighter-rouge">PNA</code>相比，<code class="language-plaintext highlighter-rouge">EGC-M</code>提供了显著更低的延迟和内存消耗，但确切的好处取决于数据集。扩展结果见附录E，包括以下结果：<code class="language-plaintext highlighter-rouge">Arxiv</code>上的精度标准化模型，进一步证明了<code class="language-plaintext highlighter-rouge">EGC</code>的资源效率。</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../../../img/gnn/egcConv_paper/mem_and_latency.png" alt="image-20220806101141737" width="1086" height="542" /></p><h1 id="讨论及结论">讨论及结论</h1><h3 id="我们的结果有多令人惊讶">我们的结果有多令人惊讶？</h3><p>我们观察到，设计一个在6个基准上与最先进的各向异性<code class="language-plaintext highlighter-rouge">GNN</code>模型竞争的各向同性<code class="language-plaintext highlighter-rouge">GNN</code>模型是可能的。这一结果与<code class="language-plaintext highlighter-rouge">GNN</code>社区的普遍看法相矛盾。然而，我们的结果可能被视为<code class="language-plaintext highlighter-rouge">ML</code>社区中可见模式的一部分：<code class="language-plaintext highlighter-rouge">NLP</code>和<code class="language-plaintext highlighter-rouge">CV</code>社区都看到了表明<code class="language-plaintext highlighter-rouge">Transformer</code>提供的各向异性不是必须的，这与我们的观察结果一致。</p><p>值得一问的是，鉴于我们观察到的结果与理论上已经证明的性质相矛盾，我们为什么要观察这些结果。我们相信有多种原因，但最重要的是，大多数真实世界的数据集不需要这些更具表达力的模型提供的理论能力来获得良好的结果。特别是，许多真实世界的数据集是同质的：因此，像<code class="language-plaintext highlighter-rouge">EGC</code>这样的简单方法可以实现这一点高性能。这意味着社区应考虑向标准基准添加更复杂的数据集，例如<code class="language-plaintext highlighter-rouge">Lim</code>等人和<code class="language-plaintext highlighter-rouge">Velickoviˇc</code>等人中提出的数据集。</p><p>我们提出的<code class="language-plaintext highlighter-rouge">EGC</code>可以作为现有<code class="language-plaintext highlighter-rouge">GNN</code>层的替代品，与优秀的基准相比，它在6个基准数据集上实现了更好的结果，同时大大降低了资源消耗。我们的工作为研究界提出了重要问题，同时在资源消耗方面提供了重大的实际好处。我们相信我们工作的下一步是合并边缘特征，例如通过<code class="language-plaintext highlighter-rouge">line graphs</code>或拓扑消息传递。</p><h1 id="附录a进一步实验细节">附录A：进一步实验细节</h1><h2 id="确保公平">确保公平</h2><p>我们扩展了我们的实验方案，特别侧重于描述我们采取的措施，以确保我们报告的结果不会不公平地偏向<code class="language-plaintext highlighter-rouge">EGC</code>。</p><p>对于<code class="language-plaintext highlighter-rouge">EGC-S</code>，我们使用 $H=8$ 和 $B=4$，as implied by our ablation for all experiments，只有一个例外，即<code class="language-plaintext highlighter-rouge">OGB Code</code>使用 $H=B=8$。使用较小的<code class="language-plaintext highlighter-rouge">bases</code>的集合的好处是我们可以增加隐藏层维度，但在这种情况下，这是不可行的，因为模型中的11M参数的大多数对应于令牌读取层，其随着模型隐藏维度的增长而快速增加。如第5.3节<code class="language-plaintext highlighter-rouge">附加研究</code>所示，如果我们不能增加隐藏维度，最好增加<code class="language-plaintext highlighter-rouge">bases</code>。这是我们唯一的例外。</p><p>对于<code class="language-plaintext highlighter-rouge">EGC-M</code>，所有实验中都使用 $H=B=4$。主要挑战是聚合器的选择，这仍然是我们工作的主要挑战。我们无法找到一种令人满意的自动发现聚合器选择的技术，因此我们依赖启发式来找到它们。我们限制每个模型使用3个聚合器（产生35个可能的选择）。为了确定聚合器，我们使用了两种启发式方法：</p><ul><li>聚合器应该是“多样化的”<li>聚合器的选择应该基于任务的归纳偏差。</ul><p>使用这两条规则，我们尝试最多3种可能的聚合器选择；所有考虑的选择如表6所示。我们注意到，虽然一些选择确实提高了性能，但我们的结论并没有失效；还值得注意的是，可能会找到更好的聚合器选择。</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../../../img/gnn/egcConv_paper/agg_for_egcm.png" alt="image-20220806173620154" width="1086" height="542" /> <em>表6：为<code class="language-plaintext highlighter-rouge">EGC-M</code>尝试的聚合器组合。尝试了最多3种组合。我们的结论没有改变，且可能通过更优的聚合器选择达到更好的结果。</em></p><h2 id="集群详情">集群详情</h2><p>我们的大多数实验都是在<code class="language-plaintext highlighter-rouge">SLURM</code>集群中使用<code class="language-plaintext highlighter-rouge">Intel CPU</code>和<code class="language-plaintext highlighter-rouge">NVIDIA GPU</code>的几台机器上运行的。每台机器都运行<code class="language-plaintext highlighter-rouge">Ubuntu 18.04</code>。我们集群中的<code class="language-plaintext highlighter-rouge">GPU</code>型号是<code class="language-plaintext highlighter-rouge">RTX 2080Ti</code>和<code class="language-plaintext highlighter-rouge">GTX 1080Ti</code>。我们在集群中的<code class="language-plaintext highlighter-rouge">V100</code>和我们访问的<code class="language-plaintext highlighter-rouge">RTX 8000</code>虚拟机上运行了高内存实验。</p><h1 id="附录b现有的局限性">附录B：现有的局限性</h1><p>正如相关工作中所解释的，现有的提高<code class="language-plaintext highlighter-rouge">GNN</code>效率的方法具有严重的局限性。在本节中，我们将详细阐述它们，并在必要时提供实验证据。</p><p>我们首先研究<code class="language-plaintext highlighter-rouge">graph-augmented</code> <code class="language-plaintext highlighter-rouge">MLPs</code>(<code class="language-plaintext highlighter-rouge">GA MLPs</code>)，据我们所知，当这些模型应用于需要泛化到看不见的图的问题时，很少有实验结果评估这些模型的性能。在文献中，它们通常适用于大规模节点分类基准，如<code class="language-plaintext highlighter-rouge">OGB</code>基准套件中的那些。众所周知，这些模型在理论上比标准<code class="language-plaintext highlighter-rouge">GNN</code>表达能力差，但是当应用于节点分类数据集时，它们的性能是可以接受的。</p><p>我们考虑以下形式的模型：</p>\[\mathbf{Y}=\text{Readout}(\text{MLP}({\LARGE ||}_{k=0}^{4} \mathbf{S}^{k} \mathbf{XW}))\]<p>我们使用扩散算子(<code class="language-plaintext highlighter-rouge">ScatterXXX</code>算子？) $\mathbf{S}$ 的四次方来模拟相应<code class="language-plaintext highlighter-rouge">GNN</code>深度，这些<code class="language-plaintext highlighter-rouge">GNN</code>都使用4层。我们将 $\mathbf{S}$ 设置为使用对称归一化，如<code class="language-plaintext highlighter-rouge">GCN</code>和<code class="language-plaintext highlighter-rouge">EGC-S</code>所用；我们还考虑将 $\mathbf{S}$ 设置为<code class="language-plaintext highlighter-rouge">ZINC</code>上的邻接矩阵，以模拟<code class="language-plaintext highlighter-rouge">GIN</code>使用的操作。结果如表7所示。</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../../../img/gnn/egcConv_paper/apply_ga-mlp.png" alt="image-20220806173303961" width="1086" height="542" /> <em>表7：将图<code class="language-plaintext highlighter-rouge">graph-augmented</code> <code class="language-plaintext highlighter-rouge">MLP</code>(<code class="language-plaintext highlighter-rouge">GA MLP</code>)应用于需要泛化到看不见图的任务的结果。我们发现，性能与相应的<code class="language-plaintext highlighter-rouge">GCN</code>模型大致相似，远低于使用相同聚合器的<code class="language-plaintext highlighter-rouge">EGC-S</code>。我们还考虑了在<code class="language-plaintext highlighter-rouge">ZINC</code>上使用<code class="language-plaintext highlighter-rouge">add</code>聚合器，获得了<code class="language-plaintext highlighter-rouge">0.444±0.019</code>。相比之下，等效<code class="language-plaintext highlighter-rouge">GIN</code>模型（使用<code class="language-plaintext highlighter-rouge">add</code>聚合器)获得了<code class="language-plaintext highlighter-rouge">0.387±0.015</code>。</em></p><p>我们发现<code class="language-plaintext highlighter-rouge">GA-MLP</code>模型的性能与相应的<code class="language-plaintext highlighter-rouge">GNN</code>基线类似（但往往更差）。这些模型与<code class="language-plaintext highlighter-rouge">GAT</code>或<code class="language-plaintext highlighter-rouge">MPNN</code>等方法比没有竞争力，并且<code class="language-plaintext highlighter-rouge">EGC-S</code>的性能大大优于它们。至少根据我们目前对这些模型的理解，使用<code class="language-plaintext highlighter-rouge">GA-MLP</code>实现最先进的性能似乎是不可能的。</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../../../img/gnn/egcConv_paper/sample_egcs_result.png" alt="image-20220806164839322" width="1086" height="542" /> <em>表8：将采样方法应用于<code class="language-plaintext highlighter-rouge">EGC-S</code>的结果。我们在测试时不使能采样，因此这些方法不会减少计算的测试时间。原则上，采样适用于任何基础<code class="language-plaintext highlighter-rouge">GNN</code>：我们的结论将转移到其他基础<code class="language-plaintext highlighter-rouge">GNN</code>。我们发现，具有低<code class="language-plaintext highlighter-rouge">drop</code>概率($p=0.1$)的<code class="language-plaintext highlighter-rouge">DropEdge</code>可以提升模型性能；然而，将 $p$ 设置如此低并不会显著减少内存消耗或计算量。将 $p$ 增加到<code class="language-plaintext highlighter-rouge">0.5</code>确实会显著减少资源消耗，但会导致模型效果显著下降。</em></p><p>我们现在继续研究采样：每个训练步骤不是在整图上运行，而是在一些子图上运行。当应用于节点分类等任务时，这些方法非常流行，它们能够提供超过20倍的采样率，因此显著改善了内存消耗（大规模训练的主要限制）。然而，我们注意到，对于许多拓扑图，这些方法的收益尚未仔细检查；虽然它们已被证明对许多图中出现的许多“小世界”图拓扑有效，但并非所有图都属于这一类别。例如，分子图不适合这一类别。</p><p>我们首先通过将<code class="language-plaintext highlighter-rouge">GraphSAINT</code>的采样策略应用于<code class="language-plaintext highlighter-rouge">EGC-S</code>模型来评估它。实验中，我们使用了以节点为中心的采样器。结果见表8；我们在测试时禁用采样。即使具有相对低的10%的<code class="language-plaintext highlighter-rouge">dropping</code>概率，模型性能下降严重。我们注意到，当使用这种<code class="language-plaintext highlighter-rouge">dropping</code>概率时，所节省的计算量是适度的。当使用<code class="language-plaintext highlighter-rouge">GraphSAINT</code>的以边为中心的采样器时，我们也观察到了类似的结果。</p><p>我们还尝试了<code class="language-plaintext highlighter-rouge">DropEdge</code>的不同采样方法，如前所述，我们将其应用于<code class="language-plaintext highlighter-rouge">EGC-S</code>模型。在这个简单的方案中，删除了邻接矩阵的元素；该方案被证明对训练深图网络是有效的，但它也有助于减少模型的计算量，因为它有效地减少了必须计算的消息数量。表8中提供了结果。结果明显优于使用<code class="language-plaintext highlighter-rouge">GraphSAINT</code>采样策略时观察到的结果：在10%的<code class="language-plaintext highlighter-rouge">dropping</code>概率下，由于正则化效应，我们甚至在某些情况下看到了改善。然而，一旦我们将<code class="language-plaintext highlighter-rouge">dropping</code>概率提高到可以观察到计算需求显著减少的水平，我们就会观察到模型效果下降。总之，虽然<code class="language-plaintext highlighter-rouge">DropEdge</code>对于许多归纳任务来说是一种更有效的采样策略，但作为一种减少计算负担的方法，它并不有益。</p><p>最后，我们讨论了文献中其他方法的局限性。量化是一种通常应用于推理的方法；混合精度方法可以在训练时应用，但必须对于<code class="language-plaintext highlighter-rouge">GNN</code>必须要留意，以避免梯度偏移。此外，如果搜索目标设置得当，<code class="language-plaintext highlighter-rouge"> neural architecture search</code>(<code class="language-plaintext highlighter-rouge">NAS</code>)可能有助于找到内存高效的模型，但它们受到一些限制，主要是搜索时间和内存消耗。最后，可逆残差等方法对架构设计很有用，它们不能解决诸如消息传递步骤引起的较高峰值内存等问题。</p><h1 id="附录cgnn的硬件加速方法">附录C：GNN的硬件加速方法</h1><p>读者应该注意到，大多数现有的<code class="language-plaintext highlighter-rouge">GNN</code>硬件加速工作只支持<code class="language-plaintext highlighter-rouge">GNN</code>的一个子集：具体来说，他们只支持可以使用<code class="language-plaintext highlighter-rouge">SpMM</code>实现的模型。包括陈等、严等、游等和张等的文献中的方法都局限于这个领域。可以为加速器增加更大的灵活性，以支持更具表现力的消息传递方案，但这必然意味着更大的复杂性。正如阿姆达尔定律所暗示的，增加灵活性可能会降低峰值性能，同时增加硅面积要求。因此，以最简单的原语为目标（正如我们对<code class="language-plaintext highlighter-rouge">EGC</code>所做的那样）是获得硬件加速的最明智的方法。</p><h1 id="附录d聚合器融合">附录D：聚合器融合</h1><p>顺序执行每个聚合器的朴素方法将导致时延按照 \(\lvert \mathcal{A} \rvert\) 线性增加。然而需要注意的一个关键点，我们是内存瓶颈<code class="language-plaintext highlighter-rouge">memory bpund</code>的：稀疏操作的瓶颈是等待数据从内存中到达。这一观察结果适用于<code class="language-plaintext highlighter-rouge">GPU</code>和<code class="language-plaintext highlighter-rouge">CPU</code>，并通过<code class="language-plaintext highlighter-rouge">profiiing</code>证明了其合理性。使用<code class="language-plaintext highlighter-rouge">GTX 1080Ti</code>上的<code class="language-plaintext highlighter-rouge">profiler</code>，我们观察到使用<code class="language-plaintext highlighter-rouge">Reddit graph Hamilton</code>等人的<code class="language-plaintext highlighter-rouge">SpMM</code>（特征尺寸为256）仅实现了<code class="language-plaintext highlighter-rouge">GPU</code> <code class="language-plaintext highlighter-rouge">FLOPS</code>峰值的1.2%，88.5%的<code class="language-plaintext highlighter-rouge">stall</code>是由未满足的内存依赖性造成的。最快的处理顺序是对已从内存中提取的数据执行尽可能多的工作，而不是多次提取。算法1说明了这一概念。我们可以对<code class="language-plaintext highlighter-rouge">standard compressed sparse row</code> (<code class="language-plaintext highlighter-rouge">CSR</code>) <code class="language-plaintext highlighter-rouge">SpMM</code>算法做轻量修改，来执行所有的聚合器。</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../../../img/gnn/egcConv_paper/aggregator_fusion_algorithm.png" alt="image-20220806152705749" width="1086" height="542" /> <em>聚合器融合算法</em></p><p>我们所做的第二个观察是，没必要在推理时保存所有聚合的结果。注意<code class="language-plaintext highlighter-rouge">CSR</code> <code class="language-plaintext highlighter-rouge">SpMM</code>算法按顺序处理输出矩阵中的每一行：与其存储每一行的聚合，不如只存储加权的结果。这种方法不仅减少了内存消耗，还减少了延迟，因为我们提高了缓存效率并减少了内存系统竞争。在实践中，当对具有更频繁，计算密集型周期的拓扑进行推理时，这种优化尤其重要，因为这样可以减少负载和存储单元之间的争用。算法1也证明了这一点。</p><p>我们在<code class="language-plaintext highlighter-rouge">CPU</code>和<code class="language-plaintext highlighter-rouge">GPU</code>上评估了四种不同拓扑的聚合器融合，结果在表9中。我们假设所有运算都是32位浮点运算，并且我们使用了三个聚合器：<code class="language-plaintext highlighter-rouge">sum</code>、<code class="language-plaintext highlighter-rouge">max</code>和<code class="language-plaintext highlighter-rouge">min</code>；这些与<code class="language-plaintext highlighter-rouge">EGC-M</code> <code class="language-plaintext highlighter-rouge">Code</code>用的聚合器匹配。我们的基准测试是在来自<code class="language-plaintext highlighter-rouge">ZINC</code>、<code class="language-plaintext highlighter-rouge">Code</code> 、<code class="language-plaintext highlighter-rouge">Arxiv</code>以及流行的<code class="language-plaintext highlighter-rouge">Reddit</code>数据集(<code class="language-plaintext highlighter-rouge">GNN</code>文献中常用的最大图数据集之一)的10k个图上进行的。我们<code class="language-plaintext highlighter-rouge">GPU</code>上的<code class="language-plaintext highlighter-rouge">SpMM</code>实现基于Yang等人。具体实现代码在我们的代码仓里。</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../../../img/gnn/egcConv_paper/infer_latency_for_csr_spmm.png" alt="image-20220806160657156" width="1086" height="542" /> <em>表9: <code class="language-plaintext highlighter-rouge">GCN/GIN</code>使用的<code class="language-plaintext highlighter-rouge">CSR</code> <code class="language-plaintext highlighter-rouge">SpMM</code>和聚合器融合的推理时延(平均值和标准偏差)。假设算法1的特征维数为256，$H=B=1$。我们观察到，在最坏的情况下，聚合器融合导致结果增加了34%；相反，简单实现在最坏情况下增加了466%。还包括与权重方阵密集相乘的计时，我们观察到稀疏操作占时延的大头。</em></p><p>正如预期，我们的输入重用优化技术实现了比应用多聚合器的简单方法更低的推理延迟。简单方法导致延迟平均增加331%，我们的方法与<code class="language-plaintext highlighter-rouge">GCN</code>和<code class="language-plaintext highlighter-rouge">GIN</code>使用的普通<code class="language-plaintext highlighter-rouge">SpMM</code>相比，平均增加仅14%。这种增加取决于拓扑结构，对于内存限制较少的拓扑结构，可以观察到延迟的更大增加。我们还提供了密集矩阵乘法（例如 $\mathbf{X \Theta}$）的计时，以证明我们在这项工作中对优化稀疏运算的观点：<code class="language-plaintext highlighter-rouge">CSR</code> <code class="language-plaintext highlighter-rouge">SpMM</code>运算比相应的权重乘法慢4.7倍()<code class="language-plaintext highlighter-rouge">geomean</code>)。我们相信，通过使用自动调优框架(如<code class="language-plaintext highlighter-rouge">TVM</code>)，可以实现进一步优化，但这超出了本工作的范围。</p><h1 id="附录e其他数据集上的延迟和内存消耗">附录E：其他数据集上的延迟和内存消耗</h1><p>在评估中，我们评估了<code class="language-plaintext highlighter-rouge">Arxiv</code>上参数归一化模型的内存消耗和延迟。本节中，我们将考虑<code class="language-plaintext highlighter-rouge">OGB</code> <code class="language-plaintext highlighter-rouge">CODE</code>模型的类似练习。结果如表10所示。</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../../../img/gnn/egcConv_paper/latency_mem_on_codev2.png" alt="image-20220806161855512" width="1086" height="542" /><em>表10: <code class="language-plaintext highlighter-rouge">OGB</code> <code class="language-plaintext highlighter-rouge">Code-V2</code>上参数归一化模型的延迟和内存结果。尽管相对于<code class="language-plaintext highlighter-rouge">Arxiv</code>比率较低，为2.75，但我们发现在<code class="language-plaintext highlighter-rouge">Arxiv</code>中观察到的趋势大致保持不变。值得再次注意的是，<code class="language-plaintext highlighter-rouge">EGC-M</code>在延迟和内存方面都比<code class="language-plaintext highlighter-rouge">PNA</code>有效得多。</em></p><p>我们的结论大致相似，与<code class="language-plaintext highlighter-rouge">PNA</code>相比，<code class="language-plaintext highlighter-rouge">EGC-M</code>在内存消耗、延迟和参数效率方面有明显改善。<code class="language-plaintext highlighter-rouge">EGC-S</code>优于<code class="language-plaintext highlighter-rouge">GAT</code>，具有相似的推理延迟、更好的模型性能和显著更低的内存消耗。我们使用<code class="language-plaintext highlighter-rouge">batch size = 128</code>来训练模型；读者应该注意到，由于图的节点和边的数量不同，不同次运行之间的内存消耗数字可能会有所不同（即使对于相同的模型也是如此）。</p><p>目前为止，我们还没有证明我们的方法在<code class="language-plaintext highlighter-rouge">Arxiv</code>上比基线更有效，<code class="language-plaintext highlighter-rouge">Arxiv</code>是唯一一个<code class="language-plaintext highlighter-rouge">EGC-M</code>和<code class="language-plaintext highlighter-rouge">PNA</code>表现不是最佳的数据集。为了证明我们的效率更高，我们必须证明我们在给定精度水平下实现了更低的内存消耗和延迟，即我们必须考虑精度标准化的模型。我们评估增加基线模型的参数计数，直到它们达到与<code class="language-plaintext highlighter-rouge">EGC-S</code>相同的精度；</p><p>我们还通过增加超参搜索，使这些模型比我们的方法有额外的优势。结果如表11所示，我们观察到一旦模型达到相同精度，<code class="language-plaintext highlighter-rouge">EGC-S</code>更有效。</p><div class="table-wrapper"><table><thead><tr><th>精度归一化的模型<th>参数<th>GPU训练时延(ms)<th>GPU推理时延 (ms)<th>内存峰值(MB)<tbody><tr><td><code class="language-plaintext highlighter-rouge">GCN</code><td>184k<td>208.6 ± 2.2<td>44.9 ± 0.4<td>2549<tr><td><code class="language-plaintext highlighter-rouge">GIN</code><td>FAIL<td>FAIL<td>FAIL<td>FAIL<tr><td><code class="language-plaintext highlighter-rouge">GraphSAGE</code><td>593k<td>335.8 ± 2.4<td>60.2 ± 0.2<td>3208<tr><td><strong><code class="language-plaintext highlighter-rouge">EGC-S</code></strong><td><strong>100k</strong><td><strong>77.7 ± 2.2</strong><td><strong>37.3 ± 0.1</strong><td><strong>2430</strong></table></div><p><em>表11:<code class="language-plaintext highlighter-rouge">Arxiv</code>上精度标准化模型的延迟和内存统计。为了实现上述<code class="language-plaintext highlighter-rouge">EGC-S</code>一样的精度，我们必须提高基线模型的规模。我们观察到<code class="language-plaintext highlighter-rouge">EGC-S</code>在给定精度水平下，显著降低了内存消耗和延迟。</em></p><p>读者应该注意到，通过将注意力向量的左半部和右半部分别计算，并酌情相加，<code class="language-plaintext highlighter-rouge">GAT</code>(而不是<code class="language-plaintext highlighter-rouge">GATv2</code>)的实现可以减少内存消耗。我们在实验中使用了<code class="language-plaintext highlighter-rouge">GAT</code>的优化实现(来自<code class="language-plaintext highlighter-rouge">PyG</code>)；读者可以参考实现以了解更多细节。</p><h1 id="附录f推广到异构图">附录F：推广到异构图</h1><p>我们的<code class="language-plaintext highlighter-rouge">R-EGC</code>模型类似于<code class="language-plaintext highlighter-rouge">OGB</code>库中包含的基线<code class="language-plaintext highlighter-rouge">R-GCN</code>模型。<code class="language-plaintext highlighter-rouge">OGB</code>模型不同于<code class="language-plaintext highlighter-rouge">R-GCN</code>的标准定义，因为它处理不同的节点类型，而不仅仅是边类型。基线模型具有用于为每个关系类型生成消息的权重，以及用于更新每个单独节点类型的权重矩阵。这对应于：</p>\[\mathbf{y}^{(i)}=\mathbf{\Theta}_{\eta}\mathbf{x}^{(i)}+\sum_{r \in \mathcal{R}} \frac{1}{|\mathcal{N}_{r}(i)|} \sum_{j \in \mathcal{N}_{r}(i)}\mathbf{\Theta}_{\mathcal{r}} \mathbf{x}^{(j)}\]<p>其中 $\eta$ 对应于节点 $i$ 的节点类型，$\mathcal{R}$ 表示关系类型的集合。注意使用聚合器<code class="language-plaintext highlighter-rouge">mean</code>。</p><p>我们通过使用单一组基权重基线来改变基线。相反，我们使用了一种每个节点和关系类型都不同的加权计算层 $\mathbf{w}^{(i)}=\mathbf{\Phi x} + \mathbf{b}$。</p>\[\mathbf{y}^{(i)}={\LARGE ||}_{h=1}^{H} \sum_{b=1}^{B} w_{\eta,h,b}^{(i)} \mathbf{\Theta}_{b} \mathbf{x}^{(j)} + \sum_{\mathcal{r} \in \mathcal{R}}\frac{1}{|\mathcal{N}_{r}(i)|} {\LARGE ||}_{h=1}^{H} \sum_{b=1}^{B} w_{\mathcal{r},h,b}^{(i)} \sum_{j \in \mathcal{N}(i)}\mathbf{\Theta}_{b} \mathbf{x}^{(j)}\]</div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/gnn/'>GNN</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/egconv/" class="post-tag no-text-decoration" >EGConv</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=EGConv-DO WE NEED ANISOTROPIC GRAPH NEURAL NETWORKS - zuo&url=https://dazuozcy.github.io/posts/EGConv/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=EGConv-DO WE NEED ANISOTROPIC GRAPH NEURAL NETWORKS - zuo&u=https://dazuozcy.github.io/posts/EGConv/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=EGConv-DO WE NEED ANISOTROPIC GRAPH NEURAL NETWORKS - zuo&url=https://dazuozcy.github.io/posts/EGConv/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/simd/">SIMD</a><li><a href="/posts/new-from-cpp11/">C++11开始的新特性</a><li><a href="/posts/keywords/">关键字</a><li><a href="/posts/empty-class/">class的内存模型</a><li><a href="/posts/diff-between-c-cpp/">C++ vs C</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/gpu/">GPU</a> <a class="post-tag" href="/tags/cuda/">CUDA</a> <a class="post-tag" href="/tags/openmp/">openMP</a> <a class="post-tag" href="/tags/c/">C++</a> <a class="post-tag" href="/tags/parallel-computing/">Parallel Computing</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/forecasting-at-scale/">Forecasting at Scale</a> <a class="post-tag" href="/tags/onednn/">oneDNN</a> <a class="post-tag" href="/tags/%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88/">智能指针</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/GAT/"><div class="card-body"> <span class="timeago small" > Jul 2, 2020 <i class="unloaded">2020-07-02T20:19:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>GAT</h3><div class="text-muted small"><p> GCN缺点： 模型对于同阶邻域上分配给不同邻居的权重是完全相同的（也就是GAT论文里说的：无法允许为邻居中的不同节点指定不同的权重）。这一点限制了模型对于空间信息的相关性的捕捉能力，这也是在很多任务上不如GAT的根本原因。 GCN结合临近节点特征的方式和图的结构依依相关，这局限了训练所得模型在其他图结构上的泛化能力。 GAT提出了用注意力机制对邻近节点特征加权求和。邻近节点特...</p></div></div></a></div><div class="card"> <a href="/posts/GCN/"><div class="card-body"> <span class="timeago small" > Jul 2, 2020 <i class="unloaded">2020-07-02T20:19:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>GCN</h3><div class="text-muted small"><p> Graph Convolutional Networks(GCN): 图卷积神经网络，实际上跟CNN的作用类似，就是一个特征提取器，只不过它的对象是图数据。GCN精妙地设计了一种从图数据中提取特征的方法，从而让我们可以使用这些特征去对图数据进行：节点分类 、图分类、边预测，还可顺便得到图的嵌入表示。 【图结构】之图神经网络GCN详解 图卷积网络(Graph Convolutional N...</p></div></div></a></div><div class="card"> <a href="/posts/GNN/"><div class="card-body"> <span class="timeago small" > Jul 2, 2020 <i class="unloaded">2020-07-02T20:19:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>GNN</h3><div class="text-muted small"><p> CNN系列： 做图像识别时，对象是图片，是一个二维的结构，于是人们发明了CNN这种神奇的模型来提取图片的特征。CNN的核心在于它的kernel，kernel是一个个小窗口，在图片上平移，通过卷积的方式来提取特征。这里的关键在于图片结构上的平移不变性： 一个小窗口无论移动到图片的哪一个位置，其内部的结构都是一模一样的，因此是CNN可以实现所在。 RNN系列:...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/ASTGCN/" class="btn btn-outline-primary" prompt="Older"><p>ASTGCN-Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting</p></a> <a href="/posts/GATConv/" class="btn btn-outline-primary" prompt="Newer"><p>GATConv-Graph Attention Networks</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/dazuozcy">zuo</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/gpu/">GPU</a> <a class="post-tag" href="/tags/cuda/">CUDA</a> <a class="post-tag" href="/tags/openmp/">openMP</a> <a class="post-tag" href="/tags/c/">C++</a> <a class="post-tag" href="/tags/parallel-computing/">Parallel Computing</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/forecasting-at-scale/">Forecasting at Scale</a> <a class="post-tag" href="/tags/onednn/">oneDNN</a> <a class="post-tag" href="/tags/%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88/">智能指针</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { let initTheme = "default"; if ($("html[mode=dark]").length > 0 || ($("html[mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://dazuozcy.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script async src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script>
