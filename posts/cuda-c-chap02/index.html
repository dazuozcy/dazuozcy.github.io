<!DOCTYPE html><html lang="en-US" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="读薄《Professional CUDA C Programming》——CUDA编程模型" /><meta name="author" content="dazuo" /><meta property="og:locale" content="en_US" /><meta name="description" content="二、CUDA编程模型" /><meta property="og:description" content="二、CUDA编程模型" /><link rel="canonical" href="https://dazuozcy.github.io/posts/cuda-c-chap02/" /><meta property="og:url" content="https://dazuozcy.github.io/posts/cuda-c-chap02/" /><meta property="og:site_name" content="zuo" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-07-01T20:19:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="读薄《Professional CUDA C Programming》——CUDA编程模型" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"dazuo"},"dateModified":"2022-10-31T22:37:31+08:00","datePublished":"2020-07-01T20:19:00+08:00","description":"二、CUDA编程模型","headline":"读薄《Professional CUDA C Programming》——CUDA编程模型","mainEntityOfPage":{"@type":"WebPage","@id":"https://dazuozcy.github.io/posts/cuda-c-chap02/"},"url":"https://dazuozcy.github.io/posts/cuda-c-chap02/"}</script><title>读薄《Professional CUDA C Programming》——CUDA编程模型 | zuo</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="zuo"><meta name="application-name" content="zuo"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" as="script"> <script async src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://cdn.jsdelivr.net/gh/cotes2020/chirpy-images/commons/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">zuo</a></div><div class="site-subtitle font-italic">感谢永远有歌把心境道破</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/dazuozcy" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['dazuozcy','163.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>读薄《Professional CUDA C Programming》——CUDA编程模型</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>读薄《Professional CUDA C Programming》——CUDA编程模型</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> dazuo </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Wed, Jul 1, 2020, 8:19 PM +0800" prep="on" > Jul 1, 2020 <i class="unloaded">2020-07-01T20:19:00+08:00</i> </span></div><div> <span> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Oct 31, 2022, 10:37 PM +0800" prefix="Updated " > Oct 31, 2022 <i class="unloaded">2022-10-31T22:37:31+08:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2875 words">15 min</span></div></div><div class="post-content"><h1 id="二cuda编程模型">二、CUDA编程模型</h1><h2 id="cuda编程模型概述">CUDA编程模型概述</h2><h3 id="cuda编程结构">CUDA编程结构</h3><p>站在程序员的角度，可以从以下几个不同层面来看待并行计算：</p><ul><li><strong>领域层</strong>(Domain level)<li><strong>逻辑层</strong>(Logic level)<li><strong>硬件层</strong>(Hardware level)</ul><p>这三个层面对应了并行计算编程的不同阶段：</p><ul><li>算法<strong>设计阶段</strong>，最关心的应是在<strong>领域层</strong>如何解析数据和函数，以便在并行环境中能正确、高效地解决问题。<li><strong>编码阶段</strong>，关注点应转向如何组织并发线程。在这个阶段，需要从<strong>逻辑层面</strong>来思考，以确保线程和计算能正确地解决问题。在C语言并行编程中，需要使用<code class="language-plaintext highlighter-rouge">pthreads</code>或<code class="language-plaintext highlighter-rouge">OpenMP</code>技术来显式地管理线程。CUDA提出了一个线程层次结构抽象的概念，以允许控制线程行为。<li>在<strong>硬件层</strong>，通过理解线程是如何映射到核心可以帮助提高其性能。</ul><p>CUDA编程模型主要是异步的，因此在GPU上进行的运算可以与主机-设备通信重叠。内核一旦被启动，管理权立刻返回给主机，释放CPU来执行由设备上运行的并行代码实现的额外的任务。</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/professional_cuda_c_programming/chap02/cuda_exe_flow.png" alt="cuda_exe_flow" width="1086" height="542" /></p><h3 id="内存管理">内存管理</h3><p><strong>CUDA运行时</strong>负责分配与释放设备内存， 并且在主机内存和设备内存之间传输数据。</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">标准C函数<th style="text-align: center">CUDA C函数<tbody><tr><td style="text-align: center"><code class="language-plaintext highlighter-rouge">malloc</code><td style="text-align: center"><code class="language-plaintext highlighter-rouge">cudaError_t cudaMalloc(void** devPtr, size_t size)</code><tr><td style="text-align: center"><code class="language-plaintext highlighter-rouge">free</code><td style="text-align: center"><code class="language-plaintext highlighter-rouge">cudaError_t cudaFree(void *devPtr)</code><tr><td style="text-align: center"><code class="language-plaintext highlighter-rouge">memset</code><td style="text-align: center"><code class="language-plaintext highlighter-rouge">cudaError_t cudaMemset(void* devPtr, int value, size_t count)</code><tr><td style="text-align: center"><code class="language-plaintext highlighter-rouge">memcpy</code><td style="text-align: center"><code class="language-plaintext highlighter-rouge">cudaError_t cudaMemcpy(void* dst, const void* src, size_t count, cudaMemcpyKind kind)</code><br />同步函数，会导致主机阻塞</table></div><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/professional_cuda_c_programming/chap02/gpu_memory_structure.png" alt="gpu_memory_structure" width="1086" height="542" /></p><p>上图是一个简化的GPU内存结构， 它主要包含两部分： <strong>全局内存</strong>和<strong>共享内存</strong>。全局内存类似于CPU的系统内存， 共享内存类似于CPU的缓存。 不过GPU的共享内存可以由CUDA C的kernel直接控制。</p><h3 id="线程管理">线程管理</h3><p>当kernel函数在host侧启动后， 它的执行会移动到device侧， 此时设备中会产生大量线程且每个线程都执行由kernel函数指定的语句。</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/professional_cuda_c_programming/chap02/two-level-thread-hierarchy.png" alt="two-level-thread-hierarchy" width="1086" height="542" /></p><p>一个<code class="language-plaintext highlighter-rouge">kernel</code>函数启动后产生的所有线程统称为一个<strong><code class="language-plaintext highlighter-rouge">Grid</code></strong>。 同一<strong><code class="language-plaintext highlighter-rouge">Grid</code></strong>的所有线程共享全局内存空间。一个<strong><code class="language-plaintext highlighter-rouge">Grid</code></strong>由多个<code class="language-plaintext highlighter-rouge">thread blocks</code>构成，一个<code class="language-plaintext highlighter-rouge">thread block</code>包含一组线程，同一线程块内的线程协作可以通过2种方式来实现：同步<code class="language-plaintext highlighter-rouge">Block-local synchronization</code>和共享内存<code class="language-plaintext highlighter-rouge">Block-local shared memory</code>。（不同block内的线程不能协作）</p><p>在CUDA程序中有两组不同的网格和块变量：手动定义的<code class="language-plaintext highlighter-rouge">dim3</code>数据类型和预定义的<code class="language-plaintext highlighter-rouge">uint3</code>数据类型。在主机端，作为内核调用的一部分，你可以使用<code class="language-plaintext highlighter-rouge">dim3</code>数据类型定义一个网格和块的维度。当执行核函数时，CUDA运行时会生成相应的内置预初始化的网格、块和线程变量，它们在核函数内均可被访问到且为<code class="language-plaintext highlighter-rouge">unit3</code>类型。手动定义的<code class="language-plaintext highlighter-rouge">dim3</code>类型的网格和块变量仅在主机端可见，而<code class="language-plaintext highlighter-rouge">unit3</code>类型的内置预初始化的网格和块变量仅在设备端可见。</p><h4 id="内置变量">内置变量</h4><ul><li><strong><code class="language-plaintext highlighter-rouge">gridDim</code></strong>, grid dimension, measured in blocks<li><strong><code class="language-plaintext highlighter-rouge">blockDim</code></strong>, block dimension, measured in threads<li><strong><code class="language-plaintext highlighter-rouge">blockIdx</code></strong>, block index within a grid<li><strong><code class="language-plaintext highlighter-rouge">threadIdx</code></strong>, thread index within a block</ul><h3 id="启动一个cuda核函数">启动一个CUDA核函数</h3><div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="kt">void</span> <span class="n">kernel_name</span> <span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span> <span class="n">block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">argument_list</span><span class="p">);</span>
</pre></table></code></div></div><p>执行配置的第一个值是网格维度， 也就是启动块的数目。 第二个值是块维度， 也就是每个块中线程的数目。</p><p>下图所示就是<code class="language-plaintext highlighter-rouge">void kernel_name &lt;&lt;&lt;4, 8&gt;&gt;&gt;(argument_list);</code>配置下的线程布局。</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1086 542'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/professional_cuda_c_programming/chap02/thread-orgnization.png" alt="thread-orgnization" width="1086" height="542" /></p><h4 id="cuda核函数的限制">CUDA核函数的限制</h4><ul><li>只能访问设备内存<li>必须具有void返回类型<li>不支持可变数量的参数<li>不支持静态变量<li>显式异步行为</ul><h4 id="验证kernel函数">验证kernel函数</h4><p>除了使用调试工具外， 还有两个非常简单实用的方法可以验证核函数。</p><ul><li>在<code class="language-plaintext highlighter-rouge">Fermi</code>及更高版本的设备端的核函数中使用<code class="language-plaintext highlighter-rouge">printf</code>函数。<li>可以将执行参数设置为<code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;1, 1&gt;&gt;&gt;</code>， 因此强制用<code class="language-plaintext highlighter-rouge">1</code>个块和<code class="language-plaintext highlighter-rouge">1</code>个线程执行核函数， 这模拟了串行执行。</ul><h2 id="给核函数计时">给核函数计时</h2><h3 id="用cpu计时器计时">用CPU计时器计时</h3><div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre><td class="rouge-code"><pre><span class="cp">#include</span> <span class="cpf">&lt;sys/time.h&gt;</span><span class="cp">
</span>
<span class="cp">#define CHECK(call)                                                            \
{                                                                              \
    const cudaError_t error = call;                                            \
    if (error != cudaSuccess)                                                  \
    {                                                                          \
        fprintf(stderr, "Error: %s:%d, ", __FILE__, __LINE__);                 \
        fprintf(stderr, "code: %d, reason: %s\n", error,                       \
                cudaGetErrorString(error));                                    \
        exit(1);                                                               \
    }                                                                          \
}
</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
    <span class="kt">double</span> <span class="n">iStart</span><span class="p">,</span> <span class="n">iElaps</span><span class="p">;</span>
    <span class="p">...</span>
    <span class="n">iStart</span> <span class="o">=</span> <span class="n">seconds</span><span class="p">();</span>
    <span class="n">sumArraysOnHost</span><span class="p">(</span><span class="n">h_A</span><span class="p">,</span> <span class="n">h_B</span><span class="p">,</span> <span class="n">hostRef</span><span class="p">,</span> <span class="n">nElem</span><span class="p">);</span>
    <span class="n">iElaps</span> <span class="o">=</span> <span class="n">seconds</span><span class="p">()</span> <span class="o">-</span> <span class="n">iStart</span><span class="p">;</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"sumArraysOnHost Time elapsed %f sec</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">iElaps</span><span class="p">);</span>
    <span class="p">...</span>
    <span class="n">iStart</span> <span class="o">=</span> <span class="n">seconds</span><span class="p">();</span>
    <span class="n">sumArraysOnGPU</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span> <span class="n">block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_A</span><span class="p">,</span> <span class="n">d_B</span><span class="p">,</span> <span class="n">d_C</span><span class="p">,</span> <span class="n">nElem</span><span class="p">);</span>
    <span class="n">CHECK</span><span class="p">(</span><span class="n">cudaDeviceSynchronize</span><span class="p">());</span>  <span class="c1">// 等待所有的GPU线程运行结束</span>
    <span class="n">iElaps</span> <span class="o">=</span> <span class="n">seconds</span><span class="p">()</span> <span class="o">-</span> <span class="n">iStart</span><span class="p">;</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"sumArraysOnGPU &lt;&lt;&lt;  %d, %d  &gt;&gt;&gt;  Time elapsed %f sec</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">grid</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">block</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">iElaps</span><span class="p">);</span>
    
    <span class="c1">// check kernel error</span>
    <span class="n">CHECK</span><span class="p">(</span><span class="n">cudaGetLastError</span><span class="p">())</span> <span class="p">;</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></table></code></div></div><h3 id="用nvprof工具计时">用<code class="language-plaintext highlighter-rouge">nvprof</code>工具计时</h3><p>自CUDA 5.0以来，NVIDIA提供了一个名为nvprof的命令行分析工具。</p><div class="language-shell highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="nv">$ </span>nvprof <span class="o">[</span>nvprof_args] &lt;application&gt; <span class="o">[</span>application_args]
</pre></table></code></div></div><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1024 512'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/professional_cuda_c_programming/chap02/nvprof-example.png" alt="thread-orgnization" width="1024" height="512" /></p><p>以上结果的前半部分来自于程序的输出，后半部分来自于<code class="language-plaintext highlighter-rouge">nvprof</code>的输出。可以注意到，CPU计时器显示消耗的内核时间为3.26ms，而<code class="language-plaintext highlighter-rouge">nvprof</code>显示消耗的内核时间为2.90ms。在这个例子中，<code class="language-plaintext highlighter-rouge">nvprof</code>的结果更为精确，因为CPU计时器测量的时间中包含了来自<code class="language-plaintext highlighter-rouge">nvprof</code>附加的时间。</p><h2 id="组织并行线程">组织并行线程</h2><p>使用合适的网格和块大小来正确地组织线程，可以对内核性能产生很大的影响。</p><p>下图说明了块和线程索引、矩阵坐标以及线性全局内存索引之间的对应关系。</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1024 512'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/professional_cuda_c_programming/chap02/matrix-coordinate.png" alt="thread-orgnization" width="1024" height="512" /></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1024 512'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/professional_cuda_c_programming/chap02/matrix-coordinate2.png" alt="thread-orgnization" width="1024" height="512" /></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1024 512'%3E%3C/svg%3E" data-proofer-ignore data-src="../../img/professional_cuda_c_programming/chap02/matrix-coordinate3.png" alt="thread-orgnization" width="1024" height="512" /></p><h2 id="设备管理">设备管理</h2><h3 id="使用运行时api查询设备信息">使用运行时API查询设备信息</h3><div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
</pre><td class="rouge-code"><pre><span class="cp">#include</span> <span class="cpf">"../common/common.h"</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
</span>
<span class="cm">/*
 * Display a variety of information on the first CUDA device in this system,
 * including driver version, runtime version, compute capability, bytes of global memory, etc.
 */</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">**</span><span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"%s Starting...</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">argv</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>

    <span class="kt">int</span> <span class="n">deviceCount</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">cudaGetDeviceCount</span><span class="p">(</span><span class="o">&amp;</span><span class="n">deviceCount</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">deviceCount</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"There are no available device(s) that support CUDA</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"Detected %d CUDA Capable device(s)</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">deviceCount</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="kt">int</span> <span class="n">dev</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">driverVersion</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">runtimeVersion</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">CHECK</span><span class="p">(</span><span class="n">cudaSetDevice</span><span class="p">(</span><span class="n">dev</span><span class="p">));</span>
    <span class="n">cudaDeviceProp</span> <span class="n">deviceProp</span><span class="p">;</span> <span class="c1">// cudaDeviceProp结构体存放GPU设备的属性</span>
    <span class="n">CHECK</span><span class="p">(</span><span class="n">cudaGetDeviceProperties</span><span class="p">(</span><span class="o">&amp;</span><span class="n">deviceProp</span><span class="p">,</span> <span class="n">dev</span><span class="p">));</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"Device %d: </span><span class="se">\"</span><span class="s">%s</span><span class="se">\"\n</span><span class="s">"</span><span class="p">,</span> <span class="n">dev</span><span class="p">,</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">name</span><span class="p">);</span>

    <span class="n">cudaDriverGetVersion</span><span class="p">(</span><span class="o">&amp;</span><span class="n">driverVersion</span><span class="p">);</span>
    <span class="n">cudaRuntimeGetVersion</span><span class="p">(</span><span class="o">&amp;</span><span class="n">runtimeVersion</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  CUDA Driver Version / Runtime Version          %d.%d / %d.%d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span>
           <span class="n">driverVersion</span> <span class="o">/</span> <span class="mi">1000</span><span class="p">,</span> <span class="p">(</span><span class="n">driverVersion</span> <span class="o">%</span> <span class="mi">100</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10</span><span class="p">,</span>
           <span class="n">runtimeVersion</span> <span class="o">/</span> <span class="mi">1000</span><span class="p">,</span> <span class="p">(</span><span class="n">runtimeVersion</span> <span class="o">%</span> <span class="mi">100</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  CUDA Capability Major/Minor version number:    %d.%d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">major</span><span class="p">,</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">minor</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  Total amount of global memory:                 %.2f GBytes (%llu bytes)</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> 
           <span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="n">deviceProp</span><span class="p">.</span><span class="n">totalGlobalMem</span> <span class="o">/</span> <span class="n">pow</span><span class="p">(</span><span class="mf">1024.0</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="kt">long</span><span class="p">)</span><span class="n">deviceProp</span><span class="p">.</span><span class="n">totalGlobalMem</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  GPU Clock rate:                                %.0f MHz (%0.2f "</span>
           <span class="s">"GHz)</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">clockRate</span> <span class="o">*</span> <span class="mf">1e-3</span><span class="n">f</span><span class="p">,</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">clockRate</span> <span class="o">*</span> <span class="mf">1e-6</span><span class="n">f</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  Memory Clock rate:                             %.0f Mhz</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">memoryClockRate</span> <span class="o">*</span> <span class="mf">1e-3</span><span class="n">f</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  Memory Bus Width:                              %d-bit</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">memoryBusWidth</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">deviceProp</span><span class="p">.</span><span class="n">l2CacheSize</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"  L2 Cache Size:                                 %d bytes</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">l2CacheSize</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">"  Max Texture Dimension Size (x,y,z)             1D=(%d), "</span>
           <span class="s">"2D=(%d,%d), 3D=(%d,%d,%d)</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxTexture1D</span><span class="p">,</span>
           <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxTexture2D</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxTexture2D</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
           <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxTexture3D</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxTexture3D</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
           <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxTexture3D</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  Max Layered Texture Size (dim) x layers        1D=(%d) x %d, "</span>
           <span class="s">"2D=(%d,%d) x %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxTexture1DLayered</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
           <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxTexture1DLayered</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxTexture2DLayered</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
           <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxTexture2DLayered</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
           <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxTexture2DLayered</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  Total amount of constant memory:               %lu bytes</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">totalConstMem</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  Total amount of shared memory per block:       %lu bytes</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">sharedMemPerBlock</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  Total number of registers available per block: %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">regsPerBlock</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  Warp size:                                     %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">warpSize</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  Maximum number of threads per multiprocessor:  %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxThreadsPerMultiProcessor</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  Maximum number of threads per block:           %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxThreadsPerBlock</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  Maximum sizes of each dimension of a block:    %d x %d x %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span>
           <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxThreadsDim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
           <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxThreadsDim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
           <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxThreadsDim</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  Maximum sizes of each dimension of a grid:     %d x %d x %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span>
           <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxGridSize</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
           <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxGridSize</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
           <span class="n">deviceProp</span><span class="p">.</span><span class="n">maxGridSize</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"  Maximum memory pitch:                          %lu bytes</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">deviceProp</span><span class="p">.</span><span class="n">memPitch</span><span class="p">);</span>

    <span class="n">exit</span><span class="p">(</span><span class="n">EXIT_SUCCESS</span><span class="p">);</span>
<span class="p">}</span>
</pre></table></code></div></div><h3 id="确定最优gpu">确定最优GPU</h3><p>一些系统支持多GPU。在每个GPU都不同的情况下，选择性能最好的GPU运行核函数是非常重要的。 通过比较GPU包含的多处理器的数量选出计算能力最佳的GPU。如果你有一个多GPU系统， 可以使用以下代码来选择计算能力最优的设备。</p><div class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre><span class="kt">int</span> <span class="n">numDevices</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="n">cudaGetDeviceCount</span><span class="p">(</span><span class="o">&amp;</span><span class="n">numDevices</span><span class="p">);</span>  <span class="c1">// GPU卡的数量</span>
<span class="k">if</span> <span class="p">(</span><span class="n">numDevice</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">maxMultiprocessors</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">maxDevice</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">numDevices</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">cudaDeviceProp</span> <span class="n">props</span><span class="p">;</span>
        <span class="n">cudaGetDeviceProperties</span><span class="p">(</span><span class="o">&amp;</span><span class="n">props</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">maxMultiprocessors</span> <span class="o">&lt;</span> <span class="n">props</span><span class="p">.</span><span class="n">maxMultiprocessors</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">maxMultiprocessors</span> <span class="o">=</span> <span class="n">props</span><span class="p">.</span><span class="n">maxMultiprocessors</span><span class="p">;</span>
            <span class="n">maxDevice</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">cudaSetDevice</span><span class="p">(</span><span class="n">maxDevice</span><span class="p">);</span>
<span class="p">}</span>
</pre></table></code></div></div><h3 id="在运行时设置设备">在运行时设置设备</h3><p>对于一个有<code class="language-plaintext highlighter-rouge">N</code>个GPU的系统，<code class="language-plaintext highlighter-rouge">nvidia-smi</code>将设备ID标记为<code class="language-plaintext highlighter-rouge">0</code>到<code class="language-plaintext highlighter-rouge">N-1</code>。 使用环境变量<code class="language-plaintext highlighter-rouge">CUDA_VISIBLE_DEVICES</code>，就可在运行时指定所选的GPU且无须更改应用程序。设置运行时环境变量<code class="language-plaintext highlighter-rouge">export CUDA_VISIBLE_DEVICES=2</code>。 nvidia驱动程序会屏蔽其他GPU，这时设备<code class="language-plaintext highlighter-rouge">2</code>作为设备<code class="language-plaintext highlighter-rouge">0</code>出现在应用程序中。<code class="language-plaintext highlighter-rouge">CUDA_VISIBLE_DEVICES</code>也可用来指定多个设备。例如，如果想测试<code class="language-plaintext highlighter-rouge">GPU 2</code>和<code class="language-plaintext highlighter-rouge">GPU3</code>，可设置<code class="language-plaintext highlighter-rouge">CUDA_VISIBLE_DEVICES=2,3</code>。那么在运行时，nvidia驱动程序将只使用ID为2和3的设备，并且会将设备ID分别映射为0和1。</p><h2 id="习题">习题</h2><h3 id="1">1</h3><p><strong>在文件<code class="language-plaintext highlighter-rouge">sumArraysOnGPU-timer.cu</code>中，设置<code class="language-plaintext highlighter-rouge">block.x＝1023</code>，重新编译并运行。与执行配置为<code class="language-plaintext highlighter-rouge">block.x＝1024</code>的运行结果进行比较，试解释其区别和原因。</strong></p><blockquote><p><code class="language-plaintext highlighter-rouge">block.x＝1024</code>比<code class="language-plaintext highlighter-rouge">block.x＝1023</code> kernel函数的性能更好，在Tesla V100上测试前者是后者的1.05左右。</p><p>由于线程调度是以32个为1组，<code class="language-plaintext highlighter-rouge">block.x＝1023</code> 配置有2点缺陷：</p><ul><li>由于1023不能整除32，每个线程块的最后一个线程束都有一个disabled线程不工作。<li>每个线程块的线程束更少导致同样规模输入需要更多的线程块。由于只有一定数量的线程块可以真正并行，更多的线程块导致更长的执行时间。</ul></blockquote><h3 id="2">2</h3><p><strong>参考文件<code class="language-plaintext highlighter-rouge">sumArraysOnGPU-timer.cu</code>，设置<code class="language-plaintext highlighter-rouge">block.x＝256</code>。新建一个内核，使得每个线程处理两个元素。将此结果和其他的执行配置进行比较。</strong></p><blockquote><p>第一种方法：一个线程处理的2个元素是相邻的。</p></blockquote><div class="language-diff highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre> __global__ void sumArraysOnGPU(float *A, float *B, float *C, const int N)
 {
<span class="gd">-    int i = blockIdx.x * blockDim.x + threadIdx.x;
</span><span class="gi">+    int i = 2 * (blockIdx.x * blockDim.x + threadIdx.x);
+    int j = i + 1;
</span> 
     if (i &lt; N) C[i] = A[i] + B[i];
<span class="gi">+    if (j &lt; N) C[j] = A[j] + B[j];
</span> }
 
 int main(int argc, char **argv)
<span class="p">@@ -116,7 +118,7 @@</span> int main(int argc, char **argv)
     // invoke kernel at host side
     int iLen = 512;
     dim3 block (iLen);
<span class="gd">-    dim3 grid  ((nElem + block.x - 1) / block.x);
</span><span class="gi">+    dim3 grid  ((nElem / 2 + block.x - 1) / block.x);
</span> 
     iStart = seconds();
</pre></table></code></div></div><h3 id="3">3</h3><p><strong>参考文件<code class="language-plaintext highlighter-rouge">sumMatrixOnGPU-2D-grid-2D-block.cu</code>，并将它用于整数矩阵的加法运算中，获取最佳的执行配置</strong>。</p><blockquote><p>直接将float改为int即可。</p><p>dim3 block(dimx, dimy);中</p><p>(dimx, dimy) = (2, 128)的耗时几乎是<strong>(dimx, dimy) = (128, 2)</strong>的2倍</p><p>(dimx, dimy) = (64, 16)和(dimx, dimy) = (256, 4)的性能相当，是最佳的执行配置。</p></blockquote><h3 id="4">4</h3><p><strong>参考文件<code class="language-plaintext highlighter-rouge">sumMatrixOnGPU-2D-grid-1D-block.cu</code>，新建一个内核，使得每个线程处理两个元素，获取最佳的执行配置</strong>。</p><blockquote><p>dim3 block(dimx);中</p><p>dimx=256是最佳配置。配置范围{1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024}</p><p>总体规律是：dimx&lt;=256时，随着dimx的增加，性能呈线性趋势增加；在dimx&gt;256时，随着dimx的增加，性能缓缓下降。</p></blockquote><h3 id="5">5</h3><p><strong>借助程序<code class="language-plaintext highlighter-rouge">checkDeviceInfor.cu</code>，找到你的系统所支持的网格和块的最大尺寸。</strong></p><blockquote><p>在Tesla V100上</p><p>Maximum sizes of each dimension of a block: 1024 x 1024 x 64 Maximum sizes of each dimension of a grid: 2147483647 x 65535 x 65535</p></blockquote></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/cuda/'>CUDA</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/gpu/" class="post-tag no-text-decoration" >GPU</a> <a href="/tags/cuda/" class="post-tag no-text-decoration" >CUDA</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=读薄《Professional CUDA C Programming》——CUDA编程模型 - zuo&url=https://dazuozcy.github.io/posts/cuda-c-chap02/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=读薄《Professional CUDA C Programming》——CUDA编程模型 - zuo&u=https://dazuozcy.github.io/posts/cuda-c-chap02/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=读薄《Professional CUDA C Programming》——CUDA编程模型 - zuo&url=https://dazuozcy.github.io/posts/cuda-c-chap02/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/simd/">SIMD</a><li><a href="/posts/new-from-cpp11/">C++11开始的新特性</a><li><a href="/posts/keywords/">关键字</a><li><a href="/posts/empty-class/">class的内存模型</a><li><a href="/posts/diff-between-c-cpp/">C++ vs C</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/gpu/">GPU</a> <a class="post-tag" href="/tags/cuda/">CUDA</a> <a class="post-tag" href="/tags/openmp/">openMP</a> <a class="post-tag" href="/tags/c/">C++</a> <a class="post-tag" href="/tags/parallel-computing/">Parallel Computing</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/forecasting-at-scale/">Forecasting at Scale</a> <a class="post-tag" href="/tags/onednn/">oneDNN</a> <a class="post-tag" href="/tags/%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88/">智能指针</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/cuda-c-chap01/"><div class="card-body"> <span class="timeago small" > Jul 1, 2020 <i class="unloaded">2020-07-01T20:19:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>读薄《Professional CUDA C Programming》——基于CUDA的异构并行编程</h3><div class="text-muted small"><p> 一、基于CUDA的异构并行编程 并行计算 并行计算的主要目标是提高计算速度。并行计算的软件和硬件层面是紧密联系的，传说中的软硬件协同。并行计算通常涉及两个不同的计算技术领域： 计算机架构（硬件方面），关注的是在结构层次上支持并行性。 并行程序设计（软件方面），关注的是充分使用架构的计算能力来并发地解决问题。 并行性 在并行算法的实现中， 分析数据的相关性是最基本的内容，...</p></div></div></a></div><div class="card"> <a href="/posts/cuda-c-chap03/"><div class="card-body"> <span class="timeago small" > Jul 1, 2020 <i class="unloaded">2020-07-01T20:19:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>读薄《Professional CUDA C Programming》——CUDA执行模型</h3><div class="text-muted small"><p> 三、CUDA执行模型 通过第二章的练习，已经了解了如何在网格和线程块中组织线程以获得最佳的性能。尽管可通过反复试验找到最佳的执行配置，但可能仍然会感到疑惑，为什么选择这样的执行配置会更好。你可能想知道是否有一些选择网格和块配置的准则。本章将回答这些问题， 并从硬件方面深入介绍内核启动配置和性能分析的信息。 CUDA执行模型概述 一般来说，执行模型会提供一个操作视图，说明如何在特定的计算...</p></div></div></a></div><div class="card"> <a href="/posts/cuda-c-chap04/"><div class="card-body"> <span class="timeago small" > Jul 1, 2020 <i class="unloaded">2020-07-01T20:19:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>读薄《Professional CUDA C Programming》——全局内存</h3><div class="text-muted small"><p> 四、全局内存 本章将剖析核函数与全局内存的联系及其对性能的影响。本章将介绍CUDA内存模型， 并通过分析不同的全局内存访问模式来介绍如何通过核函数高效地利用全局内存。 CUDA内存模型概述 在现有的硬件存储子系统下，必须依靠内存模型获得最佳的延迟和带宽。CUDA内存模型结合了主机和设备的内存系统，展现了完整的内存层次结构，使你能显式地控制数据布局以优化性能。 内存层次结构的优点 一...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/cuda-c-chap01/" class="btn btn-outline-primary" prompt="Older"><p>读薄《Professional CUDA C Programming》——基于CUDA的异构并行编程</p></a> <a href="/posts/cuda-c-chap03/" class="btn btn-outline-primary" prompt="Newer"><p>读薄《Professional CUDA C Programming》——CUDA执行模型</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/dazuozcy">zuo</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/gpu/">GPU</a> <a class="post-tag" href="/tags/cuda/">CUDA</a> <a class="post-tag" href="/tags/openmp/">openMP</a> <a class="post-tag" href="/tags/c/">C++</a> <a class="post-tag" href="/tags/parallel-computing/">Parallel Computing</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/forecasting-at-scale/">Forecasting at Scale</a> <a class="post-tag" href="/tags/onednn/">oneDNN</a> <a class="post-tag" href="/tags/%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88/">智能指针</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { let initTheme = "default"; if ($("html[mode=dark]").length > 0 || ($("html[mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://dazuozcy.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script async src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script>
